{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Filtering and Querying\n",
    "\n",
    "This notebook demonstrates advanced data filtering, querying, and subsetting techniques using pyCLIF for efficient analysis of large healthcare datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Healthcare datasets are often large and complex. Effective filtering enables:\n",
    "- Memory-efficient data loading\n",
    "- Focused analysis on specific patient populations\n",
    "- Time-based subsetting for longitudinal studies\n",
    "- Custom cohort creation for research questions\n",
    "- Performance optimization for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pyCLIF components\n",
    "from pyclif import CLIF\n",
    "from pyclif.tables.vitals import vitals\n",
    "from pyclif.tables.patient import patient\n",
    "from pyclif.tables.hospitalization import hospitalization\n",
    "from pyclif.utils.io import load_data\n",
    "\n",
    "print(f\"Data filtering environment setup complete!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = \"/Users/vaishvik/downloads/CLIF_MIMIC\"\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Filtering During Data Loading\n",
    "\n",
    "The most efficient way to filter data is during the loading process, which reduces memory usage and improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only specific columns for memory efficiency\n",
    "essential_vitals_columns = [\n",
    "    'patient_id', \n",
    "    'hospitalization_id', \n",
    "    'vital_category', \n",
    "    'vital_value', \n",
    "    'recorded_dttm'\n",
    "]\n",
    "\n",
    "vitals_subset = load_data(\n",
    "    table_name=\"vitals\",\n",
    "    table_path=DATA_DIR,\n",
    "    table_format_type=\"parquet\",\n",
    "    columns=essential_vitals_columns,\n",
    "    site_tz=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(f\"=== COLUMN SELECTION FILTERING ===\")\n",
    "print(f\"Selected columns: {essential_vitals_columns}\")\n",
    "print(f\"Loaded data shape: {vitals_subset.shape}\")\n",
    "print(f\"Actual columns: {list(vitals_subset.columns)}\")\n",
    "print(f\"Memory usage: {vitals_subset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by specific vital categories during loading\n",
    "cardiac_vitals = load_data(\n",
    "    table_name=\"vitals\",\n",
    "    table_path=DATA_DIR,\n",
    "    table_format_type=\"parquet\",\n",
    "    columns=essential_vitals_columns,\n",
    "    filters={'vital_category': ['heart_rate', 'sbp', 'dbp']},  # Only cardiac vitals\n",
    "    site_tz=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(f\"=== VALUE-BASED FILTERING ===\")\n",
    "print(f\"Filter: cardiac vitals only\")\n",
    "print(f\"Loaded data shape: {cardiac_vitals.shape}\")\n",
    "if 'vital_category' in cardiac_vitals.columns:\n",
    "    print(f\"Unique vital categories: {cardiac_vitals['vital_category'].unique()}\")\n",
    "    print(f\"Category counts:\")\n",
    "    print(cardiac_vitals['vital_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by multiple criteria\n",
    "respiratory_vitals = load_data(\n",
    "    table_name=\"vitals\",\n",
    "    table_path=DATA_DIR,\n",
    "    table_format_type=\"parquet\",\n",
    "    columns=essential_vitals_columns,\n",
    "    filters={\n",
    "        'vital_category': ['respiratory_rate', 'oxygen_saturation'],  # Respiratory vitals\n",
    "    },\n",
    "    sample_size=2000,  # Limit sample size\n",
    "    site_tz=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(f\"=== MULTI-CRITERIA FILTERING ===\")\n",
    "print(f\"Filter: respiratory vitals with sample limit\")\n",
    "print(f\"Loaded data shape: {respiratory_vitals.shape}\")\n",
    "if 'vital_category' in respiratory_vitals.columns:\n",
    "    print(f\"Unique vital categories: {respiratory_vitals['vital_category'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Loading Filtering with Table Methods\n",
    "\n",
    "Use built-in table methods for filtering after data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a full vitals table for demonstration\n",
    "vitals_table = vitals.from_file(DATA_DIR, \"parquet\")\n",
    "\n",
    "print(f\"=== FULL VITALS TABLE ===\")\n",
    "print(f\"Total records: {len(vitals_table.df):,}\")\n",
    "print(f\"Unique patients: {vitals_table.df['patient_id'].nunique():,}\")\n",
    "print(f\"Vital categories: {len(vitals_table.get_vital_categories())}\")\n",
    "\n",
    "# Get date range\n",
    "if 'recorded_dttm' in vitals_table.df.columns:\n",
    "    date_min = vitals_table.df['recorded_dttm'].min()\n",
    "    date_max = vitals_table.df['recorded_dttm'].max()\n",
    "    print(f\"Date range: {date_min} to {date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Vital Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by specific vital category\n",
    "available_vitals = vitals_table.get_vital_categories()\n",
    "print(f\"Available vital categories: {available_vitals[:10]}...\")  # Show first 10\n",
    "\n",
    "# Focus on heart rate if available\n",
    "if 'heart_rate' in available_vitals:\n",
    "    hr_data = vitals_table.filter_by_vital_category('heart_rate')\n",
    "    \n",
    "    print(f\"\\n=== HEART RATE FILTERING ===\")\n",
    "    print(f\"Heart rate records: {len(hr_data):,}\")\n",
    "    print(f\"Unique patients with HR data: {hr_data['patient_id'].nunique():,}\")\n",
    "    \n",
    "    if 'vital_value' in hr_data.columns:\n",
    "        print(f\"HR range: {hr_data['vital_value'].min():.1f} - {hr_data['vital_value'].max():.1f} bpm\")\n",
    "        print(f\"HR mean Â± std: {hr_data['vital_value'].mean():.1f} Â± {hr_data['vital_value'].std():.1f} bpm\")\n",
    "else:\n",
    "    print(\"Heart rate data not available, using first available vital category\")\n",
    "    if available_vitals:\n",
    "        first_vital = available_vitals[0]\n",
    "        vital_data = vitals_table.filter_by_vital_category(first_vital)\n",
    "        print(f\"\\n=== {first_vital.upper()} FILTERING ===\")\n",
    "        print(f\"{first_vital} records: {len(vital_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top hospitalizations by measurement count\n",
    "hosp_counts = vitals_table.df['hospitalization_id'].value_counts()\n",
    "print(f\"=== HOSPITALIZATION FILTERING ===\")\n",
    "print(f\"Total hospitalizations: {len(hosp_counts):,}\")\n",
    "print(f\"Top 5 hospitalizations by measurement count:\")\n",
    "\n",
    "for i, (hosp_id, count) in enumerate(hosp_counts.head(5).items()):\n",
    "    print(f\"  {i+1}. {hosp_id}: {count:,} measurements\")\n",
    "\n",
    "# Filter by specific hospitalization\n",
    "if len(hosp_counts) > 0:\n",
    "    top_hosp_id = hosp_counts.index[0]\n",
    "    hosp_data = vitals_table.filter_by_hospitalization(top_hosp_id)\n",
    "    \n",
    "    print(f\"\\nAnalysis of hospitalization {top_hosp_id}:\")\n",
    "    print(f\"  Total measurements: {len(hosp_data):,}\")\n",
    "    print(f\"  Vital categories: {hosp_data['vital_category'].nunique()}\")\n",
    "    \n",
    "    if 'recorded_dttm' in hosp_data.columns:\n",
    "        duration = hosp_data['recorded_dttm'].max() - hosp_data['recorded_dttm'].min()\n",
    "        print(f\"  Duration: {duration.days} days, {duration.seconds//3600} hours\")\n",
    "    \n",
    "    # Show vital category breakdown\n",
    "    print(f\"  Vital breakdown:\")\n",
    "    vital_breakdown = hosp_data['vital_category'].value_counts().head(5)\n",
    "    for vital, count in vital_breakdown.items():\n",
    "        print(f\"    {vital}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date range\n",
    "if 'recorded_dttm' in vitals_table.df.columns:\n",
    "    # Get overall date range\n",
    "    full_date_range = {\n",
    "        'start': vitals_table.df['recorded_dttm'].min(),\n",
    "        'end': vitals_table.df['recorded_dttm'].max()\n",
    "    }\n",
    "    \n",
    "    print(f\"=== DATE RANGE FILTERING ===\")\n",
    "    print(f\"Full dataset range: {full_date_range['start']} to {full_date_range['end']}\")\n",
    "    total_days = (full_date_range['end'] - full_date_range['start']).days\n",
    "    print(f\"Total span: {total_days} days\")\n",
    "    \n",
    "    # Filter to recent data (last 30 days of available data)\n",
    "    recent_start = full_date_range['end'] - timedelta(days=30)\n",
    "    recent_data = vitals_table.filter_by_date_range(recent_start, full_date_range['end'])\n",
    "    \n",
    "    print(f\"\\nRecent data (last 30 days):\")\n",
    "    print(f\"  Date range: {recent_start} to {full_date_range['end']}\")\n",
    "    print(f\"  Records: {len(recent_data):,}\")\n",
    "    print(f\"  Patients: {recent_data['patient_id'].nunique():,}\")\n",
    "    \n",
    "    # Filter to middle period\n",
    "    mid_start = full_date_range['start'] + timedelta(days=total_days//3)\n",
    "    mid_end = full_date_range['start'] + timedelta(days=2*total_days//3)\n",
    "    mid_data = vitals_table.filter_by_date_range(mid_start, mid_end)\n",
    "    \n",
    "    print(f\"\\nMiddle period data:\")\n",
    "    print(f\"  Date range: {mid_start} to {mid_end}\")\n",
    "    print(f\"  Records: {len(mid_data):,}\")\n",
    "    print(f\"  Patients: {mid_data['patient_id'].nunique():,}\")\nelse:\n    print(\"No datetime column available for date filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Filtering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Multi-Condition Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex filters using pandas operations\n",
    "def apply_complex_filters(df, conditions):\n",
    "    \"\"\"Apply multiple filtering conditions to a DataFrame.\"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    print(f\"=== COMPLEX FILTERING ===\")\n",
    "    print(f\"Starting records: {len(filtered_df):,}\")\n",
    "    \n",
    "    for i, (description, condition) in enumerate(conditions):\n",
    "        before_count = len(filtered_df)\n",
    "        filtered_df = filtered_df[condition(filtered_df)]\n",
    "        after_count = len(filtered_df)\n",
    "        removed = before_count - after_count\n",
    "        \n",
    "        print(f\"  {i+1}. {description}:\")\n",
    "        print(f\"     Removed: {removed:,} records\")\n",
    "        print(f\"     Remaining: {after_count:,} records\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(filtered_df):,} records\")\n",
    "    return filtered_df\n",
    "\n",
    "# Define complex filtering conditions\n",
    "if 'heart_rate' in available_vitals:\n",
    "    complex_conditions = [\n",
    "        (\"Heart rate data only\", \n",
    "         lambda df: df['vital_category'] == 'heart_rate'),\n",
    "        (\"Valid heart rate range (30-200 bpm)\", \n",
    "         lambda df: (df['vital_value'] >= 30) & (df['vital_value'] <= 200)),\n",
    "        (\"Recent data (last 90 days)\", \n",
    "         lambda df: df['recorded_dttm'] >= (df['recorded_dttm'].max() - timedelta(days=90)))\n",
    "    ]\n",
    "    \n",
    "    filtered_hr_data = apply_complex_filters(vitals_table.df, complex_conditions)\n",
    "    \n",
    "    # Analyze filtered results\n",
    "    print(f\"\\n=== FILTERED RESULTS ANALYSIS ===\")\n",
    "    print(f\"Unique patients: {filtered_hr_data['patient_id'].nunique():,}\")\n",
    "    print(f\"Unique hospitalizations: {filtered_hr_data['hospitalization_id'].nunique():,}\")\n",
    "    print(f\"HR statistics:\")\n",
    "    print(f\"  Mean: {filtered_hr_data['vital_value'].mean():.1f} bpm\")\n",
    "    print(f\"  Std: {filtered_hr_data['vital_value'].std():.1f} bpm\")\n",
    "    print(f\"  Range: {filtered_hr_data['vital_value'].min():.1f} - {filtered_hr_data['vital_value'].max():.1f} bpm\")\nelse:\n",
    "    print(\"Heart rate data not available for complex filtering demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Cohort Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patient cohorts based on data characteristics\n",
    "def create_patient_cohorts(vitals_df):\n",
    "    \"\"\"Create patient cohorts based on vital signs data characteristics.\"\"\"\n",
    "    \n",
    "    # Calculate patient-level statistics\n",
    "    patient_stats = vitals_df.groupby('patient_id').agg({\n",
    "        'vital_value': ['count', 'mean', 'std'],\n",
    "        'vital_category': 'nunique',\n",
    "        'recorded_dttm': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    patient_stats.columns = [\n",
    "        'patient_id', 'total_measurements', 'mean_vital_value', 'std_vital_value',\n",
    "        'unique_vital_categories', 'first_measurement', 'last_measurement'\n",
    "    ]\n",
    "    \n",
    "    # Calculate length of stay in dataset\n",
    "    patient_stats['los_days'] = (patient_stats['last_measurement'] - patient_stats['first_measurement']).dt.days\n",
    "    \n",
    "    print(f\"=== PATIENT COHORT CREATION ===\")\n",
    "    print(f\"Total patients analyzed: {len(patient_stats):,}\")\n",
    "    \n",
    "    # Define cohorts\n",
    "    cohorts = {}\n",
    "    \n",
    "    # Cohort 1: High-frequency monitoring (many measurements)\n",
    "    high_freq_threshold = patient_stats['total_measurements'].quantile(0.75)\n",
    "    cohorts['high_frequency'] = patient_stats[patient_stats['total_measurements'] >= high_freq_threshold]\n",
    "    \n",
    "    # Cohort 2: Long-stay patients (many days in dataset)\n",
    "    long_stay_threshold = patient_stats['los_days'].quantile(0.75)\n",
    "    cohorts['long_stay'] = patient_stats[patient_stats['los_days'] >= long_stay_threshold]\n",
    "    \n",
    "    # Cohort 3: Comprehensive monitoring (many vital types)\n",
    "    comprehensive_threshold = patient_stats['unique_vital_categories'].quantile(0.75)\n",
    "    cohorts['comprehensive'] = patient_stats[patient_stats['unique_vital_categories'] >= comprehensive_threshold]\n",
    "    \n",
    "    # Report cohort characteristics\n",
    "    for cohort_name, cohort_data in cohorts.items():\n",
    "        print(f\"\\n{cohort_name.upper()} COHORT:\")\n",
    "        print(f\"  Patients: {len(cohort_data):,}\")\n",
    "        print(f\"  Avg measurements: {cohort_data['total_measurements'].mean():.1f}\")\n",
    "        print(f\"  Avg length of stay: {cohort_data['los_days'].mean():.1f} days\")\n",
    "        print(f\"  Avg vital categories: {cohort_data['unique_vital_categories'].mean():.1f}\")\n",
    "    \n",
    "    return patient_stats, cohorts\n",
    "\n",
    "# Create cohorts from available data\nif len(vitals_table.df) > 0:\n    patient_statistics, patient_cohorts = create_patient_cohorts(vitals_table.df)\nelse:\n    print(\"Insufficient data for cohort analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Filtering and Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical filtering to remove outliers\n",
    "def statistical_filtering(df, vital_category, method='iqr', factor=1.5):\n",
    "    \"\"\"Apply statistical filtering to remove outliers.\"\"\"\n",
    "    \n",
    "    vital_data = df[df['vital_category'] == vital_category].copy()\n",
    "    \n",
    "    if len(vital_data) == 0:\n",
    "        print(f\"No data available for {vital_category}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    original_count = len(vital_data)\n",
    "    \n",
    "    print(f\"=== STATISTICAL FILTERING: {vital_category.upper()} ===\")\n",
    "    print(f\"Original records: {original_count:,}\")\n",
    "    print(f\"Method: {method.upper()}\")\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        # Interquartile Range method\n",
    "        Q1 = vital_data['vital_value'].quantile(0.25)\n",
    "        Q3 = vital_data['vital_value'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        print(f\"IQR bounds: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        # Z-score method\n",
    "        mean_val = vital_data['vital_value'].mean()\n",
    "        std_val = vital_data['vital_value'].std()\n",
    "        \n",
    "        lower_bound = mean_val - factor * std_val\n",
    "        upper_bound = mean_val + factor * std_val\n",
    "        \n",
    "        print(f\"Z-score bounds: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
    "    \n",
    "    elif method == 'percentile':\n",
    "        # Percentile method\n",
    "        lower_percentile = (100 - 95) / 2  # 2.5th percentile\n",
    "        upper_percentile = 100 - lower_percentile  # 97.5th percentile\n",
    "        \n",
    "        lower_bound = vital_data['vital_value'].quantile(lower_percentile / 100)\n",
    "        upper_bound = vital_data['vital_value'].quantile(upper_percentile / 100)\n",
    "        \n",
    "        print(f\"Percentile bounds: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_data = vital_data[\n",
    "        (vital_data['vital_value'] >= lower_bound) & \n",
    "        (vital_data['vital_value'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "    filtered_count = len(filtered_data)\n",
    "    removed_count = original_count - filtered_count\n",
    "    \n",
    "    print(f\"Removed outliers: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
    "    print(f\"Remaining records: {filtered_count:,}\")\n",
    "    \n",
    "    # Statistics comparison\n",
    "    print(f\"\\nBefore filtering:\")\n",
    "    print(f\"  Mean Â± Std: {vital_data['vital_value'].mean():.1f} Â± {vital_data['vital_value'].std():.1f}\")\n",
    "    print(f\"  Range: {vital_data['vital_value'].min():.1f} - {vital_data['vital_value'].max():.1f}\")\n",
    "    \n",
    "    print(f\"After filtering:\")\n",
    "    print(f\"  Mean Â± Std: {filtered_data['vital_value'].mean():.1f} Â± {filtered_data['vital_value'].std():.1f}\")\n",
    "    print(f\"  Range: {filtered_data['vital_value'].min():.1f} - {filtered_data['vital_value'].max():.1f}\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Apply statistical filtering to available vital\nif available_vitals:\n    test_vital = available_vitals[0]  # Use first available vital\n    \n",
    "    # Test different methods\n    for method in ['iqr', 'zscore', 'percentile']:\n",
    "        filtered_result = statistical_filtering(vitals_table.df, test_vital, method=method)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\nelse:\n    print(\"No vital categories available for statistical filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance-Optimized Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate performance considerations\n",
    "import time\n",
    "\n",
    "def benchmark_filtering_methods(df, sample_size=10000):\n",
    "    \"\"\"Benchmark different filtering approaches for performance.\"\"\"\n",
    "    \n",
    "    # Use a sample for benchmarking\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    print(f\"=== FILTERING PERFORMANCE BENCHMARK ===\")\n",
    "    print(f\"Sample size: {len(df_sample):,} records\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Method 1: Basic pandas filtering\n",
    "    start_time = time.time()\n",
    "    if 'vital_category' in df_sample.columns:\n",
    "        method1_result = df_sample[df_sample['vital_category'].isin(['heart_rate', 'sbp'])]\n",
    "    else:\n",
    "        method1_result = df_sample.head(100)  # Fallback\n",
    "    method1_time = time.time() - start_time\n",
    "    results['pandas_isin'] = {'time': method1_time, 'records': len(method1_result)}\n",
    "    \n",
    "    # Method 2: Query method\n",
    "    start_time = time.time()\n",
    "    if 'vital_category' in df_sample.columns:\n",
    "        method2_result = df_sample.query(\"vital_category in ['heart_rate', 'sbp']\")\n",
    "    else:\n",
    "        method2_result = df_sample.head(100)  # Fallback\n",
    "    method2_time = time.time() - start_time\n",
    "    results['pandas_query'] = {'time': method2_time, 'records': len(method2_result)}\n",
    "    \n",
    "    # Method 3: Boolean indexing\n",
    "    start_time = time.time()\n",
    "    if 'vital_category' in df_sample.columns:\n",
    "        mask = (df_sample['vital_category'] == 'heart_rate') | (df_sample['vital_category'] == 'sbp')\n",
    "        method3_result = df_sample[mask]\n",
    "    else:\n",
    "        method3_result = df_sample.head(100)  # Fallback\n",
    "    method3_time = time.time() - start_time\n",
    "    results['boolean_indexing'] = {'time': method3_time, 'records': len(method3_result)}\n",
    "    \n",
    "    # Report results\n",
    "    print(\"Performance comparison:\")\n",
    "    for method, result in results.items():\n",
    "        print(f\"  {method:<18}: {result['time']*1000:>6.2f} ms ({result['records']:,} records)\")\n",
    "    \n",
    "    # Determine fastest method\n",
    "    fastest_method = min(results.keys(), key=lambda x: results[x]['time'])\n",
    "    print(f\"\\nðŸ† Fastest method: {fastest_method}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance benchmark\nif len(vitals_table.df) > 0:\n    perf_results = benchmark_filtering_methods(vitals_table.df)\nelse:\n    print(\"Insufficient data for performance benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Filtering Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient filtering for large datasets\n",
    "def memory_efficient_analysis(data_dir, chunk_size=1000):\n",
    "    \"\"\"Demonstrate memory-efficient processing of large datasets.\"\"\"\n",
    "    \n",
    "    print(f\"=== MEMORY-EFFICIENT FILTERING ===\")\n",
    "    print(f\"Strategy: Load data in chunks of {chunk_size:,} records\")\n",
    "    print()\n",
    "    \n",
    "    # Load data in chunks with filters\n",
    "    chunk_results = []\n",
    "    \n",
    "    try:\n",
    "        # Load small chunks with specific filters\n",
    "        for i in range(3):  # Process 3 chunks as demonstration\n",
    "            chunk_data = load_data(\n",
    "                table_name=\"vitals\",\n",
    "                table_path=data_dir,\n",
    "                table_format_type=\"parquet\",\n",
    "                columns=['patient_id', 'vital_category', 'vital_value'],\n",
    "                sample_size=chunk_size,\n",
    "                filters={'vital_category': ['heart_rate']} if i == 0 else None\n",
    "            )\n",
    "            \n",
    "            if not chunk_data.empty:\n",
    "                # Process chunk\n",
    "                chunk_summary = {\n",
    "                    'chunk_id': i,\n",
    "                    'records': len(chunk_data),\n",
    "                    'patients': chunk_data['patient_id'].nunique() if 'patient_id' in chunk_data.columns else 0,\n",
    "                    'memory_mb': chunk_data.memory_usage(deep=True).sum() / 1024**2\n",
    "                }\n",
    "                \n",
    "                if 'vital_category' in chunk_data.columns:\n",
    "                    chunk_summary['vital_categories'] = chunk_data['vital_category'].nunique()\n",
    "                \n",
    "                chunk_results.append(chunk_summary)\n",
    "                \n",
    "                print(f\"Chunk {i+1}:\")\n",
    "                print(f\"  Records: {chunk_summary['records']:,}\")\n",
    "                print(f\"  Patients: {chunk_summary['patients']:,}\")\n",
    "                print(f\"  Memory: {chunk_summary['memory_mb']:.2f} MB\")\n",
    "                if 'vital_categories' in chunk_summary:\n",
    "                    print(f\"  Vital categories: {chunk_summary['vital_categories']}\")\n",
    "                \n",
    "                # Clear chunk from memory\n",
    "                del chunk_data\n",
    "                print(f\"  âœ… Chunk processed and cleared from memory\")\n",
    "                print()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunk processing: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    if chunk_results:\n",
    "        total_records = sum(chunk['records'] for chunk in chunk_results)\n",
    "        total_memory = sum(chunk['memory_mb'] for chunk in chunk_results)\n",
    "        \n",
    "        print(f\"CHUNK PROCESSING SUMMARY:\")\n",
    "        print(f\"  Total chunks: {len(chunk_results)}\")\n",
    "        print(f\"  Total records processed: {total_records:,}\")\n",
    "        print(f\"  Peak memory usage: {max(chunk['memory_mb'] for chunk in chunk_results):.2f} MB\")\n",
    "        print(f\"  ðŸ’¡ Memory efficient: Process large datasets without loading all data at once\")\n",
    "    \n",
    "    return chunk_results\n",
    "\n",
    "# Demonstrate memory-efficient processing\nmemory_analysis = memory_efficient_analysis(DATA_DIR, chunk_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive filtering best practices\n",
    "def filtering_best_practices_summary():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"          FILTERING BEST PRACTICES SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸš€ PERFORMANCE OPTIMIZATION:\")\n",
    "    print(\"  1. Filter during data loading when possible\")\n",
    "    print(\"     â€¢ Use 'filters' parameter in load_data()\")\n",
    "    print(\"     â€¢ Select only needed columns\")\n",
    "    print(\"     â€¢ Use sample_size for testing\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ’¾ MEMORY MANAGEMENT:\")\n",
    "    print(\"  2. Process large datasets in chunks\")\n",
    "    print(\"     â€¢ Load, process, and clear chunks iteratively\")\n",
    "    print(\"     â€¢ Use generators for streaming processing\")\n",
    "    print(\"     â€¢ Monitor memory usage during processing\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸŽ¯ FILTERING STRATEGIES:\")\n",
    "    print(\"  3. Layer your filters from general to specific\")\n",
    "    print(\"     â€¢ Start with broad categorical filters\")\n",
    "    print(\"     â€¢ Apply temporal filters\")\n",
    "    print(\"     â€¢ Finish with value-based filters\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ“Š DATA QUALITY:\")\n",
    "    print(\"  4. Always validate filtered results\")\n",
    "    print(\"     â€¢ Check record counts make sense\")\n",
    "    print(\"     â€¢ Verify patient/hospitalization counts\")\n",
    "    print(\"     â€¢ Review statistical summaries\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ”§ IMPLEMENTATION TIPS:\")\n",
    "    print(\"  5. Use appropriate pandas methods\")\n",
    "    print(\"     â€¢ .isin() for multiple values\")\n",
    "    print(\"     â€¢ .query() for complex conditions\")\n",
    "    print(\"     â€¢ Boolean indexing for simple conditions\")\n",
    "    print()\n",
    "    \n",
    "    print(\"âš ï¸  COMMON PITFALLS TO AVOID:\")\n",
    "    print(\"  â€¢ Loading entire dataset before filtering\")\n",
    "    print(\"  â€¢ Using loops instead of vectorized operations\")\n",
    "    print(\"  â€¢ Not considering timezone effects in date filters\")\n",
    "    print(\"  â€¢ Forgetting to validate filter results\")\n",
    "    print(\"  â€¢ Not documenting filter criteria for reproducibility\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸŽ¯ YOUR RECOMMENDED WORKFLOW:\")\n",
    "    print(\"  1. Define your research question and required data\")\n",
    "    print(\"  2. Identify optimal filters for data loading\")\n",
    "    print(\"  3. Load data with initial filters applied\")\n",
    "    print(\"  4. Apply additional post-loading filters as needed\")\n",
    "    print(\"  5. Validate and document your filtering decisions\")\n",
    "    print(\"  6. Create reusable filter functions for consistency\")\n    print()\n    print(\"=\" * 60)\n\nfiltering_best_practices_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Complete Filtering Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a complete filtering workflow for a research question\n",
    "def complete_filtering_workflow():\n",
    "    \"\"\"Complete example: Analyze cardiac vitals for ICU patients with long stays.\"\"\"\n",
    "    \n",
    "    print(\"=== COMPLETE FILTERING WORKFLOW EXAMPLE ===\")\n",
    "    print(\"Research Question: Analyze cardiac vital patterns for patients with extended ICU stays\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Load data with initial filters\n",
    "    print(\"Step 1: Load cardiac vitals data\")\n",
    "    cardiac_data = load_data(\n",
    "        table_name=\"vitals\",\n",
    "        table_path=DATA_DIR,\n",
    "        table_format_type=\"parquet\",\n",
    "        columns=['patient_id', 'hospitalization_id', 'vital_category', 'vital_value', 'recorded_dttm'],\n",
    "        filters={'vital_category': ['heart_rate', 'sbp', 'dbp']},\n",
    "        site_tz=\"US/Eastern\"\n",
    "    )\n",
    "    print(f\"  Loaded: {len(cardiac_data):,} cardiac vital records\")\n",
    "    \n",
    "    if cardiac_data.empty:\n",
    "        print(\"  No cardiac data available, ending workflow\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Identify long-stay patients\n",
    "    print(\"\\nStep 2: Identify patients with extended stays\")\n",
    "    patient_stay_duration = cardiac_data.groupby('patient_id')['recorded_dttm'].agg(['min', 'max'])\n",
    "    patient_stay_duration['stay_days'] = (patient_stay_duration['max'] - patient_stay_duration['min']).dt.days\n",
    "    \n",
    "    # Define \"long stay\" as top quartile\n",
    "    long_stay_threshold = patient_stay_duration['stay_days'].quantile(0.75)\n",
    "    long_stay_patients = patient_stay_duration[patient_stay_duration['stay_days'] >= long_stay_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"  Long stay threshold: {long_stay_threshold:.1f} days\")\n",
    "    print(f\"  Long stay patients: {len(long_stay_patients):,}\")\n",
    "    \n",
    "    # Step 3: Filter to long-stay patients\n",
    "    print(\"\\nStep 3: Filter data to long-stay patients\")\n",
    "    long_stay_data = cardiac_data[cardiac_data['patient_id'].isin(long_stay_patients)]\n",
    "    print(f\"  Filtered records: {len(long_stay_data):,}\")\n",
    "    \n",
    "    # Step 4: Apply data quality filters\n",
    "    print(\"\\nStep 4: Apply data quality filters\")\n",
    "    \n",
    "    # Remove physiologically implausible values\n",
    "    quality_filters = {\n",
    "        'heart_rate': (30, 200),\n",
    "        'sbp': (50, 250),\n",
    "        'dbp': (20, 150)\n",
    "    }\n",
    "    \n",
    "    clean_data = long_stay_data.copy()\n",
    "    for vital, (min_val, max_val) in quality_filters.items():\n",
    "        before_count = len(clean_data[clean_data['vital_category'] == vital])\n",
    "        clean_data = clean_data[\n",
    "            (clean_data['vital_category'] != vital) |\n",
    "            ((clean_data['vital_category'] == vital) & \n",
    "             (clean_data['vital_value'] >= min_val) & \n",
    "             (clean_data['vital_value'] <= max_val))\n",
    "        ]\n",
    "        after_count = len(clean_data[clean_data['vital_category'] == vital])\n",
    "        removed = before_count - after_count\n",
    "        print(f\"  {vital}: removed {removed:,} outliers ({removed/before_count*100:.1f}% if before_count > 0 else 0)\")\n",
    "    \n",
    "    # Step 5: Final analysis\n",
    "    print(f\"\\nStep 5: Final dataset characteristics\")\n",
    "    print(f\"  Final records: {len(clean_data):,}\")\n",
    "    print(f\"  Patients: {clean_data['patient_id'].nunique():,}\")\n",
    "    print(f\"  Hospitalizations: {clean_data['hospitalization_id'].nunique():,}\")\n",
    "    print(f\"  Date range: {clean_data['recorded_dttm'].min()} to {clean_data['recorded_dttm'].max()}\")\n",
    "    \n",
    "    # Vital-specific summaries\n",
    "    print(f\"\\nVital sign summaries:\")\n",
    "    for vital in clean_data['vital_category'].unique():\n",
    "        vital_subset = clean_data[clean_data['vital_category'] == vital]['vital_value']\n",
    "        print(f\"  {vital}: {len(vital_subset):,} values, mean={vital_subset.mean():.1f}, std={vital_subset.std():.1f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Workflow complete! Ready for analysis.\")\n",
    "    return clean_data\n",
    "\n",
    "# Execute the complete workflow\nanalysis_ready_data = complete_filtering_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive data filtering and querying techniques:\n",
    "\n",
    "### Key Filtering Methods:\n",
    "1. **Load-time filtering** - Most efficient for large datasets\n",
    "2. **Table method filtering** - Built-in methods for common operations\n",
    "3. **Complex multi-condition filtering** - Advanced pandas operations\n",
    "4. **Statistical filtering** - Outlier detection and removal\n",
    "5. **Memory-efficient chunking** - For very large datasets\n",
    "\n",
    "### Best Practices Applied:\n",
    "- Filter early and often to reduce memory usage\n",
    "- Layer filters from general to specific\n",
    "- Always validate filtering results\n",
    "- Document filtering decisions for reproducibility\n",
    "- Consider performance implications of different methods\n",
    "\n",
    "### Your Optimized Setup:\n",
    "- **Data format**: Parquet (efficient for filtering)\n",
    "- **Timezone**: US/Eastern (applied during loading)\n",
    "- **Recommended approach**: Use `load_data()` with filters parameter\n",
    "\n",
    "### Next Steps:\n",
    "- Apply these techniques to your specific research questions\n",
    "- Create reusable filter functions for common analyses\n",
    "- Combine filtering with other analysis techniques from previous notebooks\n",
    "\n",
    "### Explore Other Notebooks:\n",
    "- `01_basic_usage.ipynb` - Basic pyCLIF usage\n",
    "- `02_individual_tables.ipynb` - Individual table classes\n",
    "- `03_data_validation.ipynb` - Data validation techniques\n",
    "- `04_vitals_analysis.ipynb` - Advanced vitals analysis\n",
    "- `05_timezone_handling.ipynb` - Timezone conversion and management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}