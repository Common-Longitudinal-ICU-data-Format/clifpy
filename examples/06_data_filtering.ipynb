{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Filtering and Querying\n",
    "\n",
    "This notebook demonstrates advanced data filtering, querying, and subsetting techniques using pyCLIF for efficient analysis of large healthcare datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Healthcare datasets are often large and complex. Effective filtering enables:\n",
    "- Memory-efficient data loading\n",
    "- Focused analysis on specific patient populations\n",
    "- Time-based subsetting for longitudinal studies\n",
    "- Custom cohort creation for research questions\n",
    "- Performance optimization for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data filtering environment setup complete!\n",
      "Python version: 3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]\n",
      "Pandas version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pyCLIF components\n",
    "from pyclif import CLIF\n",
    "from pyclif.tables.vitals import vitals\n",
    "from pyclif.tables.patient import patient\n",
    "from pyclif.tables.hospitalization import hospitalization\n",
    "from pyclif.utils.io import load_data\n",
    "\n",
    "print(f\"Data filtering environment setup complete!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../src/pyclif/data/clif_demo/\n"
     ]
    }
   ],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = \"../src/pyclif/data/clif_demo/\"\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Filtering During Data Loading\n",
    "\n",
    "The most efficient way to filter data is during the loading process, which reduces memory usage and improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "recorded_dttm: null count before conversion= 0\n",
      "recorded_dttm: Your timezone is UTC, Converting to your site timezone (US/Eastern).\n",
      "recorded_dttm: null count after conversion= 0\n",
      "=== COLUMN SELECTION FILTERING ===\n",
      "Selected columns: ['hospitalization_id', 'vital_category', 'vital_value', 'recorded_dttm']\n",
      "Loaded data shape: (89085, 4)\n",
      "Actual columns: ['hospitalization_id', 'vital_category', 'vital_value', 'recorded_dttm']\n",
      "Memory usage: 12.27 MB\n"
     ]
    }
   ],
   "source": [
    "# Load only specific columns for memory efficiency\n",
    "essential_vitals_columns = [\n",
    "    'hospitalization_id', \n",
    "    'vital_category', \n",
    "    'vital_value', \n",
    "    'recorded_dttm'\n",
    "]\n",
    "\n",
    "vitals_subset = load_data(\n",
    "    table_name=\"vitals\",\n",
    "    table_path=DATA_DIR,\n",
    "    table_format_type=\"parquet\",\n",
    "    columns=essential_vitals_columns,\n",
    "    site_tz=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(f\"=== COLUMN SELECTION FILTERING ===\")\n",
    "print(f\"Selected columns: {essential_vitals_columns}\")\n",
    "print(f\"Loaded data shape: {vitals_subset.shape}\")\n",
    "print(f\"Actual columns: {list(vitals_subset.columns)}\")\n",
    "print(f\"Memory usage: {vitals_subset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "recorded_dttm: null count before conversion= 0\n",
      "recorded_dttm: Your timezone is UTC, Converting to your site timezone (US/Eastern).\n",
      "recorded_dttm: null count after conversion= 0\n",
      "=== VALUE-BASED FILTERING ===\n",
      "Filter: cardiac vitals only\n",
      "Loaded data shape: (42620, 4)\n",
      "Unique vital categories: ['sbp' 'heart_rate' 'dbp']\n",
      "Category counts:\n",
      "vital_category\n",
      "sbp           14356\n",
      "dbp           14351\n",
      "heart_rate    13913\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter by specific vital categories during loading\n",
    "cardiac_vitals = load_data(\n",
    "    table_name=\"vitals\",\n",
    "    table_path=DATA_DIR,\n",
    "    table_format_type=\"parquet\",\n",
    "    columns=essential_vitals_columns,\n",
    "    filters={'vital_category': ['heart_rate', 'sbp', 'dbp']},  # Only cardiac vitals\n",
    "    site_tz=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(f\"=== VALUE-BASED FILTERING ===\")\n",
    "print(f\"Filter: cardiac vitals only\")\n",
    "print(f\"Loaded data shape: {cardiac_vitals.shape}\")\n",
    "if 'vital_category' in cardiac_vitals.columns:\n",
    "    print(f\"Unique vital categories: {cardiac_vitals['vital_category'].unique()}\")\n",
    "    print(f\"Category counts:\")\n",
    "    print(cardiac_vitals['vital_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "recorded_dttm: null count before conversion= 0\n",
      "recorded_dttm: Your timezone is UTC, Converting to your site timezone (US/Eastern).\n",
      "recorded_dttm: null count after conversion= 0\n",
      "=== MULTI-CRITERIA FILTERING ===\n",
      "Filter: respiratory vitals with sample limit\n",
      "Loaded data shape: (2000, 4)\n",
      "Unique vital categories: ['respiratory_rate']\n"
     ]
    }
   ],
   "source": [
    "# Filter by multiple criteria\n",
    "respiratory_vitals = load_data(\n",
    "    table_name=\"vitals\",\n",
    "    table_path=DATA_DIR,\n",
    "    table_format_type=\"parquet\",\n",
    "    columns=essential_vitals_columns,\n",
    "    filters={\n",
    "        'vital_category': ['respiratory_rate', 'oxygen_saturation'],  # Respiratory vitals\n",
    "    },\n",
    "    sample_size=2000,  # Limit sample size\n",
    "    site_tz=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(f\"=== MULTI-CRITERIA FILTERING ===\")\n",
    "print(f\"Filter: respiratory vitals with sample limit\")\n",
    "print(f\"Loaded data shape: {respiratory_vitals.shape}\")\n",
    "if 'vital_category' in respiratory_vitals.columns:\n",
    "    print(f\"Unique vital categories: {respiratory_vitals['vital_category'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Loading Filtering with Table Methods\n",
    "\n",
    "Use built-in table methods for filtering after data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "recorded_dttm: null count before conversion= 0\n",
      "recorded_dttm: Your timezone is UTC, Converting to your site timezone (UTC).\n",
      "recorded_dttm: null count after conversion= 0\n",
      "Validation completed with 5 error(s).\n",
      "  - 5 range validation error(s)\n",
      "See `errors` and `range_validation_errors` attributes for details.\n",
      "=== FULL VITALS TABLE ===\n",
      "Total records: 89,085\n",
      "Unique patients: 128\n",
      "Vital categories: 9\n",
      "Date range: 2110-04-11 20:52:00+00:00 to 2201-12-13 23:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load a full vitals table for demonstration\n",
    "vitals_table = vitals.from_file(DATA_DIR, \"parquet\")\n",
    "\n",
    "print(f\"=== FULL VITALS TABLE ===\")\n",
    "print(f\"Total records: {len(vitals_table.df):,}\")\n",
    "print(f\"Unique patients: {vitals_table.df['hospitalization_id'].nunique():,}\")\n",
    "print(f\"Vital categories: {len(vitals_table.get_vital_categories())}\")\n",
    "\n",
    "# Get date range\n",
    "if 'recorded_dttm' in vitals_table.df.columns:\n",
    "    date_min = vitals_table.df['recorded_dttm'].min()\n",
    "    date_max = vitals_table.df['recorded_dttm'].max()\n",
    "    print(f\"Date range: {date_min} to {date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Vital Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available vital categories: ['spo2', 'map', 'sbp', 'heart_rate', 'dbp', 'respiratory_rate', 'weight_kg', 'height_cm', 'temp_c']...\n",
      "\n",
      "=== HEART RATE FILTERING ===\n",
      "Heart rate records: 13,913\n",
      "Unique hospitalization with HR data: 128\n",
      "HR range: 0.0 - 200.0 bpm\n",
      "HR mean ± std: 91.1 ± 18.7 bpm\n"
     ]
    }
   ],
   "source": [
    "# Filter by specific vital category\n",
    "available_vitals = vitals_table.get_vital_categories()\n",
    "print(f\"Available vital categories: {available_vitals[:10]}...\")  # Show first 10\n",
    "\n",
    "# Focus on heart rate if available\n",
    "if 'heart_rate' in available_vitals:\n",
    "    hr_data = vitals_table.filter_by_vital_category('heart_rate')\n",
    "    \n",
    "    print(f\"\\n=== HEART RATE FILTERING ===\")\n",
    "    print(f\"Heart rate records: {len(hr_data):,}\")\n",
    "    print(f\"Unique hospitalization with HR data: {hr_data['hospitalization_id'].nunique():,}\")\n",
    "    \n",
    "    if 'vital_value' in hr_data.columns:\n",
    "        print(f\"HR range: {hr_data['vital_value'].min():.1f} - {hr_data['vital_value'].max():.1f} bpm\")\n",
    "        print(f\"HR mean ± std: {hr_data['vital_value'].mean():.1f} ± {hr_data['vital_value'].std():.1f} bpm\")\n",
    "else:\n",
    "    print(\"Heart rate data not available, using first available vital category\")\n",
    "    if available_vitals:\n",
    "        first_vital = available_vitals[0]\n",
    "        vital_data = vitals_table.filter_by_vital_category(first_vital)\n",
    "        print(f\"\\n=== {first_vital.upper()} FILTERING ===\")\n",
    "        print(f\"{first_vital} records: {len(vital_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HOSPITALIZATION FILTERING ===\n",
      "Total hospitalizations: 128\n",
      "Top 5 hospitalizations by measurement count:\n",
      "  1. 28258130: 4,377 measurements\n",
      "  2. 22987108: 3,439 measurements\n",
      "  3. 23831430: 3,202 measurements\n",
      "  4. 23559586: 2,934 measurements\n",
      "  5. 28661809: 2,757 measurements\n",
      "\n",
      "Analysis of hospitalization 28258130:\n",
      "  Total measurements: 4,377\n",
      "  Vital categories: 9\n",
      "  Duration: 16 days, 4 hours\n",
      "  Vital breakdown:\n",
      "    map: 751\n",
      "    dbp: 749\n",
      "    sbp: 747\n",
      "    heart_rate: 652\n",
      "    respiratory_rate: 648\n"
     ]
    }
   ],
   "source": [
    "# Get top hospitalizations by measurement count\n",
    "hosp_counts = vitals_table.df['hospitalization_id'].value_counts()\n",
    "print(f\"=== HOSPITALIZATION FILTERING ===\")\n",
    "print(f\"Total hospitalizations: {len(hosp_counts):,}\")\n",
    "print(f\"Top 5 hospitalizations by measurement count:\")\n",
    "\n",
    "for i, (hosp_id, count) in enumerate(hosp_counts.head(5).items()):\n",
    "    print(f\"  {i+1}. {hosp_id}: {count:,} measurements\")\n",
    "\n",
    "# Filter by specific hospitalization\n",
    "if len(hosp_counts) > 0:\n",
    "    top_hosp_id = hosp_counts.index[0]\n",
    "    hosp_data = vitals_table.filter_by_hospitalization(top_hosp_id)\n",
    "    \n",
    "    print(f\"\\nAnalysis of hospitalization {top_hosp_id}:\")\n",
    "    print(f\"  Total measurements: {len(hosp_data):,}\")\n",
    "    print(f\"  Vital categories: {hosp_data['vital_category'].nunique()}\")\n",
    "    \n",
    "    if 'recorded_dttm' in hosp_data.columns:\n",
    "        duration = hosp_data['recorded_dttm'].max() - hosp_data['recorded_dttm'].min()\n",
    "        print(f\"  Duration: {duration.days} days, {duration.seconds//3600} hours\")\n",
    "    \n",
    "    # Show vital category breakdown\n",
    "    print(f\"  Vital breakdown:\")\n",
    "    vital_breakdown = hosp_data['vital_category'].value_counts().head(5)\n",
    "    for vital, count in vital_breakdown.items():\n",
    "        print(f\"    {vital}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATE RANGE FILTERING ===\n",
      "Full dataset range: 2110-04-11 20:52:00+00:00 to 2201-12-13 23:00:00+00:00\n",
      "Total span: 33483 days\n",
      "\n",
      "Recent data (last 30 days):\n",
      "  Date range: 2201-11-13 23:00:00+00:00 to 2201-12-13 23:00:00+00:00\n",
      "  Records: 324\n",
      "  Patients: 1\n",
      "\n",
      "Middle period data:\n",
      "  Date range: 2140-10-31 20:52:00+00:00 to 2171-05-23 20:52:00+00:00\n",
      "  Records: 28,329\n",
      "  Patients: 43\n"
     ]
    }
   ],
   "source": [
    "# Filter by date range\n",
    "if 'recorded_dttm' in vitals_table.df.columns:\n",
    "    # Get overall date range\n",
    "    full_date_range = {\n",
    "        'start': vitals_table.df['recorded_dttm'].min(),\n",
    "        'end': vitals_table.df['recorded_dttm'].max()\n",
    "    }\n",
    "    \n",
    "    print(f\"=== DATE RANGE FILTERING ===\")\n",
    "    print(f\"Full dataset range: {full_date_range['start']} to {full_date_range['end']}\")\n",
    "    total_days = (full_date_range['end'] - full_date_range['start']).days\n",
    "    print(f\"Total span: {total_days} days\")\n",
    "    \n",
    "    # Filter to recent data (last 30 days of available data)\n",
    "    recent_start = full_date_range['end'] - timedelta(days=30)\n",
    "    recent_data = vitals_table.filter_by_date_range(recent_start, full_date_range['end'])\n",
    "    \n",
    "    print(f\"\\nRecent data (last 30 days):\")\n",
    "    print(f\"  Date range: {recent_start} to {full_date_range['end']}\")\n",
    "    print(f\"  Records: {len(recent_data):,}\")\n",
    "    print(f\"  Patients: {recent_data['hospitalization_id'].nunique():,}\")\n",
    "    \n",
    "    # Filter to middle period\n",
    "    mid_start = full_date_range['start'] + timedelta(days=total_days//3)\n",
    "    mid_end = full_date_range['start'] + timedelta(days=2*total_days//3)\n",
    "    mid_data = vitals_table.filter_by_date_range(mid_start, mid_end)\n",
    "    \n",
    "    print(f\"\\nMiddle period data:\")\n",
    "    print(f\"  Date range: {mid_start} to {mid_end}\")\n",
    "    print(f\"  Records: {len(mid_data):,}\")\n",
    "    print(f\"  Patients: {mid_data['hospitalization_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"No datetime column available for date filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Filtering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Multi-Condition Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLEX FILTERING ===\n",
      "Starting records: 89,085\n",
      "  1. Heart rate data only:\n",
      "     Removed: 75,172 records\n",
      "     Remaining: 13,913 records\n",
      "  2. Valid heart rate range (30-200 bpm):\n",
      "     Removed: 5 records\n",
      "     Remaining: 13,908 records\n",
      "  3. Recent data (last 90 days):\n",
      "     Removed: 13,535 records\n",
      "     Remaining: 373 records\n",
      "\n",
      "Final dataset: 373 records\n",
      "\n",
      "=== FILTERED RESULTS ANALYSIS ===\n",
      "Unique hospitalizations: 2\n",
      "HR statistics:\n",
      "  Mean: 96.1 bpm\n",
      "  Std: 11.0 bpm\n",
      "  Range: 71.0 - 138.0 bpm\n"
     ]
    }
   ],
   "source": [
    "# Create complex filters using pandas operations\n",
    "def apply_complex_filters(df, conditions):\n",
    "    \"\"\"Apply multiple filtering conditions to a DataFrame.\"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    print(f\"=== COMPLEX FILTERING ===\")\n",
    "    print(f\"Starting records: {len(filtered_df):,}\")\n",
    "    \n",
    "    for i, (description, condition) in enumerate(conditions):\n",
    "        before_count = len(filtered_df)\n",
    "        filtered_df = filtered_df[condition(filtered_df)]\n",
    "        after_count = len(filtered_df)\n",
    "        removed = before_count - after_count\n",
    "        \n",
    "        print(f\"  {i+1}. {description}:\")\n",
    "        print(f\"     Removed: {removed:,} records\")\n",
    "        print(f\"     Remaining: {after_count:,} records\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(filtered_df):,} records\")\n",
    "    return filtered_df\n",
    "\n",
    "# Define complex filtering conditions\n",
    "if 'heart_rate' in available_vitals:\n",
    "    complex_conditions = [\n",
    "        (\"Heart rate data only\", \n",
    "         lambda df: df['vital_category'] == 'heart_rate'),\n",
    "        (\"Valid heart rate range (30-200 bpm)\", \n",
    "         lambda df: (df['vital_value'] >= 30) & (df['vital_value'] <= 200)),\n",
    "        (\"Recent data (last 90 days)\", \n",
    "         lambda df: df['recorded_dttm'] >= (df['recorded_dttm'].max() - timedelta(days=90)))\n",
    "    ]\n",
    "    \n",
    "    filtered_hr_data = apply_complex_filters(vitals_table.df, complex_conditions)\n",
    "    \n",
    "    # Analyze filtered results\n",
    "    print(f\"\\n=== FILTERED RESULTS ANALYSIS ===\")\n",
    "    print(f\"Unique hospitalizations: {filtered_hr_data['hospitalization_id'].nunique():,}\")\n",
    "    print(f\"HR statistics:\")\n",
    "    print(f\"  Mean: {filtered_hr_data['vital_value'].mean():.1f} bpm\")\n",
    "    print(f\"  Std: {filtered_hr_data['vital_value'].std():.1f} bpm\")\n",
    "    print(f\"  Range: {filtered_hr_data['vital_value'].min():.1f} - {filtered_hr_data['vital_value'].max():.1f} bpm\")\n",
    "else:\n",
    "    print(\"Heart rate data not available for complex filtering demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Filtering and Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STATISTICAL FILTERING: SPO2 ===\n",
      "Original records: 13,540\n",
      "Method: IQR\n",
      "IQR bounds: 89.0 - 105.0\n",
      "Removed outliers: 111 (0.8%)\n",
      "Remaining records: 13,429\n",
      "\n",
      "Before filtering:\n",
      "  Mean ± Std: 96.8 ± 3.0\n",
      "  Range: 29.0 - 100.0\n",
      "After filtering:\n",
      "  Mean ± Std: 96.9 ± 2.5\n",
      "  Range: 89.0 - 100.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== STATISTICAL FILTERING: SPO2 ===\n",
      "Original records: 13,540\n",
      "Method: ZSCORE\n",
      "Z-score bounds: 92.4 - 101.2\n",
      "Removed outliers: 791 (5.8%)\n",
      "Remaining records: 12,749\n",
      "\n",
      "Before filtering:\n",
      "  Mean ± Std: 96.8 ± 3.0\n",
      "  Range: 29.0 - 100.0\n",
      "After filtering:\n",
      "  Mean ± Std: 97.2 ± 2.1\n",
      "  Range: 93.0 - 100.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== STATISTICAL FILTERING: SPO2 ===\n",
      "Original records: 13,540\n",
      "Method: PERCENTILE\n",
      "Percentile bounds: 91.0 - 100.0\n",
      "Removed outliers: 281 (2.1%)\n",
      "Remaining records: 13,259\n",
      "\n",
      "Before filtering:\n",
      "  Mean ± Std: 96.8 ± 3.0\n",
      "  Range: 29.0 - 100.0\n",
      "After filtering:\n",
      "  Mean ± Std: 97.0 ± 2.3\n",
      "  Range: 91.0 - 100.0\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistical filtering to remove outliers\n",
    "def statistical_filtering(df, vital_category, method='iqr', factor=1.5):\n",
    "    \"\"\"Apply statistical filtering to remove outliers.\"\"\"\n",
    "    \n",
    "    vital_data = df[df['vital_category'] == vital_category].copy()\n",
    "    \n",
    "    if len(vital_data) == 0:\n",
    "        print(f\"No data available for {vital_category}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    original_count = len(vital_data)\n",
    "    \n",
    "    print(f\"=== STATISTICAL FILTERING: {vital_category.upper()} ===\")\n",
    "    print(f\"Original records: {original_count:,}\")\n",
    "    print(f\"Method: {method.upper()}\")\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        # Interquartile Range method\n",
    "        Q1 = vital_data['vital_value'].quantile(0.25)\n",
    "        Q3 = vital_data['vital_value'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        print(f\"IQR bounds: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        # Z-score method\n",
    "        mean_val = vital_data['vital_value'].mean()\n",
    "        std_val = vital_data['vital_value'].std()\n",
    "        \n",
    "        lower_bound = mean_val - factor * std_val\n",
    "        upper_bound = mean_val + factor * std_val\n",
    "        \n",
    "        print(f\"Z-score bounds: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
    "    \n",
    "    elif method == 'percentile':\n",
    "        # Percentile method\n",
    "        lower_percentile = (100 - 95) / 2  # 2.5th percentile\n",
    "        upper_percentile = 100 - lower_percentile  # 97.5th percentile\n",
    "        \n",
    "        lower_bound = vital_data['vital_value'].quantile(lower_percentile / 100)\n",
    "        upper_bound = vital_data['vital_value'].quantile(upper_percentile / 100)\n",
    "        \n",
    "        print(f\"Percentile bounds: {lower_bound:.1f} - {upper_bound:.1f}\")\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_data = vital_data[\n",
    "        (vital_data['vital_value'] >= lower_bound) & \n",
    "        (vital_data['vital_value'] <= upper_bound)\n",
    "    ]\n",
    "    \n",
    "    filtered_count = len(filtered_data)\n",
    "    removed_count = original_count - filtered_count\n",
    "    \n",
    "    print(f\"Removed outliers: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
    "    print(f\"Remaining records: {filtered_count:,}\")\n",
    "    \n",
    "    # Statistics comparison\n",
    "    print(f\"\\nBefore filtering:\")\n",
    "    print(f\"  Mean ± Std: {vital_data['vital_value'].mean():.1f} ± {vital_data['vital_value'].std():.1f}\")\n",
    "    print(f\"  Range: {vital_data['vital_value'].min():.1f} - {vital_data['vital_value'].max():.1f}\")\n",
    "    \n",
    "    print(f\"After filtering:\")\n",
    "    print(f\"  Mean ± Std: {filtered_data['vital_value'].mean():.1f} ± {filtered_data['vital_value'].std():.1f}\")\n",
    "    print(f\"  Range: {filtered_data['vital_value'].min():.1f} - {filtered_data['vital_value'].max():.1f}\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Apply statistical filtering to available vital\n",
    "if available_vitals:\n",
    "    test_vital = available_vitals[0]  # Use first available vital\n",
    "    \n",
    "    # Test different methods\n",
    "    for method in ['iqr', 'zscore', 'percentile']:\n",
    "        filtered_result = statistical_filtering(vitals_table.df, test_vital, method=method)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "else:\n",
    "    print(\"No vital categories available for statistical filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance-Optimized Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILTERING PERFORMANCE BENCHMARK ===\n",
      "Sample size: 10,000 records\n",
      "\n",
      "Performance comparison:\n",
      "  pandas_isin       :   1.30 ms (3,183 records)\n",
      "  pandas_query      :   1.39 ms (3,183 records)\n",
      "  boolean_indexing  :   0.95 ms (3,183 records)\n",
      "\n",
      "🏆 Fastest method: boolean_indexing\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate performance considerations\n",
    "import time\n",
    "\n",
    "def benchmark_filtering_methods(df, sample_size=10000):\n",
    "    \"\"\"Benchmark different filtering approaches for performance.\"\"\"\n",
    "    \n",
    "    # Use a sample for benchmarking\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    print(f\"=== FILTERING PERFORMANCE BENCHMARK ===\")\n",
    "    print(f\"Sample size: {len(df_sample):,} records\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Method 1: Basic pandas filtering\n",
    "    start_time = time.time()\n",
    "    if 'vital_category' in df_sample.columns:\n",
    "        method1_result = df_sample[df_sample['vital_category'].isin(['heart_rate', 'sbp'])]\n",
    "    else:\n",
    "        method1_result = df_sample.head(100)  # Fallback\n",
    "    method1_time = time.time() - start_time\n",
    "    results['pandas_isin'] = {'time': method1_time, 'records': len(method1_result)}\n",
    "    \n",
    "    # Method 2: Query method\n",
    "    start_time = time.time()\n",
    "    if 'vital_category' in df_sample.columns:\n",
    "        method2_result = df_sample.query(\"vital_category in ['heart_rate', 'sbp']\")\n",
    "    else:\n",
    "        method2_result = df_sample.head(100)  # Fallback\n",
    "    method2_time = time.time() - start_time\n",
    "    results['pandas_query'] = {'time': method2_time, 'records': len(method2_result)}\n",
    "    \n",
    "    # Method 3: Boolean indexing\n",
    "    start_time = time.time()\n",
    "    if 'vital_category' in df_sample.columns:\n",
    "        mask = (df_sample['vital_category'] == 'heart_rate') | (df_sample['vital_category'] == 'sbp')\n",
    "        method3_result = df_sample[mask]\n",
    "    else:\n",
    "        method3_result = df_sample.head(100)  # Fallback\n",
    "    method3_time = time.time() - start_time\n",
    "    results['boolean_indexing'] = {'time': method3_time, 'records': len(method3_result)}\n",
    "    \n",
    "    # Report results\n",
    "    print(\"Performance comparison:\")\n",
    "    for method, result in results.items():\n",
    "        print(f\"  {method:<18}: {result['time']*1000:>6.2f} ms ({result['records']:,} records)\")\n",
    "    \n",
    "    # Determine fastest method\n",
    "    fastest_method = min(results.keys(), key=lambda x: results[x]['time'])\n",
    "    print(f\"\\n🏆 Fastest method: {fastest_method}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance benchmark\n",
    "if len(vitals_table.df) > 0:\n",
    "    perf_results = benchmark_filtering_methods(vitals_table.df)\n",
    "else:\n",
    "    print(\"Insufficient data for performance benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Filtering Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMORY-EFFICIENT FILTERING ===\n",
      "Strategy: Load data in chunks of 500 records\n",
      "\n",
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "Chunk 1:\n",
      "  Records: 500\n",
      "  Patients: 5\n",
      "  Memory: 0.07 MB\n",
      "  Vital categories: 1\n",
      "  ✅ Chunk processed and cleared from memory\n",
      "\n",
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "Chunk 2:\n",
      "  Records: 500\n",
      "  Patients: 1\n",
      "  Memory: 0.07 MB\n",
      "  Vital categories: 7\n",
      "  ✅ Chunk processed and cleared from memory\n",
      "\n",
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "Chunk 3:\n",
      "  Records: 500\n",
      "  Patients: 1\n",
      "  Memory: 0.07 MB\n",
      "  Vital categories: 7\n",
      "  ✅ Chunk processed and cleared from memory\n",
      "\n",
      "CHUNK PROCESSING SUMMARY:\n",
      "  Total chunks: 3\n",
      "  Total records processed: 1,500\n",
      "  Peak memory usage: 0.07 MB\n",
      "  💡 Memory efficient: Process large datasets without loading all data at once\n"
     ]
    }
   ],
   "source": [
    "# Memory-efficient filtering for large datasets\n",
    "def memory_efficient_analysis(data_dir, chunk_size=1000):\n",
    "    \"\"\"Demonstrate memory-efficient processing of large datasets.\"\"\"\n",
    "    \n",
    "    print(f\"=== MEMORY-EFFICIENT FILTERING ===\")\n",
    "    print(f\"Strategy: Load data in chunks of {chunk_size:,} records\")\n",
    "    print()\n",
    "    \n",
    "    # Load data in chunks with filters\n",
    "    chunk_results = []\n",
    "    \n",
    "    try:\n",
    "        # Load small chunks with specific filters\n",
    "        for i in range(3):  # Process 3 chunks as demonstration\n",
    "            chunk_data = load_data(\n",
    "                table_name=\"vitals\",\n",
    "                table_path=data_dir,\n",
    "                table_format_type=\"parquet\",\n",
    "                columns=['hospitalization_id', 'vital_category', 'vital_value'],\n",
    "                sample_size=chunk_size,\n",
    "                filters={'vital_category': ['heart_rate']} if i == 0 else None\n",
    "            )\n",
    "            \n",
    "            if not chunk_data.empty:\n",
    "                # Process chunk\n",
    "                chunk_summary = {\n",
    "                    'chunk_id': i,\n",
    "                    'records': len(chunk_data),\n",
    "                    'patients': chunk_data['hospitalization_id'].nunique() if 'hospitalization_id' in chunk_data.columns else 0,\n",
    "                    'memory_mb': chunk_data.memory_usage(deep=True).sum() / 1024**2\n",
    "                }\n",
    "                \n",
    "                if 'vital_category' in chunk_data.columns:\n",
    "                    chunk_summary['vital_categories'] = chunk_data['vital_category'].nunique()\n",
    "                \n",
    "                chunk_results.append(chunk_summary)\n",
    "                \n",
    "                print(f\"Chunk {i+1}:\")\n",
    "                print(f\"  Records: {chunk_summary['records']:,}\")\n",
    "                print(f\"  Patients: {chunk_summary['patients']:,}\")\n",
    "                print(f\"  Memory: {chunk_summary['memory_mb']:.2f} MB\")\n",
    "                if 'vital_categories' in chunk_summary:\n",
    "                    print(f\"  Vital categories: {chunk_summary['vital_categories']}\")\n",
    "                \n",
    "                # Clear chunk from memory\n",
    "                del chunk_data\n",
    "                print(f\"  ✅ Chunk processed and cleared from memory\")\n",
    "                print()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunk processing: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    if chunk_results:\n",
    "        total_records = sum(chunk['records'] for chunk in chunk_results)\n",
    "        total_memory = sum(chunk['memory_mb'] for chunk in chunk_results)\n",
    "        \n",
    "        print(f\"CHUNK PROCESSING SUMMARY:\")\n",
    "        print(f\"  Total chunks: {len(chunk_results)}\")\n",
    "        print(f\"  Total records processed: {total_records:,}\")\n",
    "        print(f\"  Peak memory usage: {max(chunk['memory_mb'] for chunk in chunk_results):.2f} MB\")\n",
    "        print(f\"  💡 Memory efficient: Process large datasets without loading all data at once\")\n",
    "    \n",
    "    return chunk_results\n",
    "\n",
    "# Demonstrate memory-efficient processing\n",
    "memory_analysis = memory_efficient_analysis(DATA_DIR, chunk_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "          FILTERING BEST PRACTICES SUMMARY\n",
      "============================================================\n",
      "\n",
      "🚀 PERFORMANCE OPTIMIZATION:\n",
      "  1. Filter during data loading when possible\n",
      "     • Use 'filters' parameter in load_data()\n",
      "     • Select only needed columns\n",
      "     • Use sample_size for testing\n",
      "\n",
      "💾 MEMORY MANAGEMENT:\n",
      "  2. Process large datasets in chunks\n",
      "     • Load, process, and clear chunks iteratively\n",
      "     • Use generators for streaming processing\n",
      "     • Monitor memory usage during processing\n",
      "\n",
      "🎯 FILTERING STRATEGIES:\n",
      "  3. Layer your filters from general to specific\n",
      "     • Start with broad categorical filters\n",
      "     • Apply temporal filters\n",
      "     • Finish with value-based filters\n",
      "\n",
      "📊 DATA QUALITY:\n",
      "  4. Always validate filtered results\n",
      "     • Check record counts make sense\n",
      "     • Verify patient/hospitalization counts\n",
      "     • Review statistical summaries\n",
      "\n",
      "🔧 IMPLEMENTATION TIPS:\n",
      "  5. Use appropriate pandas methods\n",
      "     • .isin() for multiple values\n",
      "     • .query() for complex conditions\n",
      "     • Boolean indexing for simple conditions\n",
      "\n",
      "⚠️  COMMON PITFALLS TO AVOID:\n",
      "  • Loading entire dataset before filtering\n",
      "  • Using loops instead of vectorized operations\n",
      "  • Not considering timezone effects in date filters\n",
      "  • Forgetting to validate filter results\n",
      "  • Not documenting filter criteria for reproducibility\n",
      "\n",
      "🎯 YOUR RECOMMENDED WORKFLOW:\n",
      "  1. Define your research question and required data\n",
      "  2. Identify optimal filters for data loading\n",
      "  3. Load data with initial filters applied\n",
      "  4. Apply additional post-loading filters as needed\n",
      "  5. Validate and document your filtering decisions\n",
      "  6. Create reusable filter functions for consistency\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive filtering best practices\n",
    "def filtering_best_practices_summary():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"          FILTERING BEST PRACTICES SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    print(\"🚀 PERFORMANCE OPTIMIZATION:\")\n",
    "    print(\"  1. Filter during data loading when possible\")\n",
    "    print(\"     • Use 'filters' parameter in load_data()\")\n",
    "    print(\"     • Select only needed columns\")\n",
    "    print(\"     • Use sample_size for testing\")\n",
    "    print()\n",
    "    \n",
    "    print(\"💾 MEMORY MANAGEMENT:\")\n",
    "    print(\"  2. Process large datasets in chunks\")\n",
    "    print(\"     • Load, process, and clear chunks iteratively\")\n",
    "    print(\"     • Use generators for streaming processing\")\n",
    "    print(\"     • Monitor memory usage during processing\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎯 FILTERING STRATEGIES:\")\n",
    "    print(\"  3. Layer your filters from general to specific\")\n",
    "    print(\"     • Start with broad categorical filters\")\n",
    "    print(\"     • Apply temporal filters\")\n",
    "    print(\"     • Finish with value-based filters\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📊 DATA QUALITY:\")\n",
    "    print(\"  4. Always validate filtered results\")\n",
    "    print(\"     • Check record counts make sense\")\n",
    "    print(\"     • Verify patient/hospitalization counts\")\n",
    "    print(\"     • Review statistical summaries\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔧 IMPLEMENTATION TIPS:\")\n",
    "    print(\"  5. Use appropriate pandas methods\")\n",
    "    print(\"     • .isin() for multiple values\")\n",
    "    print(\"     • .query() for complex conditions\")\n",
    "    print(\"     • Boolean indexing for simple conditions\")\n",
    "    print()\n",
    "    \n",
    "    print(\"⚠️  COMMON PITFALLS TO AVOID:\")\n",
    "    print(\"  • Loading entire dataset before filtering\")\n",
    "    print(\"  • Using loops instead of vectorized operations\")\n",
    "    print(\"  • Not considering timezone effects in date filters\")\n",
    "    print(\"  • Forgetting to validate filter results\")\n",
    "    print(\"  • Not documenting filter criteria for reproducibility\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎯 YOUR RECOMMENDED WORKFLOW:\")\n",
    "    print(\"  1. Define your research question and required data\")\n",
    "    print(\"  2. Identify optimal filters for data loading\")\n",
    "    print(\"  3. Load data with initial filters applied\")\n",
    "    print(\"  4. Apply additional post-loading filters as needed\")\n",
    "    print(\"  5. Validate and document your filtering decisions\")\n",
    "    print(\"  6. Create reusable filter functions for consistency\")\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "filtering_best_practices_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive data filtering and querying techniques:\n",
    "\n",
    "### Key Filtering Methods:\n",
    "1. **Load-time filtering** - Most efficient for large datasets\n",
    "2. **Table method filtering** - Built-in methods for common operations\n",
    "3. **Complex multi-condition filtering** - Advanced pandas operations\n",
    "4. **Statistical filtering** - Outlier detection and removal\n",
    "5. **Memory-efficient chunking** - For very large datasets\n",
    "\n",
    "### Best Practices Applied:\n",
    "- Filter early and often to reduce memory usage\n",
    "- Layer filters from general to specific\n",
    "- Always validate filtering results\n",
    "- Document filtering decisions for reproducibility\n",
    "- Consider performance implications of different methods\n",
    "\n",
    "### Your Optimized Setup:\n",
    "- **Data format**: Parquet (efficient for filtering)\n",
    "- **Timezone**: US/Eastern (applied during loading)\n",
    "- **Recommended approach**: Use `load_data()` with filters parameter\n",
    "\n",
    "### Next Steps:\n",
    "- Apply these techniques to your specific research questions\n",
    "- Create reusable filter functions for common analyses\n",
    "- Combine filtering with other analysis techniques from previous notebooks\n",
    "\n",
    "### Explore Other Notebooks:\n",
    "- `01_basic_usage.ipynb` - Basic pyCLIF usage\n",
    "- `02_individual_tables.ipynb` - Individual table classes\n",
    "- `03_data_validation.ipynb` - Data validation techniques\n",
    "- `04_vitals_analysis.ipynb` - Advanced vitals analysis\n",
    "- `05_timezone_handling.ipynb` - Timezone conversion and management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyclif_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
