{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Wide Dataset Creation with pyCLIF\n",
    "\n",
    "This notebook demonstrates how to create hourly aggregated wide datasets using pyCLIF. The hourly aggregation converts wide datasets into regular hourly buckets with user-defined aggregation methods.\n",
    "\n",
    "## Key Features\n",
    "- **Regular time intervals**: Convert irregular events to hourly buckets\n",
    "- **Multiple aggregation methods**: max, min, mean, median, first, last, boolean, one_hot_encode\n",
    "- **Suffixed column names**: All aggregated columns get clear suffixes (e.g., `map_max`, `heart_rate_mean`)\n",
    "- **Time tracking**: \n",
    "  - `nth_hour` starts from 0 for first event, increments with each hour change\n",
    "  - `event_time_hour` provides clean hour-truncated timestamps (e.g., 2023-01-01 13:00:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "print(\"=== pyCLIF Hourly Wide Dataset Example ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "from pyclif.utils.wide_dataset import convert_wide_to_hourly\n",
    "\n",
    "# Initialize CLIF with your data directory\n",
    "data_dir = \"/Users/vaishvik/Downloads/CLIF_MIMIC\"  # Update this path\n",
    "clif = CLIF(data_dir=data_dir, filetype='parquet', timezone=\"US/Eastern\")\n",
    "\n",
    "print(\"âœ… CLIF initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Wide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wide dataset with essential tables and categories\n",
    "wide_df = clif.create_wide_dataset(\n",
    "    optional_tables=['vitals', 'labs', 'medication_admin_continuous'],\n",
    "    category_filters={\n",
    "        'vitals': ['map', 'heart_rate', 'spo2', 'temp_c'],\n",
    "        'labs': ['hemoglobin', 'sodium', 'creatinine'], \n",
    "        'medication_admin_continuous': ['norepinephrine', 'propofol']\n",
    "    },\n",
    "    sample=True  # Use small sample for demo\n",
    ")\n",
    "\n",
    "print(f\"Wide dataset created: {len(wide_df)} rows, {len(wide_df.columns)} columns\")\n",
    "print(f\"Hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n",
    "print(f\"Time range: {wide_df['event_time'].min()} to {wide_df['event_time'].max()}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "cols_to_show = ['hospitalization_id', 'event_time', 'day_number', 'map', 'heart_rate', 'norepinephrine']\n",
    "available_cols = [col for col in cols_to_show if col in wide_df.columns]\n",
    "print(wide_df[available_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how each column should be aggregated into hourly buckets\n",
    "# Note: All columns will get suffixes based on aggregation method\n",
    "aggregation_config = {\n",
    "    'max': ['map', 'temp_c'],              # â†’ map_max, temp_c_max\n",
    "    'mean': ['heart_rate', 'spo2'],        # â†’ heart_rate_mean, spo2_mean  \n",
    "    'min': ['spo2'],                       # â†’ spo2_min (in addition to spo2_mean)\n",
    "    'first': ['hemoglobin', 'sodium'],     # â†’ hemoglobin_first, sodium_first\n",
    "    'boolean': ['norepinephrine', 'propofol']  # â†’ norepinephrine_boolean, propofol_boolean\n",
    "}\n",
    "\n",
    "print(\"Aggregation configuration:\")\n",
    "for method, columns in aggregation_config.items():\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    for col in columns:\n",
    "        print(f\"  {col} â†’ {col}_{method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wide dataset to hourly aggregation\n",
    "hourly_df = convert_wide_to_hourly(wide_df, aggregation_config)\n",
    "\n",
    "print(f\"Hourly dataset created: {len(hourly_df)} rows, {len(hourly_df.columns)} columns\")\n",
    "print(f\"Reduction: {len(wide_df)} â†’ {len(hourly_df)} rows ({(1-len(hourly_df)/len(wide_df))*100:.1f}% reduction)\")\n",
    "print(f\"Max nth_hour: {hourly_df['nth_hour'].max()} (â‰ˆ {hourly_df['nth_hour'].max()/24:.1f} days)\")\n",
    "\n",
    "# Show sample hourly data\n",
    "print(\"\\nSample hourly data:\")\n",
    "sample_cols = ['hospitalization_id', 'nth_hour', 'day_number', 'hour_bucket', \n",
    "               'map_max', 'heart_rate_mean', 'spo2_mean', 'spo2_min', 'norepinephrine_boolean']\n",
    "available_cols = [col for col in sample_cols if col in hourly_df.columns]\n",
    "print(hourly_df[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wide dataset to hourly aggregation\n",
    "hourly_df = convert_wide_to_hourly(wide_df, aggregation_config)\n",
    "\n",
    "print(f\"Hourly dataset created: {len(hourly_df)} rows, {len(hourly_df.columns)} columns\")\n",
    "print(f\"Reduction: {len(wide_df)} â†’ {len(hourly_df)} rows ({(1-len(hourly_df)/len(wide_df))*100:.1f}% reduction)\")\n",
    "print(f\"Max nth_hour: {hourly_df['nth_hour'].max()} (â‰ˆ {hourly_df['nth_hour'].max():.1f} hours from first event)\")\n",
    "\n",
    "# Show sample hourly data with new columns\n",
    "print(\"\\nSample hourly data:\")\n",
    "sample_cols = ['hospitalization_id', 'nth_hour', 'event_time_hour', 'hour_bucket', \n",
    "               'map_max', 'heart_rate_mean', 'spo2_mean', 'spo2_min', 'norepinephrine_boolean']\n",
    "available_cols = [col for col in sample_cols if col in hourly_df.columns]\n",
    "print(hourly_df[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show one patient's first 24 hours\n",
    "first_hosp = hourly_df['hospitalization_id'].iloc[0]\n",
    "patient_data = hourly_df[\n",
    "    (hourly_df['hospitalization_id'] == first_hosp) & \n",
    "    (hourly_df['nth_hour'] <= 24)\n",
    "]\n",
    "\n",
    "print(f\"First 24 hours for patient {first_hosp}:\")\n",
    "trend_cols = ['nth_hour', 'event_time_hour', 'map_max', 'heart_rate_mean', 'spo2_mean', 'norepinephrine_boolean']\n",
    "available_trend_cols = [col for col in trend_cols if col in patient_data.columns]\n",
    "print(patient_data[available_trend_cols])\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"â€¢ Total patients: {hourly_df['hospitalization_id'].nunique()}\")\n",
    "print(f\"â€¢ Total hourly records: {len(hourly_df)}\")\n",
    "print(f\"â€¢ Average records per patient: {len(hourly_df) / hourly_df['hospitalization_id'].nunique():.1f}\")\n",
    "\n",
    "# Show nth_hour progression\n",
    "print(f\"\\nâ° nth_hour progression:\")\n",
    "print(f\"â€¢ Min nth_hour: {hourly_df['nth_hour'].min()} (first event)\")\n",
    "print(f\"â€¢ Max nth_hour: {hourly_df['nth_hour'].max()} (hours elapsed)\")\n",
    "print(f\"â€¢ nth_hour starts at 0 for first event, increments with each hour change\")\n",
    "\n",
    "# Check medication usage\n",
    "if 'norepinephrine_boolean' in hourly_df.columns:\n",
    "    norepi_hours = hourly_df['norepinephrine_boolean'].sum()\n",
    "    print(f\"â€¢ Hours with norepinephrine: {norepi_hours} ({norepi_hours/len(hourly_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Hourly wide dataset ready for analysis!\")\n",
    "print(f\"ðŸ“Š All aggregated columns have clear suffixes indicating the aggregation method\")\n",
    "print(f\"ðŸ“… event_time_hour provides clean hour-level timestamps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert to Hourly Dataset\n",
    "\n",
    "# Convert wide dataset to hourly aggregation\n",
    "hourly_df = convert_wide_to_hourly(wide_df, aggregation_config)\n",
    "\n",
    "print(f\"Hourly dataset created: {len(hourly_df)} rows, {len(hourly_df.columns)} columns\")\n",
    "print(f\"Reduction: {len(wide_df)} â†’ {len(hourly_df)} rows ({(1-len(hourly_df)/len(wide_df))*100:.1f}% reduction)\")\n",
    "print(f\"Max nth_hour: {hourly_df['nth_hour'].max()} (â‰ˆ {hourly_df['nth_hour'].max()/24:.1f} days)\")\n",
    "\n",
    "# Show sample hourly data\n",
    "print(\"\\nSample hourly data:\")\n",
    "sample_cols = ['hospitalization_id', 'nth_hour', 'day_number', 'hour_bucket', \n",
    "               'map_max', 'heart_rate_mean', 'spo2_mean', 'spo2_min', 'norepinephrine_boolean']\n",
    "available_cols = [col for col in sample_cols if col in hourly_df.columns]\n",
    "print(hourly_df[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Analyze Results\n",
    "\n",
    "# Show one patient's first 24 hours\n",
    "first_hosp = hourly_df['hospitalization_id'].iloc[0]\n",
    "patient_data = hourly_df[\n",
    "    (hourly_df['hospitalization_id'] == first_hosp) & \n",
    "    (hourly_df['nth_hour'] <= 24)\n",
    "]\n",
    "\n",
    "print(f\"First 24 hours for patient {first_hosp}:\")\n",
    "trend_cols = ['nth_hour', 'map_max', 'heart_rate_mean', 'spo2_mean', 'norepinephrine_boolean']\n",
    "available_trend_cols = [col for col in trend_cols if col in patient_data.columns]\n",
    "print(patient_data[available_trend_cols])\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"â€¢ Total patients: {hourly_df['hospitalization_id'].nunique()}\")\n",
    "print(f\"â€¢ Total hourly records: {len(hourly_df)}\")\n",
    "print(f\"â€¢ Average records per patient: {len(hourly_df) / hourly_df['hospitalization_id'].nunique():.1f}\")\n",
    "\n",
    "# Check medication usage\n",
    "if 'norepinephrine_boolean' in hourly_df.columns:\n",
    "    norepi_hours = hourly_df['norepinephrine_boolean'].sum()\n",
    "    print(f\"â€¢ Hours with norepinephrine: {norepi_hours} ({norepi_hours/len(hourly_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Hourly wide dataset ready for analysis!\")\n",
    "print(f\"ðŸ“Š All aggregated columns have clear suffixes indicating the aggregation method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 3: Create Features for Machine Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
