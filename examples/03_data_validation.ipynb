{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation and Quality Assurance\n",
    "\n",
    "This notebook demonstrates the comprehensive data validation features in pyCLIF, including schema validation, range checking, and error handling.\n",
    "\n",
    "## Overview\n",
    "\n",
    "pyCLIF provides multiple levels of validation:\n",
    "1. **Schema Validation** - Ensures data conforms to CLIF specifications\n",
    "2. **Range Validation** - Checks if values are within expected clinical ranges\n",
    "3. **Data Type Validation** - Verifies correct data types for each column\n",
    "4. **Missing Data Detection** - Identifies and reports missing required fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Import pyCLIF components\n",
    "from pyclif import CLIF\n",
    "from pyclif.tables.patient import patient\n",
    "from pyclif.tables.vitals import vitals\n",
    "from pyclif.tables.hospitalization import hospitalization\n",
    "from pyclif.utils.validator import validate_table\n",
    "\n",
    "print(f\"pyCLIF validation components imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = \"/Users/vaishvik/downloads/CLIF_MIMIC\"\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Validation\n",
    "\n",
    "pyCLIF validates data against JSON schema specifications stored in the mCIDE directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Validate Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patient data\n",
    "patient_table = patient.from_file(DATA_DIR, \"parquet\")\n",
    "\n",
    "print(f\"Patient data loaded: {patient_table.df.shape}\")\n",
    "print(f\"\\nValidation status:\")\n",
    "print(f\"Is valid: {patient_table.isvalid()}\")\n",
    "print(f\"Number of validation errors: {len(patient_table.errors)}\")\n",
    "\n",
    "# Show validation errors if any\n",
    "if patient_table.errors:\n",
    "    print(\"\\nValidation errors:\")\n",
    "    for i, error in enumerate(patient_table.errors[:5]):  # Show first 5 errors\n",
    "        print(f\"{i+1}. {error}\")\n",
    "    if len(patient_table.errors) > 5:\n",
    "        print(f\"... and {len(patient_table.errors) - 5} more errors\")\n",
    "else:\n",
    "    print(\"\\n✅ No validation errors found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Validate Vitals Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vitals data\n",
    "vitals_table = vitals.from_file(DATA_DIR, \"parquet\")\n",
    "\n",
    "print(f\"Vitals data loaded: {vitals_table.df.shape}\")\n",
    "print(f\"\\nValidation status:\")\n",
    "print(f\"Is valid: {vitals_table.isvalid()}\")\n",
    "print(f\"Schema validation errors: {len(vitals_table.errors)}\")\n",
    "print(f\"Range validation errors: {len(vitals_table.range_validation_errors)}\")\n",
    "print(f\"Total validation issues: {len(vitals_table.errors) + len(vitals_table.range_validation_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation errors\n",
    "if vitals_table.errors:\n",
    "    print(\"=== SCHEMA VALIDATION ERRORS ===\")\n",
    "    for i, error in enumerate(vitals_table.errors[:3]):\n",
    "        print(f\"{i+1}. {error}\")\n",
    "        \n",
    "# Range validation errors\n",
    "if vitals_table.range_validation_errors:\n",
    "    print(\"\\n=== RANGE VALIDATION ERRORS ===\")\n",
    "    for i, error in enumerate(vitals_table.range_validation_errors[:3]):\n",
    "        print(f\"{i+1}. Type: {error.get('error_type')}\")\n",
    "        print(f\"   Message: {error.get('message')}\")\n",
    "        if 'vital_category' in error:\n",
    "            print(f\"   Vital: {error.get('vital_category')}\")\n",
    "            print(f\"   Affected rows: {error.get('affected_rows')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Validation Deep Dive\n",
    "\n",
    "Vitals data includes sophisticated range validation to detect clinically implausible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed range validation report\n",
    "range_report = vitals_table.get_range_validation_report()\n",
    "\n",
    "print(\"=== RANGE VALIDATION REPORT ===\")\n",
    "if not range_report.empty:\n",
    "    print(f\"Total range validation issues: {len(range_report)}\")\n",
    "    print(\"\\nRange validation summary:\")\n",
    "    print(range_report[['error_type', 'vital_category', 'affected_rows', 'message']].head(10))\n",
    "else:\n",
    "    print(\"✅ No range validation issues found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vital ranges used for validation\n",
    "vital_ranges = vitals_table.vital_ranges\n",
    "print(\"=== VITAL RANGES FOR VALIDATION ===\")\n",
    "print(f\"Total vital categories with ranges: {len(vital_ranges)}\")\n",
    "\n",
    "print(\"\\nSample vital ranges:\")\n",
    "for vital, ranges in list(vital_ranges.items())[:5]:\n",
    "    print(f\"{vital}: {ranges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Test Data with Validation Issues\n",
    "\n",
    "Let's create some test data with known validation issues to demonstrate error detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test vitals data with validation issues\n",
    "test_vitals_data = pd.DataFrame({\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'hospitalization_id': ['H001', 'H002', 'H003', 'H004', 'H005'],\n",
    "    'vital_category': ['heart_rate', 'sbp', 'temp_c', 'heart_rate', 'oxygen_saturation'],\n",
    "    'vital_value': [250, 300, 50, -10, 150],  # Extreme/invalid values\n",
    "    'vital_unit': ['bpm', 'mmHg', 'celsius', 'bpm', '%'],\n",
    "    'recorded_dttm': [\n",
    "        '2023-01-01 10:00:00',\n",
    "        '2023-01-01 11:00:00', \n",
    "        '2023-01-01 12:00:00',\n",
    "        '2023-01-01 13:00:00',\n",
    "        '2023-01-01 14:00:00'\n",
    "    ]\n",
    "    # Missing some required columns to trigger schema validation errors\n",
    "})\n",
    "\n",
    "print(\"Test vitals data created:\")\n",
    "print(test_vitals_data)\n",
    "print(f\"\\nData shape: {test_vitals_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vitals table object from test data\n",
    "test_vitals_table = vitals(data=test_vitals_data)\n",
    "\n",
    "print(\"=== TEST DATA VALIDATION RESULTS ===\")\n",
    "print(f\"Is valid: {test_vitals_table.isvalid()}\")\n",
    "print(f\"Schema validation errors: {len(test_vitals_table.errors)}\")\n",
    "print(f\"Range validation errors: {len(test_vitals_table.range_validation_errors)}\")\n",
    "\n",
    "# Show schema validation errors\n",
    "if test_vitals_table.errors:\n",
    "    print(\"\\nSchema validation errors:\")\n",
    "    for error in test_vitals_table.errors[:3]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Show range validation errors\n",
    "if test_vitals_table.range_validation_errors:\n",
    "    print(\"\\nRange validation errors:\")\n",
    "    for error in test_vitals_table.range_validation_errors:\n",
    "        print(f\"  - {error.get('message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Validation Using Validator Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validate_table utility directly\n",
    "manual_errors = validate_table(test_vitals_data, \"vitals\")\n",
    "\n",
    "print(\"=== MANUAL VALIDATION RESULTS ===\")\n",
    "print(f\"Number of validation errors: {len(manual_errors)}\")\n",
    "\n",
    "if manual_errors:\n",
    "    print(\"\\nValidation errors:\")\n",
    "    for i, error in enumerate(manual_errors[:5]):\n",
    "        print(f\"{i+1}. {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Best Practices\n",
    "\n",
    "### 1. Always Validate After Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_table_load(table_class, data_path, file_type=\"parquet\"):\n",
    "    \"\"\"Safely load a table with comprehensive validation reporting.\"\"\"\n",
    "    try:\n",
    "        # Load the table\n",
    "        table = table_class.from_file(data_path, file_type)\n",
    "        \n",
    "        # Report validation status\n",
    "        print(f\"✅ {table_class.__name__} loaded successfully\")\n",
    "        print(f\"   Shape: {table.df.shape}\")\n",
    "        print(f\"   Valid: {table.isvalid()}\")\n",
    "        \n",
    "        if hasattr(table, 'errors') and table.errors:\n",
    "            print(f\"   ⚠️  Schema errors: {len(table.errors)}\")\n",
    "            \n",
    "        if hasattr(table, 'range_validation_errors') and table.range_validation_errors:\n",
    "            print(f\"   ⚠️  Range errors: {len(table.range_validation_errors)}\")\n",
    "            \n",
    "        return table\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load {table_class.__name__}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test the safe loading function\n",
    "print(\"=== SAFE TABLE LOADING ===\")\n",
    "safe_patient = safe_table_load(patient, DATA_DIR)\n",
    "safe_vitals = safe_table_load(vitals, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_report(tables_dict):\n",
    "    \"\"\"Generate a comprehensive validation report for multiple tables.\"\"\"\n",
    "    print(\"=== COMPREHENSIVE VALIDATION REPORT ===\")\n",
    "    print(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_tables = len(tables_dict)\n",
    "    valid_tables = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for table_name, table_obj in tables_dict.items():\n",
    "        if table_obj is None:\n",
    "            print(f\"\\n{table_name.upper()}: ❌ FAILED TO LOAD\")\n",
    "            continue\n",
    "            \n",
    "        is_valid = table_obj.isvalid()\n",
    "        if is_valid:\n",
    "            valid_tables += 1\n",
    "            \n",
    "        schema_errors = len(table_obj.errors) if hasattr(table_obj, 'errors') else 0\n",
    "        range_errors = len(table_obj.range_validation_errors) if hasattr(table_obj, 'range_validation_errors') else 0\n",
    "        table_total_errors = schema_errors + range_errors\n",
    "        total_errors += table_total_errors\n",
    "        \n",
    "        status = \"✅ VALID\" if is_valid else \"⚠️  ISSUES FOUND\"\n",
    "        \n",
    "        print(f\"\\n{table_name.upper()}: {status}\")\n",
    "        print(f\"  Records: {len(table_obj.df):,}\")\n",
    "        print(f\"  Columns: {len(table_obj.df.columns)}\")\n",
    "        if schema_errors > 0:\n",
    "            print(f\"  Schema errors: {schema_errors}\")\n",
    "        if range_errors > 0:\n",
    "            print(f\"  Range errors: {range_errors}\")\n",
    "            \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"  Tables loaded: {total_tables}\")\n",
    "    print(f\"  Valid tables: {valid_tables}\")\n",
    "    print(f\"  Tables with issues: {total_tables - valid_tables}\")\n",
    "    print(f\"  Total validation errors: {total_errors}\")\n",
    "    \n",
    "    if total_errors == 0:\n",
    "        print(\"\\n🎉 All data passed validation!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  {total_errors} validation issues require attention.\")\n",
    "\n",
    "# Generate report for loaded tables\n",
    "tables_for_report = {\n",
    "    'patient': safe_patient,\n",
    "    'vitals': safe_vitals\n",
    "}\n",
    "\n",
    "generate_validation_report(tables_for_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_quality_metrics(table_obj, table_name):\n",
    "    \"\"\"Calculate comprehensive data quality metrics.\"\"\"\n",
    "    if table_obj is None or table_obj.df is None:\n",
    "        return None\n",
    "        \n",
    "    df = table_obj.df\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    \n",
    "    metrics = {\n",
    "        'table_name': table_name,\n",
    "        'total_records': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'total_cells': total_cells,\n",
    "        'missing_cells': df.isnull().sum().sum(),\n",
    "        'missing_percentage': (df.isnull().sum().sum() / total_cells) * 100,\n",
    "        'duplicate_records': df.duplicated().sum(),\n",
    "        'duplicate_percentage': (df.duplicated().sum() / len(df)) * 100,\n",
    "        'validation_passed': table_obj.isvalid(),\n",
    "        'schema_errors': len(table_obj.errors) if hasattr(table_obj, 'errors') else 0,\n",
    "        'range_errors': len(table_obj.range_validation_errors) if hasattr(table_obj, 'range_validation_errors') else 0\n",
    "    }\n",
    "    \n",
    "    # Calculate completeness per column\n",
    "    column_completeness = {}\n",
    "    for col in df.columns:\n",
    "        non_null_count = df[col].notna().sum()\n",
    "        completeness = (non_null_count / len(df)) * 100\n",
    "        column_completeness[col] = completeness\n",
    "    \n",
    "    metrics['column_completeness'] = column_completeness\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate quality metrics for our tables\n",
    "if safe_vitals:\n",
    "    vitals_metrics = calculate_data_quality_metrics(safe_vitals, 'vitals')\n",
    "    \n",
    "    print(\"=== VITALS DATA QUALITY METRICS ===\")\n",
    "    print(f\"Total records: {vitals_metrics['total_records']:,}\")\n",
    "    print(f\"Total columns: {vitals_metrics['total_columns']}\")\n",
    "    print(f\"Missing data: {vitals_metrics['missing_percentage']:.2f}%\")\n",
    "    print(f\"Duplicate records: {vitals_metrics['duplicate_percentage']:.2f}%\")\n",
    "    print(f\"Validation passed: {vitals_metrics['validation_passed']}\")\n",
    "    print(f\"Schema errors: {vitals_metrics['schema_errors']}\")\n",
    "    print(f\"Range errors: {vitals_metrics['range_errors']}\")\n",
    "    \n",
    "    # Show columns with low completeness\n",
    "    print(\"\\nColumns with <95% completeness:\")\n",
    "    low_completeness = {k: v for k, v in vitals_metrics['column_completeness'].items() if v < 95}\n",
    "    for col, completeness in sorted(low_completeness.items(), key=lambda x: x[1]):\n",
    "        print(f\"  {col}: {completeness:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Configuration and Customization\n",
    "\n",
    "Understanding the validation schemas and how to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the validation schema files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the mCIDE schema directory\n",
    "schema_dir = Path(\"src/pyclif/mCIDE\")\n",
    "if schema_dir.exists():\n",
    "    print(\"=== AVAILABLE VALIDATION SCHEMAS ===\")\n",
    "    schema_files = list(schema_dir.glob(\"*.json\"))\n",
    "    for schema_file in schema_files:\n",
    "        print(f\"  - {schema_file.name}\")\n",
    "        \n",
    "    # Load and examine a schema file\n",
    "    vitals_schema_path = schema_dir / \"VitalsModel.json\"\n",
    "    if vitals_schema_path.exists():\n",
    "        with open(vitals_schema_path, 'r') as f:\n",
    "            vitals_schema = json.load(f)\n",
    "        \n",
    "        print(\"\\n=== VITALS SCHEMA STRUCTURE ===\")\n",
    "        print(f\"Schema keys: {list(vitals_schema.keys())}\")\n",
    "        \n",
    "        if 'vital_ranges' in vitals_schema:\n",
    "            print(f\"\\nVital categories with ranges: {len(vitals_schema['vital_ranges'])}\")\n",
    "            \n",
    "        if 'vital_units' in vitals_schema:\n",
    "            print(f\"Vital categories with units: {len(vitals_schema['vital_units'])}\")\n",
    "else:\n",
    "    print(\"Schema directory not found in current location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Summary and Best Practices\n",
    "\n",
    "### Key Validation Features:\n",
    "\n",
    "1. **Automatic Validation**: All table classes automatically validate on load\n",
    "2. **Schema Compliance**: Ensures data matches CLIF specifications\n",
    "3. **Range Checking**: Validates clinical plausibility of vital signs\n",
    "4. **Error Reporting**: Detailed error messages for debugging\n",
    "5. **Quality Metrics**: Comprehensive data quality assessment\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Always check validation status** after loading data\n",
    "2. **Review validation errors** before proceeding with analysis\n",
    "3. **Use safe loading functions** that handle errors gracefully\n",
    "4. **Generate validation reports** for data quality documentation\n",
    "5. **Monitor data completeness** and missing value patterns\n",
    "6. **Validate test data** to ensure your processing pipeline works correctly\n",
    "\n",
    "### When Validation Fails:\n",
    "\n",
    "1. **Check data source** - Ensure data is in expected format\n",
    "2. **Review error messages** - Understand what specific issues exist\n",
    "3. **Clean data** - Fix known issues before reloading\n",
    "4. **Document exceptions** - Note any acceptable deviations from standards\n",
    "5. **Update schemas** - If business rules change, update validation accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook covered:\n",
    "- Schema validation against CLIF specifications\n",
    "- Range validation for clinical data\n",
    "- Error detection and reporting\n",
    "- Data quality metrics\n",
    "- Validation best practices\n",
    "- Custom validation workflows\n",
    "\n",
    "### Explore Other Notebooks:\n",
    "- `01_basic_usage.ipynb` - Basic pyCLIF usage\n",
    "- `02_individual_tables.ipynb` - Individual table classes\n",
    "- `04_vitals_analysis.ipynb` - Advanced vitals analysis\n",
    "- `05_timezone_handling.ipynb` - Timezone conversion\n",
    "- `06_data_filtering.ipynb` - Data filtering techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}