{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation and Quality Assurance\n",
    "\n",
    "This notebook demonstrates the comprehensive data validation features in pyCLIF, including schema validation, range checking, and error handling.\n",
    "\n",
    "## Overview\n",
    "\n",
    "pyCLIF provides multiple levels of validation:\n",
    "1. **Schema Validation** - Ensures data conforms to CLIF specifications\n",
    "2. **Range Validation** - Checks if values are within expected clinical ranges\n",
    "3. **Data Type Validation** - Verifies correct data types for each column\n",
    "4. **Missing Data Detection** - Identifies and reports missing required fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyCLIF validation components imported successfully!\n",
      "Python version: 3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Import pyCLIF components\n",
    "from pyclif import CLIF\n",
    "from pyclif.tables.patient import patient\n",
    "from pyclif.tables.vitals import vitals\n",
    "from pyclif.tables.hospitalization import hospitalization\n",
    "from pyclif.utils.validator import validate_table\n",
    "\n",
    "print(f\"pyCLIF validation components imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../src/pyclif/data/clif_demo/\n"
     ]
    }
   ],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = \"../src/pyclif/data/clif_demo/\"\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Validation\n",
    "\n",
    "pyCLIF validates data against JSON schema specifications stored in the mCIDE directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Validate Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clif_patient.parquet\n",
      "Data loaded successfully from clif_patient.parquet\n",
      "death_dttm: null count before conversion= 85\n",
      "death_dttm: Your timezone is UTC, Converting to your site timezone (UTC).\n",
      "death_dttm: null count after conversion= 85\n",
      "Validation completed with 2 error(s). See `errors` attribute.\n",
      "Patient data loaded: (100, 11)\n",
      "\n",
      "Validation status:\n",
      "Is valid: False\n",
      "Number of validation errors: 2\n",
      "\n",
      "Validation errors:\n",
      "1. {'type': 'null_values', 'column': 'birth_date', 'count': 100}\n",
      "2. {'type': 'null_values', 'column': 'death_dttm', 'count': 85}\n"
     ]
    }
   ],
   "source": [
    "# Load patient data\n",
    "patient_table = patient.from_file(DATA_DIR, \"parquet\")\n",
    "\n",
    "print(f\"Patient data loaded: {patient_table.df.shape}\")\n",
    "print(f\"\\nValidation status:\")\n",
    "print(f\"Is valid: {patient_table.isvalid()}\")\n",
    "print(f\"Number of validation errors: {len(patient_table.errors)}\")\n",
    "\n",
    "# Show validation errors if any\n",
    "if patient_table.errors:\n",
    "    print(\"\\nValidation errors:\")\n",
    "    for i, error in enumerate(patient_table.errors[:5]):  # Show first 5 errors\n",
    "        print(f\"{i+1}. {error}\")\n",
    "    if len(patient_table.errors) > 5:\n",
    "        print(f\"... and {len(patient_table.errors) - 5} more errors\")\n",
    "else:\n",
    "    print(\"\\n✅ No validation errors found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Validate Vitals Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "recorded_dttm: null count before conversion= 0\n",
      "recorded_dttm: Your timezone is UTC, Converting to your site timezone (UTC).\n",
      "recorded_dttm: null count after conversion= 0\n",
      "Validation completed with 5 error(s).\n",
      "  - 5 range validation error(s)\n",
      "See `errors` and `range_validation_errors` attributes for details.\n",
      "Vitals data loaded: (89085, 6)\n",
      "\n",
      "Validation status:\n",
      "Is valid: False\n",
      "Schema validation errors: 0\n",
      "Range validation errors: 5\n",
      "Total validation issues: 5\n"
     ]
    }
   ],
   "source": [
    "# Load vitals data\n",
    "vitals_table = vitals.from_file(DATA_DIR, \"parquet\")\n",
    "\n",
    "print(f\"Vitals data loaded: {vitals_table.df.shape}\")\n",
    "print(f\"\\nValidation status:\")\n",
    "print(f\"Is valid: {vitals_table.isvalid()}\")\n",
    "print(f\"Schema validation errors: {len(vitals_table.errors)}\")\n",
    "print(f\"Range validation errors: {len(vitals_table.range_validation_errors)}\")\n",
    "print(f\"Total validation issues: {len(vitals_table.errors) + len(vitals_table.range_validation_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RANGE VALIDATION ERRORS ===\n",
      "1. Type: values_out_of_range\n",
      "   Message: Vital 'height_cm' has values out of expected range: minimum value 61.0 below expected 70\n",
      "   Vital: height_cm\n",
      "   Affected rows: 71\n",
      "\n",
      "2. Type: values_out_of_range\n",
      "   Message: Vital 'map' has values out of expected range: minimum value -27.0 below expected 0; maximum value 801.0 above expected 250\n",
      "   Vital: map\n",
      "   Affected rows: 14368\n",
      "\n",
      "3. Type: values_out_of_range\n",
      "   Message: Vital 'spo2' has values out of expected range: minimum value 29.0 below expected 50\n",
      "   Vital: spo2\n",
      "   Affected rows: 13540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema validation errors\n",
    "if vitals_table.errors:\n",
    "    print(\"=== SCHEMA VALIDATION ERRORS ===\")\n",
    "    for i, error in enumerate(vitals_table.errors[:3]):\n",
    "        print(f\"{i+1}. {error}\")\n",
    "        \n",
    "# Range validation errors\n",
    "if vitals_table.range_validation_errors:\n",
    "    print(\"\\n=== RANGE VALIDATION ERRORS ===\")\n",
    "    for i, error in enumerate(vitals_table.range_validation_errors[:3]):\n",
    "        print(f\"{i+1}. Type: {error.get('error_type')}\")\n",
    "        print(f\"   Message: {error.get('message')}\")\n",
    "        if 'vital_category' in error:\n",
    "            print(f\"   Vital: {error.get('vital_category')}\")\n",
    "            print(f\"   Affected rows: {error.get('affected_rows')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Validation Deep Dive\n",
    "\n",
    "Vitals data includes sophisticated range validation to detect clinically implausible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RANGE VALIDATION REPORT ===\n",
      "Total range validation issues: 5\n",
      "\n",
      "Range validation summary:\n",
      "            error_type vital_category  affected_rows  \\\n",
      "0  values_out_of_range      height_cm             71   \n",
      "1  values_out_of_range            map          14368   \n",
      "2  values_out_of_range           spo2          13540   \n",
      "3  values_out_of_range         temp_c           3767   \n",
      "4  values_out_of_range      weight_kg            806   \n",
      "\n",
      "                                             message  \n",
      "0  Vital 'height_cm' has values out of expected r...  \n",
      "1  Vital 'map' has values out of expected range: ...  \n",
      "2  Vital 'spo2' has values out of expected range:...  \n",
      "3  Vital 'temp_c' has values out of expected rang...  \n",
      "4  Vital 'weight_kg' has values out of expected r...  \n"
     ]
    }
   ],
   "source": [
    "# Get detailed range validation report\n",
    "range_report = vitals_table.get_range_validation_report()\n",
    "\n",
    "print(\"=== RANGE VALIDATION REPORT ===\")\n",
    "if not range_report.empty:\n",
    "    print(f\"Total range validation issues: {len(range_report)}\")\n",
    "    print(\"\\nRange validation summary:\")\n",
    "    print(range_report[['error_type', 'vital_category', 'affected_rows', 'message']].head(10))\n",
    "else:\n",
    "    print(\"✅ No range validation issues found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VITAL RANGES FOR VALIDATION ===\n",
      "Total vital categories with ranges: 9\n",
      "\n",
      "Sample vital ranges:\n",
      "temp_c: {'min': 25.0, 'max': 44.0}\n",
      "heart_rate: {'min': 0, 'max': 300}\n",
      "sbp: {'min': 0, 'max': 300}\n",
      "dbp: {'min': 0, 'max': 200}\n",
      "spo2: {'min': 50, 'max': 100}\n"
     ]
    }
   ],
   "source": [
    "# Show vital ranges used for validation\n",
    "vital_ranges = vitals_table.vital_ranges\n",
    "print(\"=== VITAL RANGES FOR VALIDATION ===\")\n",
    "print(f\"Total vital categories with ranges: {len(vital_ranges)}\")\n",
    "\n",
    "print(\"\\nSample vital ranges:\")\n",
    "for vital, ranges in list(vital_ranges.items())[:5]:\n",
    "    print(f\"{vital}: {ranges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Test Data with Validation Issues\n",
    "\n",
    "Let's create some test data with known validation issues to demonstrate error detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test vitals data created:\n",
      "  patient_id hospitalization_id     vital_category  vital_value vital_unit  \\\n",
      "0       P001               H001         heart_rate          250        bpm   \n",
      "1       P002               H002                sbp          300       mmHg   \n",
      "2       P003               H003             temp_c           50    celsius   \n",
      "3       P004               H004         heart_rate          -10        bpm   \n",
      "4       P005               H005  oxygen_saturation          150          %   \n",
      "\n",
      "         recorded_dttm  \n",
      "0  2023-01-01 10:00:00  \n",
      "1  2023-01-01 11:00:00  \n",
      "2  2023-01-01 12:00:00  \n",
      "3  2023-01-01 13:00:00  \n",
      "4  2023-01-01 14:00:00  \n",
      "\n",
      "Data shape: (5, 6)\n"
     ]
    }
   ],
   "source": [
    "# Create test vitals data with validation issues\n",
    "test_vitals_data = pd.DataFrame({\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'hospitalization_id': ['H001', 'H002', 'H003', 'H004', 'H005'],\n",
    "    'vital_category': ['heart_rate', 'sbp', 'temp_c', 'heart_rate', 'oxygen_saturation'],\n",
    "    'vital_value': [250, 300, 50, -10, 150],  # Extreme/invalid values\n",
    "    'vital_unit': ['bpm', 'mmHg', 'celsius', 'bpm', '%'],\n",
    "    'recorded_dttm': [\n",
    "        '2023-01-01 10:00:00',\n",
    "        '2023-01-01 11:00:00', \n",
    "        '2023-01-01 12:00:00',\n",
    "        '2023-01-01 13:00:00',\n",
    "        '2023-01-01 14:00:00'\n",
    "    ]\n",
    "    # Missing some required columns to trigger schema validation errors\n",
    "})\n",
    "\n",
    "print(\"Test vitals data created:\")\n",
    "print(test_vitals_data)\n",
    "print(f\"\\nData shape: {test_vitals_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed with 5 error(s).\n",
      "  - 2 schema validation error(s)\n",
      "  - 3 range validation error(s)\n",
      "See `errors` and `range_validation_errors` attributes for details.\n",
      "=== TEST DATA VALIDATION RESULTS ===\n",
      "Is valid: False\n",
      "Schema validation errors: 2\n",
      "Range validation errors: 3\n",
      "\n",
      "Schema validation errors:\n",
      "  - {'type': 'datatype_mismatch', 'column': 'recorded_dttm', 'expected': 'DATETIME'}\n",
      "  - {'type': 'invalid_category', 'column': 'vital_category', 'values': ['oxygen_saturation']}\n",
      "\n",
      "Range validation errors:\n",
      "  - Vital 'heart_rate' has values out of expected range: minimum value -10 below expected 0\n",
      "  - Unknown vital category 'oxygen_saturation' affects 1 rows\n",
      "  - Vital 'temp_c' has values out of expected range: maximum value 50 above expected 44.0\n"
     ]
    }
   ],
   "source": [
    "# Create vitals table object from test data\n",
    "test_vitals_table = vitals(data=test_vitals_data)\n",
    "\n",
    "print(\"=== TEST DATA VALIDATION RESULTS ===\")\n",
    "print(f\"Is valid: {test_vitals_table.isvalid()}\")\n",
    "print(f\"Schema validation errors: {len(test_vitals_table.errors)}\")\n",
    "print(f\"Range validation errors: {len(test_vitals_table.range_validation_errors)}\")\n",
    "\n",
    "# Show schema validation errors\n",
    "if test_vitals_table.errors:\n",
    "    print(\"\\nSchema validation errors:\")\n",
    "    for error in test_vitals_table.errors[:3]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Show range validation errors\n",
    "if test_vitals_table.range_validation_errors:\n",
    "    print(\"\\nRange validation errors:\")\n",
    "    for error in test_vitals_table.range_validation_errors:\n",
    "        print(f\"  - {error.get('message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Validation Using Validator Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANUAL VALIDATION RESULTS ===\n",
      "Number of validation errors: 2\n",
      "\n",
      "Validation errors:\n",
      "1. {'type': 'datatype_mismatch', 'column': 'recorded_dttm', 'expected': 'DATETIME'}\n",
      "2. {'type': 'invalid_category', 'column': 'vital_category', 'values': ['oxygen_saturation']}\n"
     ]
    }
   ],
   "source": [
    "# Use the validate_table utility directly\n",
    "manual_errors = validate_table(test_vitals_data, \"vitals\")\n",
    "\n",
    "print(\"=== MANUAL VALIDATION RESULTS ===\")\n",
    "print(f\"Number of validation errors: {len(manual_errors)}\")\n",
    "\n",
    "if manual_errors:\n",
    "    print(\"\\nValidation errors:\")\n",
    "    for i, error in enumerate(manual_errors[:5]):\n",
    "        print(f\"{i+1}. {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Best Practices\n",
    "\n",
    "### 1. Always Validate After Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAFE TABLE LOADING ===\n",
      "Loading clif_patient.parquet\n",
      "Data loaded successfully from clif_patient.parquet\n",
      "death_dttm: null count before conversion= 85\n",
      "death_dttm: Your timezone is UTC, Converting to your site timezone (UTC).\n",
      "death_dttm: null count after conversion= 85\n",
      "Validation completed with 2 error(s). See `errors` attribute.\n",
      "✅ patient loaded successfully\n",
      "   Shape: (100, 11)\n",
      "   Valid: False\n",
      "   ⚠️  Schema errors: 2\n",
      "Loading clif_vitals.parquet\n",
      "Data loaded successfully from clif_vitals.parquet\n",
      "recorded_dttm: null count before conversion= 0\n",
      "recorded_dttm: Your timezone is UTC, Converting to your site timezone (UTC).\n",
      "recorded_dttm: null count after conversion= 0\n",
      "Validation completed with 5 error(s).\n",
      "  - 5 range validation error(s)\n",
      "See `errors` and `range_validation_errors` attributes for details.\n",
      "✅ vitals loaded successfully\n",
      "   Shape: (89085, 6)\n",
      "   Valid: False\n",
      "   ⚠️  Range errors: 5\n"
     ]
    }
   ],
   "source": [
    "def safe_table_load(table_class, data_path, file_type=\"parquet\"):\n",
    "    \"\"\"Safely load a table with comprehensive validation reporting.\"\"\"\n",
    "    try:\n",
    "        # Load the table\n",
    "        table = table_class.from_file(data_path, file_type)\n",
    "        \n",
    "        # Report validation status\n",
    "        print(f\"✅ {table_class.__name__} loaded successfully\")\n",
    "        print(f\"   Shape: {table.df.shape}\")\n",
    "        print(f\"   Valid: {table.isvalid()}\")\n",
    "        \n",
    "        if hasattr(table, 'errors') and table.errors:\n",
    "            print(f\"   ⚠️  Schema errors: {len(table.errors)}\")\n",
    "            \n",
    "        if hasattr(table, 'range_validation_errors') and table.range_validation_errors:\n",
    "            print(f\"   ⚠️  Range errors: {len(table.range_validation_errors)}\")\n",
    "            \n",
    "        return table\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load {table_class.__name__}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test the safe loading function\n",
    "print(\"=== SAFE TABLE LOADING ===\")\n",
    "safe_patient = safe_table_load(patient, DATA_DIR)\n",
    "safe_vitals = safe_table_load(vitals, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE VALIDATION REPORT ===\n",
      "Generated on: 2025-06-27 12:54:34\n",
      "==================================================\n",
      "\n",
      "PATIENT: ⚠️  ISSUES FOUND\n",
      "  Records: 100\n",
      "  Columns: 11\n",
      "  Schema errors: 2\n",
      "\n",
      "VITALS: ⚠️  ISSUES FOUND\n",
      "  Records: 89,085\n",
      "  Columns: 6\n",
      "  Range errors: 5\n",
      "\n",
      "==================================================\n",
      "SUMMARY:\n",
      "  Tables loaded: 2\n",
      "  Valid tables: 0\n",
      "  Tables with issues: 2\n",
      "  Total validation errors: 7\n",
      "\n",
      "⚠️  7 validation issues require attention.\n"
     ]
    }
   ],
   "source": [
    "def generate_validation_report(tables_dict):\n",
    "    \"\"\"Generate a comprehensive validation report for multiple tables.\"\"\"\n",
    "    print(\"=== COMPREHENSIVE VALIDATION REPORT ===\")\n",
    "    print(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_tables = len(tables_dict)\n",
    "    valid_tables = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for table_name, table_obj in tables_dict.items():\n",
    "        if table_obj is None:\n",
    "            print(f\"\\n{table_name.upper()}: ❌ FAILED TO LOAD\")\n",
    "            continue\n",
    "            \n",
    "        is_valid = table_obj.isvalid()\n",
    "        if is_valid:\n",
    "            valid_tables += 1\n",
    "            \n",
    "        schema_errors = len(table_obj.errors) if hasattr(table_obj, 'errors') else 0\n",
    "        range_errors = len(table_obj.range_validation_errors) if hasattr(table_obj, 'range_validation_errors') else 0\n",
    "        table_total_errors = schema_errors + range_errors\n",
    "        total_errors += table_total_errors\n",
    "        \n",
    "        status = \"✅ VALID\" if is_valid else \"⚠️  ISSUES FOUND\"\n",
    "        \n",
    "        print(f\"\\n{table_name.upper()}: {status}\")\n",
    "        print(f\"  Records: {len(table_obj.df):,}\")\n",
    "        print(f\"  Columns: {len(table_obj.df.columns)}\")\n",
    "        if schema_errors > 0:\n",
    "            print(f\"  Schema errors: {schema_errors}\")\n",
    "        if range_errors > 0:\n",
    "            print(f\"  Range errors: {range_errors}\")\n",
    "            \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"  Tables loaded: {total_tables}\")\n",
    "    print(f\"  Valid tables: {valid_tables}\")\n",
    "    print(f\"  Tables with issues: {total_tables - valid_tables}\")\n",
    "    print(f\"  Total validation errors: {total_errors}\")\n",
    "    \n",
    "    if total_errors == 0:\n",
    "        print(\"\\n🎉 All data passed validation!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  {total_errors} validation issues require attention.\")\n",
    "\n",
    "# Generate report for loaded tables\n",
    "tables_for_report = {\n",
    "    'patient': safe_patient,\n",
    "    'vitals': safe_vitals\n",
    "}\n",
    "\n",
    "generate_validation_report(tables_for_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VITALS DATA QUALITY METRICS ===\n",
      "Total records: 89,085\n",
      "Total columns: 6\n",
      "Missing data: 15.98%\n",
      "Duplicate records: 0.00%\n",
      "Validation passed: False\n",
      "Schema errors: 0\n",
      "Range errors: 5\n",
      "\n",
      "Columns with <95% completeness:\n",
      "  meas_site_name: 4.1%\n"
     ]
    }
   ],
   "source": [
    "def calculate_data_quality_metrics(table_obj, table_name):\n",
    "    \"\"\"Calculate comprehensive data quality metrics.\"\"\"\n",
    "    if table_obj is None or table_obj.df is None:\n",
    "        return None\n",
    "        \n",
    "    df = table_obj.df\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    \n",
    "    metrics = {\n",
    "        'table_name': table_name,\n",
    "        'total_records': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'total_cells': total_cells,\n",
    "        'missing_cells': df.isnull().sum().sum(),\n",
    "        'missing_percentage': (df.isnull().sum().sum() / total_cells) * 100,\n",
    "        'duplicate_records': df.duplicated().sum(),\n",
    "        'duplicate_percentage': (df.duplicated().sum() / len(df)) * 100,\n",
    "        'validation_passed': table_obj.isvalid(),\n",
    "        'schema_errors': len(table_obj.errors) if hasattr(table_obj, 'errors') else 0,\n",
    "        'range_errors': len(table_obj.range_validation_errors) if hasattr(table_obj, 'range_validation_errors') else 0\n",
    "    }\n",
    "    \n",
    "    # Calculate completeness per column\n",
    "    column_completeness = {}\n",
    "    for col in df.columns:\n",
    "        non_null_count = df[col].notna().sum()\n",
    "        completeness = (non_null_count / len(df)) * 100\n",
    "        column_completeness[col] = completeness\n",
    "    \n",
    "    metrics['column_completeness'] = column_completeness\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate quality metrics for our tables\n",
    "if safe_vitals:\n",
    "    vitals_metrics = calculate_data_quality_metrics(safe_vitals, 'vitals')\n",
    "    \n",
    "    print(\"=== VITALS DATA QUALITY METRICS ===\")\n",
    "    print(f\"Total records: {vitals_metrics['total_records']:,}\")\n",
    "    print(f\"Total columns: {vitals_metrics['total_columns']}\")\n",
    "    print(f\"Missing data: {vitals_metrics['missing_percentage']:.2f}%\")\n",
    "    print(f\"Duplicate records: {vitals_metrics['duplicate_percentage']:.2f}%\")\n",
    "    print(f\"Validation passed: {vitals_metrics['validation_passed']}\")\n",
    "    print(f\"Schema errors: {vitals_metrics['schema_errors']}\")\n",
    "    print(f\"Range errors: {vitals_metrics['range_errors']}\")\n",
    "    \n",
    "    # Show columns with low completeness\n",
    "    print(\"\\nColumns with <95% completeness:\")\n",
    "    low_completeness = {k: v for k, v in vitals_metrics['column_completeness'].items() if v < 95}\n",
    "    for col, completeness in sorted(low_completeness.items(), key=lambda x: x[1]):\n",
    "        print(f\"  {col}: {completeness:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Configuration and Customization\n",
    "\n",
    "Understanding the validation schemas and how to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema directory not found in current location\n"
     ]
    }
   ],
   "source": [
    "# Explore the validation schema files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the mCIDE schema directory\n",
    "schema_dir = Path(\"src/pyclif/mCIDE\")\n",
    "if schema_dir.exists():\n",
    "    print(\"=== AVAILABLE VALIDATION SCHEMAS ===\")\n",
    "    schema_files = list(schema_dir.glob(\"*.json\"))\n",
    "    for schema_file in schema_files:\n",
    "        print(f\"  - {schema_file.name}\")\n",
    "        \n",
    "    # Load and examine a schema file\n",
    "    vitals_schema_path = schema_dir / \"VitalsModel.json\"\n",
    "    if vitals_schema_path.exists():\n",
    "        with open(vitals_schema_path, 'r') as f:\n",
    "            vitals_schema = json.load(f)\n",
    "        \n",
    "        print(\"\\n=== VITALS SCHEMA STRUCTURE ===\")\n",
    "        print(f\"Schema keys: {list(vitals_schema.keys())}\")\n",
    "        \n",
    "        if 'vital_ranges' in vitals_schema:\n",
    "            print(f\"\\nVital categories with ranges: {len(vitals_schema['vital_ranges'])}\")\n",
    "            \n",
    "        if 'vital_units' in vitals_schema:\n",
    "            print(f\"Vital categories with units: {len(vitals_schema['vital_units'])}\")\n",
    "else:\n",
    "    print(\"Schema directory not found in current location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Summary and Best Practices\n",
    "\n",
    "### Key Validation Features:\n",
    "\n",
    "1. **Automatic Validation**: All table classes automatically validate on load\n",
    "2. **Schema Compliance**: Ensures data matches CLIF specifications\n",
    "3. **Range Checking**: Validates clinical plausibility of vital signs\n",
    "4. **Error Reporting**: Detailed error messages for debugging\n",
    "5. **Quality Metrics**: Comprehensive data quality assessment\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Always check validation status** after loading data\n",
    "2. **Review validation errors** before proceeding with analysis\n",
    "3. **Use safe loading functions** that handle errors gracefully\n",
    "4. **Generate validation reports** for data quality documentation\n",
    "5. **Monitor data completeness** and missing value patterns\n",
    "6. **Validate test data** to ensure your processing pipeline works correctly\n",
    "\n",
    "### When Validation Fails:\n",
    "\n",
    "1. **Check data source** - Ensure data is in expected format\n",
    "2. **Review error messages** - Understand what specific issues exist\n",
    "3. **Clean data** - Fix known issues before reloading\n",
    "4. **Document exceptions** - Note any acceptable deviations from standards\n",
    "5. **Update schemas** - If business rules change, update validation accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook covered:\n",
    "- Schema validation against CLIF specifications\n",
    "- Range validation for clinical data\n",
    "- Error detection and reporting\n",
    "- Data quality metrics\n",
    "- Validation best practices\n",
    "- Custom validation workflows\n",
    "\n",
    "### Explore Other Notebooks:\n",
    "- `01_basic_usage.ipynb` - Basic pyCLIF usage\n",
    "- `02_individual_tables.ipynb` - Individual table classes\n",
    "- `04_vitals_analysis.ipynb` - Advanced vitals analysis\n",
    "- `05_timezone_handling.ipynb` - Timezone conversion\n",
    "- `06_data_filtering.ipynb` - Data filtering techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyclif_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
