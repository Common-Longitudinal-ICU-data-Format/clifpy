{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Mortality Model - Feature Engineering\n",
    "\n",
    "This notebook loads the ICU cohort and creates hourly wide dataset for the first 24 hours of ICU stay.\n",
    "\n",
    "## Objective\n",
    "- Load ICU cohort from 01_cohort.ipynb\n",
    "- Use pyCLIF to extract features from CLIF tables\n",
    "- Create hourly wide dataset for the first 24 hours\n",
    "- Filter to encounters with complete 24-hour data\n",
    "- Save features for modeling\n",
    "\n",
    "## Feature Sources\n",
    "- **Vitals**: All vital_category values\n",
    "- **Labs**: All lab_category values\n",
    "- **Patient Assessments**: GCS_total, RASS\n",
    "- **Respiratory Support**: Mode, FiO2, PEEP, ventilator settings (with one-hot encoding)\n",
    "- **Medications**: All vasoactives and sedatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "from pyclif.utils.wide_dataset import convert_wide_to_hourly\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== ICU Mortality Model - Feature Engineering ===\")\n",
    "print(\"Setting up environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json\"\"\"\n",
    "    config_path = os.path.join(\"..\", \"config\", \"config.json\")\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        print(\"✅ Loaded configuration from config.json\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Configuration file not found. Please create config.json based on the config_template.\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Site: {config['site']}\")\n",
    "print(f\"Data path: {config['clif2_path']}\")\n",
    "print(f\"File type: {config['filetype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pyCLIF\n",
    "clif = CLIF(\n",
    "    data_dir=config['clif2_path'],\n",
    "    filetype=config['filetype'],\n",
    "    timezone=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(\"✅ pyCLIF initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ICU Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ICU cohort from 01_cohort.ipynb\n",
    "cohort_path = os.path.join('output', 'intermitted', 'icu_cohort.csv')\n",
    "\n",
    "if os.path.exists(cohort_path):\n",
    "    cohort_df = pd.read_csv(cohort_path)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['start_dttm', 'hour_24_start_dttm', 'hour_24_end_dttm']\n",
    "    for col in datetime_cols:\n",
    "        cohort_df[col] = pd.to_datetime(cohort_df[col])\n",
    "    \n",
    "    print(f\"✅ Loaded ICU cohort: {len(cohort_df)} hospitalizations\")\n",
    "    print(f\"Mortality rate: {cohort_df['disposition'].mean():.3f}\")\n",
    "    print(f\"Time range: {cohort_df['start_dttm'].min()} to {cohort_df['start_dttm'].max()}\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cohort file not found at {cohort_path}. Please run 01_cohort.ipynb first.\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample cohort records:\")\n",
    "print(cohort_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction configuration\n",
    "print(\"Configuring feature extraction...\")\n",
    "\n",
    "# Get hospitalization IDs from cohort\n",
    "cohort_ids = cohort_df['hospitalization_id'].unique().tolist()\n",
    "print(f\"Extracting features for {len(cohort_ids)} hospitalizations\")\n",
    "\n",
    "# Define category filters for each table\n",
    "category_filters = {\n",
    "    'vitals': [  # Common vital signs\n",
    "        'heart_rate', 'sbp', 'dbp', 'map', 'respiratory_rate', 'spo2', 'temp_c',\n",
    "        'weight_kg', 'height_cm', 'cvp', 'icp', 'cpp'\n",
    "    ],\n",
    "    'labs': [  # Common lab values\n",
    "        'hemoglobin', 'hematocrit', 'platelet_count', 'wbc', 'neutrophils_percent',\n",
    "        'lymphocytes_percent', 'monocytes_percent', 'eosinophils_percent', 'basophils_percent',\n",
    "        'sodium', 'potassium', 'chloride', 'co2', 'anion_gap', 'bun', 'creatinine',\n",
    "        'glucose_serum', 'calcium_total', 'magnesium', 'phosphorus', 'albumin',\n",
    "        'total_protein', 'bilirubin_total', 'bilirubin_conjugated', 'alt', 'ast',\n",
    "        'alkaline_phosphatase', 'lactate', 'ph', 'pco2', 'po2', 'bicarbonate',\n",
    "        'base_excess', 'troponin_i', 'troponin_t', 'ck', 'ck_mb', 'bnp', 'nt_probnp',\n",
    "        'pt', 'ptt', 'inr', 'fibrinogen', 'd_dimer'\n",
    "    ],\n",
    "    'patient_assessments': [  # Neurological assessments\n",
    "        'gcs_total', 'gcs_eye', 'gcs_verbal', 'gcs_motor', 'rass', 'cam_icu',\n",
    "        'sbt_delivery_pass_fail', 'sat_delivery_pass_fail'\n",
    "    ],\n",
    "    'medication_admin_continuous': [  # Vasoactives and sedatives\n",
    "        'norepinephrine', 'epinephrine', 'dopamine', 'dobutamine', 'phenylephrine',\n",
    "        'vasopressin', 'milrinone', 'midazolam', 'propofol', 'fentanyl', 'morphine',\n",
    "        'hydromorphone', 'dexmedetomidine', 'lorazepam', 'insulin', 'heparin'\n",
    "    ],\n",
    "    'respiratory_support': [  # All respiratory support categories\n",
    "        'mode', 'device_category', 'fio2', 'peep', 'pip', 'pmean', 'tv', 'mv',\n",
    "        'rr_set', 'rr_total', 'ie_ratio', 'flow_rate', 'pressure_support'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Feature extraction configuration:\")\n",
    "for table, categories in category_filters.items():\n",
    "    print(f\"  {table}: {len(categories)} categories\")\n",
    "    print(f\"    {categories[:5]}...\" if len(categories) > 5 else f\"    {categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wide Dataset Using pyCLIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wide dataset for cohort hospitalizations\n",
    "print(\"Creating wide dataset using pyCLIF...\")\n",
    "\n",
    "try:\n",
    "    wide_df = clif.create_wide_dataset(\n",
    "        hospitalization_ids=cohort_ids,\n",
    "        optional_tables=['vitals', 'labs', 'patient_assessments', 'medication_admin_continuous', 'respiratory_support'],\n",
    "        category_filters=category_filters,\n",
    "        save_to_data_location=False  # Keep in memory for processing\n",
    "    )\n",
    "    \n",
    "    if wide_df is not None:\n",
    "        print(f\"✅ Wide dataset created: {len(wide_df)} records, {len(wide_df.columns)} columns\")\n",
    "        print(f\"Hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n",
    "        print(f\"Time range: {wide_df['event_time'].min()} to {wide_df['event_time'].max()}\")\n",
    "        \n",
    "        # Show available columns by category\n",
    "        print(\"\\nAvailable features by category:\")\n",
    "        for category, expected_cols in category_filters.items():\n",
    "            available_cols = [col for col in expected_cols if col in wide_df.columns]\n",
    "            print(f\"  {category}: {len(available_cols)}/{len(expected_cols)} columns\")\n",
    "            if len(available_cols) != len(expected_cols):\n",
    "                missing = set(expected_cols) - set(available_cols)\n",
    "                print(f\"    Missing: {list(missing)[:5]}...\" if len(missing) > 5 else f\"    Missing: {list(missing)}\")\n",
    "    else:\n",
    "        raise ValueError(\"Wide dataset creation returned None\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating wide dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to 24-Hour Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter wide dataset to 24-hour windows\n",
    "print(\"Filtering to 24-hour windows...\")\n",
    "\n",
    "# Merge with cohort to get time windows\n",
    "wide_df_filtered = pd.merge(\n",
    "    wide_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(wide_df_filtered)} records\")\n",
    "\n",
    "# Filter events within 24-hour window\n",
    "wide_df_filtered = wide_df_filtered[\n",
    "    (wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) &\n",
    "    (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Filtered to 24-hour windows: {len(wide_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data: {wide_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation\n",
    "print(\"\\nTime window validation:\")\n",
    "print(f\"All events within window: {((wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) & (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(wide_df_filtered) / wide_df_filtered['hospitalization_id'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Respiratory Support One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare respiratory support for one-hot encoding\n",
    "print(\"Preparing respiratory support one-hot encoding...\")\n",
    "\n",
    "# Check for respiratory support categorical columns\n",
    "resp_categorical_cols = ['mode', 'device_category']\n",
    "resp_available_cols = [col for col in resp_categorical_cols if col in wide_df_filtered.columns]\n",
    "\n",
    "if resp_available_cols:\n",
    "    print(f\"Found respiratory support categorical columns: {resp_available_cols}\")\n",
    "    \n",
    "    # Show unique values for each categorical column\n",
    "    for col in resp_available_cols:\n",
    "        unique_vals = wide_df_filtered[col].dropna().unique()\n",
    "        print(f\"  {col}: {len(unique_vals)} unique values\")\n",
    "        print(f\"    Values: {list(unique_vals)[:10]}...\" if len(unique_vals) > 10 else f\"    Values: {list(unique_vals)}\")\n",
    "        \n",
    "        # Create one-hot encoded columns\n",
    "        for val in unique_vals:\n",
    "            if pd.notna(val) and val != '':\n",
    "                col_name = f\"{col}_{str(val).lower().replace(' ', '_').replace('/', '_').replace('-', '_')}\"\n",
    "                wide_df_filtered[col_name] = (wide_df_filtered[col] == val).astype(int)\n",
    "                print(f\"    Created: {col_name}\")\n",
    "    \n",
    "    print(f\"✅ One-hot encoding completed for respiratory support\")\n",
    "else:\n",
    "    print(\"No respiratory support categorical columns found\")\n",
    "\n",
    "# Check total columns after one-hot encoding\n",
    "print(f\"\\nTotal columns after one-hot encoding: {len(wide_df_filtered.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hourly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hourly aggregation\n",
    "print(\"Configuring hourly aggregation...\")\n",
    "\n",
    "# Define columns for different aggregation methods\n",
    "continuous_cols = []\n",
    "categorical_cols = []\n",
    "boolean_cols = []\n",
    "one_hot_cols = []\n",
    "\n",
    "# Identify column types\n",
    "for col in wide_df_filtered.columns:\n",
    "    if col in ['hospitalization_id', 'patient_id', 'event_time', 'day_number', 'hosp_id_day_key', \n",
    "               'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']:\n",
    "        continue  # Skip meta columns\n",
    "    \n",
    "    # Check if it's a one-hot encoded column (from respiratory support)\n",
    "    if any(col.startswith(f\"{cat}_\") for cat in resp_available_cols):\n",
    "        one_hot_cols.append(col)\n",
    "    # Check if it's a medication (boolean)\n",
    "    elif col in category_filters.get('medication_admin_continuous', []):\n",
    "        boolean_cols.append(col)\n",
    "    # Check if it's categorical (original respiratory support columns)\n",
    "    elif col in resp_categorical_cols:\n",
    "        categorical_cols.append(col)\n",
    "    # Otherwise, treat as continuous\n",
    "    else:\n",
    "        continuous_cols.append(col)\n",
    "\n",
    "# Configure aggregation\n",
    "aggregation_config = {\n",
    "    'min': continuous_cols,\n",
    "    'max': continuous_cols,\n",
    "    'mean': continuous_cols,\n",
    "    'boolean': boolean_cols + one_hot_cols,  # Both medications and one-hot encoded columns\n",
    "    'last': categorical_cols  # For original categorical columns\n",
    "}\n",
    "\n",
    "print(f\"Aggregation configuration:\")\n",
    "for method, cols in aggregation_config.items():\n",
    "    if cols:\n",
    "        print(f\"  {method}: {len(cols)} columns\")\n",
    "        print(f\"    Examples: {cols[:5]}...\" if len(cols) > 5 else f\"    All: {cols}\")\n",
    "    else:\n",
    "        print(f\"  {method}: No columns\")\n",
    "\n",
    "print(f\"\\n✅ Hourly aggregation configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Hourly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to hourly dataset\n",
    "print(\"Converting to hourly dataset...\")\n",
    "\n",
    "try:\n",
    "    hourly_df = convert_wide_to_hourly(wide_df_filtered, aggregation_config)\n",
    "    \n",
    "    print(f\"✅ Hourly dataset created: {len(hourly_df)} rows, {len(hourly_df.columns)} columns\")\n",
    "    print(f\"Reduction: {len(wide_df_filtered)} → {len(hourly_df)} rows ({(1-len(hourly_df)/len(wide_df_filtered))*100:.1f}% reduction)\")\n",
    "    print(f\"Hospitalizations: {hourly_df['hospitalization_id'].nunique()}\")\n",
    "    print(f\"Max nth_hour: {hourly_df['nth_hour'].max()} (≈ {hourly_df['nth_hour'].max():.1f} hours)\")\n",
    "    \n",
    "    # Show column naming convention\n",
    "    print(\"\\nColumn naming examples:\")\n",
    "    sample_cols = [col for col in hourly_df.columns if '_' in col and col not in ['hospitalization_id', 'patient_id', 'event_time']][:10]\n",
    "    for col in sample_cols:\n",
    "        print(f\"  {col}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error converting to hourly dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to Complete 24-Hour Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to encounters with complete 24-hour data\n",
    "print(\"Filtering to encounters with complete 24-hour data...\")\n",
    "\n",
    "# Count hours per hospitalization\n",
    "hours_per_hosp = hourly_df.groupby('hospitalization_id')['nth_hour'].nunique().reset_index()\n",
    "hours_per_hosp.columns = ['hospitalization_id', 'total_hours']\n",
    "\n",
    "print(f\"Hours per hospitalization distribution:\")\n",
    "print(hours_per_hosp['total_hours'].describe())\n",
    "\n",
    "# Filter to hospitalizations with at least 20 hours of data (allowing for some missing hours)\n",
    "min_hours_threshold = 20\n",
    "complete_hosp_ids = hours_per_hosp[hours_per_hosp['total_hours'] >= min_hours_threshold]['hospitalization_id'].unique()\n",
    "\n",
    "print(f\"\\nHospitalizations with ≥{min_hours_threshold} hours: {len(complete_hosp_ids)} / {len(hours_per_hosp)}\")\n",
    "\n",
    "# Filter hourly dataset\n",
    "hourly_df_complete = hourly_df[hourly_df['hospitalization_id'].isin(complete_hosp_ids)].copy()\n",
    "\n",
    "# Filter to first 24 hours only\n",
    "hourly_df_complete = hourly_df_complete[hourly_df_complete['nth_hour'] < 24].copy()\n",
    "\n",
    "print(f\"✅ Final hourly dataset: {len(hourly_df_complete)} rows\")\n",
    "print(f\"Hospitalizations: {hourly_df_complete['hospitalization_id'].nunique()}\")\n",
    "print(f\"Hour range: {hourly_df_complete['nth_hour'].min()} to {hourly_df_complete['nth_hour'].max()}\")\n",
    "\n",
    "# Add outcome variable\n",
    "hourly_df_complete = pd.merge(\n",
    "    hourly_df_complete,\n",
    "    cohort_df[['hospitalization_id', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Mortality rate in final dataset: {hourly_df_complete['disposition'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature summary\n",
    "print(\"Creating feature summary...\")\n",
    "\n",
    "# Get feature columns (exclude meta columns)\n",
    "meta_cols = ['hospitalization_id', 'patient_id', 'nth_hour', 'event_time_hour', 'hour_bucket', 'disposition']\n",
    "feature_cols = [col for col in hourly_df_complete.columns if col not in meta_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Create feature summary\n",
    "feature_summary = []\n",
    "\n",
    "for col in feature_cols:\n",
    "    non_null_count = hourly_df_complete[col].notna().sum()\n",
    "    null_count = hourly_df_complete[col].isna().sum()\n",
    "    completeness = (non_null_count / len(hourly_df_complete)) * 100\n",
    "    \n",
    "    # Determine feature type\n",
    "    if col.endswith('_boolean'):\n",
    "        feature_type = 'boolean'\n",
    "        unique_vals = hourly_df_complete[col].dropna().unique()\n",
    "        stats = {'unique_values': len(unique_vals), 'positive_rate': hourly_df_complete[col].mean()}\n",
    "    elif col.endswith('_min') or col.endswith('_max') or col.endswith('_mean'):\n",
    "        feature_type = 'continuous'\n",
    "        stats = {\n",
    "            'mean': hourly_df_complete[col].mean(),\n",
    "            'std': hourly_df_complete[col].std(),\n",
    "            'min': hourly_df_complete[col].min(),\n",
    "            'max': hourly_df_complete[col].max()\n",
    "        }\n",
    "    elif col.endswith('_last'):\n",
    "        feature_type = 'categorical'\n",
    "        unique_vals = hourly_df_complete[col].dropna().unique()\n",
    "        stats = {'unique_values': len(unique_vals), 'most_common': hourly_df_complete[col].mode().iloc[0] if not hourly_df_complete[col].empty else None}\n",
    "    else:\n",
    "        feature_type = 'other'\n",
    "        stats = {}\n",
    "    \n",
    "    feature_summary.append({\n",
    "        'feature': col,\n",
    "        'type': feature_type,\n",
    "        'non_null_count': non_null_count,\n",
    "        'null_count': null_count,\n",
    "        'completeness_pct': completeness,\n",
    "        'stats': stats\n",
    "    })\n",
    "\n",
    "feature_summary_df = pd.DataFrame(feature_summary)\n",
    "\n",
    "print(f\"\\nFeature summary by type:\")\n",
    "print(feature_summary_df.groupby('type').size())\n",
    "\n",
    "print(f\"\\nTop 10 most complete features:\")\n",
    "top_complete = feature_summary_df.nlargest(10, 'completeness_pct')\n",
    "for _, row in top_complete.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['completeness_pct']:.1f}% ({row['type']})\")\n",
    "\n",
    "print(f\"\\nFeatures with >50% completeness: {(feature_summary_df['completeness_pct'] > 50).sum()}\")\n",
    "print(f\"Features with >80% completeness: {(feature_summary_df['completeness_pct'] > 80).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature Dataset and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hourly features dataset\n",
    "print(\"Saving feature dataset and summary...\")\n",
    "\n",
    "# Save main hourly dataset\n",
    "hourly_output_path = os.path.join('output', 'intermitted', 'hourly_features_24hr.csv')\n",
    "hourly_df_complete.to_csv(hourly_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Hourly features saved to: {hourly_output_path}\")\n",
    "print(f\"File size: {os.path.getsize(hourly_output_path) / (1024*1024):.1f} MB\")\n",
    "print(f\"Shape: {hourly_df_complete.shape}\")\n",
    "\n",
    "# Save feature summary\n",
    "summary_output_path = os.path.join('output', 'intermitted', 'feature_summary.csv')\n",
    "feature_summary_df.to_csv(summary_output_path, index=False)\n",
    "\n",
    "print(f\"✅ Feature summary saved to: {summary_output_path}\")\n",
    "\n",
    "# Save high-level statistics\n",
    "stats = {\n",
    "    'total_hospitalizations': int(hourly_df_complete['hospitalization_id'].nunique()),\n",
    "    'total_hourly_records': int(len(hourly_df_complete)),\n",
    "    'total_features': len(feature_cols),\n",
    "    'mortality_rate': float(hourly_df_complete['disposition'].mean()),\n",
    "    'avg_records_per_hosp': float(len(hourly_df_complete) / hourly_df_complete['hospitalization_id'].nunique()),\n",
    "    'hour_range': [int(hourly_df_complete['nth_hour'].min()), int(hourly_df_complete['nth_hour'].max())],\n",
    "    'features_by_type': feature_summary_df.groupby('type').size().to_dict(),\n",
    "    'high_completeness_features': int((feature_summary_df['completeness_pct'] > 80).sum()),\n",
    "    'processing_info': {\n",
    "        'original_wide_records': len(wide_df_filtered),\n",
    "        'hourly_records': len(hourly_df_complete),\n",
    "        'reduction_pct': float((1 - len(hourly_df_complete) / len(wide_df_filtered)) * 100),\n",
    "        'min_hours_threshold': min_hours_threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_path = os.path.join('output', 'intermitted', 'feature_stats.json')\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"✅ Feature statistics saved to: {stats_path}\")\n",
    "\n",
    "print(\"\\n🎉 Feature engineering completed successfully!\")\n",
    "print(f\"\\n📊 Final Dataset Summary:\")\n",
    "print(f\"  - Hospitalizations: {stats['total_hospitalizations']:,}\")\n",
    "print(f\"  - Hourly records: {stats['total_hourly_records']:,}\")\n",
    "print(f\"  - Features: {stats['total_features']:,}\")\n",
    "print(f\"  - Mortality rate: {stats['mortality_rate']:.3f}\")\n",
    "print(f\"  - Hours per patient: {stats['avg_records_per_hosp']:.1f}\")\n",
    "print(f\"  - High-quality features (>80% complete): {stats['high_completeness_features']:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}