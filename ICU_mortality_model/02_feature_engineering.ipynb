{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Mortality Model - Feature Engineering\n",
    "\n",
    "This notebook loads the ICU cohort and creates hourly wide dataset for the first 24 hours of ICU stay.\n",
    "\n",
    "## Objective\n",
    "- Load ICU cohort from 01_cohort.ipynb\n",
    "- Use pyCLIF to extract features from CLIF tables\n",
    "- Create hourly wide dataset for the first 24 hours\n",
    "- Filter to encounters with complete 24-hour data\n",
    "- Save features for modeling\n",
    "\n",
    "## Feature Sources\n",
    "- **Vitals**: All vital_category values\n",
    "- **Labs**: All lab_category values\n",
    "- **Patient Assessments**: GCS_total, RASS\n",
    "- **Respiratory Support**: Mode, FiO2, PEEP, ventilator settings (with one-hot encoding)\n",
    "- **Medications**: All vasoactives and sedatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ICU Mortality Model - Feature Engineering ===\n",
      "Setting up environment...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "from pyclif.utils.wide_dataset import convert_wide_to_hourly\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== ICU Mortality Model - Feature Engineering ===\")\n",
    "print(\"Setting up environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded configuration from config.json\n",
      "Site: MIMIC\n",
      "Data path: /Users/sudo_sage/Documents/work/mimic_demo\n",
      "File type: parquet\n"
     ]
    }
   ],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json\"\"\"\n",
    "    config_path = os.path.join(\"config_demo.json\")\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        print(\"âœ… Loaded configuration from config.json\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Configuration file not found. Please create config.json based on the config_template.\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Site: {config['site']}\")\n",
    "print(f\"Data path: {config['clif2_path']}\")\n",
    "print(f\"File type: {config['filetype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIF Object Initialized.\n",
      "âœ… pyCLIF initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyCLIF\n",
    "clif = CLIF(\n",
    "    data_dir=config['clif2_path'],\n",
    "    filetype=config['filetype'],\n",
    "    timezone=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(\"âœ… pyCLIF initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ICU Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded ICU cohort: 89 hospitalizations\n",
      "Mortality rate: 0.090\n",
      "Time range: 2110-04-11 20:52:22+00:00 to 2201-12-12 01:11:52+00:00\n",
      "\n",
      "Sample cohort records:\n",
      "   hospitalization_id                start_dttm        hour_24_start_dttm  \\\n",
      "0            24597018 2157-11-21 00:18:02+00:00 2157-11-21 00:18:02+00:00   \n",
      "1            25563031 2110-04-11 20:52:22+00:00 2110-04-11 20:52:22+00:00   \n",
      "2            20321825 2156-05-01 02:53:00+00:00 2156-05-01 02:53:00+00:00   \n",
      "3            23473524 2156-05-11 19:49:34+00:00 2156-05-11 19:49:34+00:00   \n",
      "4            28662225 2156-04-12 21:24:18+00:00 2156-04-12 21:24:18+00:00   \n",
      "\n",
      "           hour_24_end_dttm  disposition  \n",
      "0 2157-11-22 00:18:02+00:00            0  \n",
      "1 2110-04-12 20:52:22+00:00            0  \n",
      "2 2156-05-02 02:53:00+00:00            0  \n",
      "3 2156-05-12 19:49:34+00:00            0  \n",
      "4 2156-04-13 21:24:18+00:00            0  \n"
     ]
    }
   ],
   "source": [
    "# Load ICU cohort from 01_cohort.ipynb\n",
    "cohort_path = os.path.join('output', 'intermitted', 'icu_cohort.csv')\n",
    "\n",
    "if os.path.exists(cohort_path):\n",
    "    cohort_df = pd.read_csv(cohort_path)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['start_dttm', 'hour_24_start_dttm', 'hour_24_end_dttm']\n",
    "    for col in datetime_cols:\n",
    "        cohort_df[col] = pd.to_datetime(cohort_df[col])\n",
    "    \n",
    "    print(f\"âœ… Loaded ICU cohort: {len(cohort_df)} hospitalizations\")\n",
    "    print(f\"Mortality rate: {cohort_df['disposition'].mean():.3f}\")\n",
    "    print(f\"Time range: {cohort_df['start_dttm'].min()} to {cohort_df['start_dttm'].max()}\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cohort file not found at {cohort_path}. Please run 01_cohort.ipynb first.\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample cohort records:\")\n",
    "print(cohort_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring feature extraction...\n",
      "Extracting features for 89 hospitalizations\n",
      "Feature extraction configuration:\n",
      "  vitals: 7 categories\n",
      "    ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c']...\n",
      "  labs: 52 categories\n",
      "    ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent']...\n",
      "  patient_assessments: 2 categories\n",
      "    ['gcs_total', 'rass']\n",
      "  medication_admin_continuous: 19 categories\n",
      "    ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin']...\n",
      "  respiratory_support: 3 categories\n",
      "    ['mode_category', 'device_category', 'fio2']\n"
     ]
    }
   ],
   "source": [
    "# Define feature extraction configuration\n",
    "print(\"Configuring feature extraction...\")\n",
    "\n",
    "# Get hospitalization IDs from cohort\n",
    "cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()\n",
    "print(f\"Extracting features for {len(cohort_ids)} hospitalizations\")\n",
    "\n",
    "# Define category filters for each table\n",
    "category_filters = {\n",
    "    'vitals': [  # Common vital signs\n",
    "        'heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c',\n",
    "        'weight_kg', 'height_cm'\n",
    "    ],\n",
    "    'labs': [  # Common lab values\n",
    "        \"albumin\",    \"alkaline_phosphatase\",    \"alt\",    \"ast\",    \"basophils_percent\",    \"basophils_absolute\",    \"bicarbonate\",    \"bilirubin_total\",    \"bilirubin_conjugated\",    \"bilirubin_unconjugated\",\n",
    "    \"bun\",\n",
    "    \"calcium_total\",    \"calcium_ionized\",    \"chloride\",    \"creatinine\",    \"crp\",    \"eosinophils_percent\",\n",
    "    \"eosinophils_absolute\",    \"esr\",    \"ferritin\",    \"glucose_fingerstick\",    \"glucose_serum\",    \"hemoglobin\",    \"phosphate\",    \"inr\",    \"lactate\",    \"ldh\",\n",
    "    \"lymphocytes_percent\",    \"lymphocytes_absolute\",    \"magnesium\",    \"monocytes_percent\",    \"monocytes_absolute\",    \"neutrophils_percent\",    \"neutrophils_absolute\",\n",
    "    \"pco2_arterial\",    \"po2_arterial\",    \"pco2_venous\",    \"ph_arterial\",    \"ph_venous\",    \"platelet_count\",    \"potassium\",    \"procalcitonin\",\n",
    "    \"pt\",    \"ptt\",    \"so2_arterial\",    \"so2_mixed_venous\",    \"so2_central_venous\",    \"sodium\",\n",
    "    \"total_protein\",    \"troponin_i\",    \"troponin_t\",    \"wbc\"\n",
    "    ],\n",
    "    'patient_assessments': [  # Neurological assessments\n",
    "        'gcs_total', 'rass'\n",
    "    ],\n",
    "    'medication_admin_continuous': [  # Vasoactives and sedatives\n",
    "        \"norepinephrine\",\n",
    "    \"epinephrine\",\n",
    "    \"phenylephrine\",\n",
    "    \"angiotensin\",\n",
    "    \"vasopressin\",\n",
    "    \"dopamine\",\n",
    "    \"dobutamine\",\n",
    "    \"milrinone\",\n",
    "    \"isoproterenol\",\n",
    "    \"propofol\",\n",
    "    \"dexmedetomidine\",\n",
    "    \"ketamine\",\n",
    "    \"midazolam\",\n",
    "    \"fentanyl\",\n",
    "    \"hydromorphone\",\n",
    "    \"morphine\",\n",
    "    \"remifentanil\",\n",
    "    \"pentobarbital\",\n",
    "    \"lorazepam\"\n",
    "    ],\n",
    "    'respiratory_support': [  # All respiratory support categories\n",
    "        'mode_category', 'device_category', 'fio2'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Feature extraction configuration:\")\n",
    "for table, categories in category_filters.items():\n",
    "    print(f\"  {table}: {len(categories)} categories\")\n",
    "    print(f\"    {categories[:5]}...\" if len(categories) > 5 else f\"    {categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wide Dataset Using pyCLIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wide dataset using pyCLIF...\n",
      "Starting wide dataset creation...\n",
      "Base tables loaded - Patient: 100, Hospitalization: 275, ADT: 1136\n",
      "Filtering to specific hospitalization IDs: 89 encounters\n",
      "Base cohort created with 89 records\n",
      "Processing vitals: 73187 records\n",
      "Filtered vitals to categories: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 16362 records with 7 category columns\n",
      "Processing labs: 21810 records\n",
      "Filtered labs to categories: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_fingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 4670 records with 44 category columns\n",
      "Processing patient_assessments: 24937 records\n",
      "Warning: Required columns not found for pivoting patient_assessments\n",
      "Processing medication_admin_continuous: 5584 records\n",
      "Filtered medication_admin_continuous to categories: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 2897 records with 13 category columns\n",
      "Processing respiratory_support: 2701 records\n",
      "Found 27378 unique event timestamps\n",
      "Expanded cohort created with 27378 records\n",
      "Joined ADT data\n",
      "Joined pivoted vitals data\n",
      "Joined pivoted labs data\n",
      "Joined pivoted medication_admin_continuous data\n",
      "Joined respiratory_support data\n",
      "Final wide dataset created with 27380 records and 126 columns\n",
      "Added missing assessment column: sbt_delivery_pass_fail\n",
      "Added missing assessment column: sbt_screen_pass_fail\n",
      "Added missing assessment column: sat_delivery_pass_fail\n",
      "Added missing assessment column: sat_screen_pass_fail\n",
      "Added missing assessment column: rass\n",
      "Added missing assessment column: gcs_total\n",
      "Added missing medication column: angiotensin\n",
      "Added missing medication column: isoproterenol\n",
      "Added missing medication column: cisatracurium\n",
      "Added missing medication column: vecuronium\n",
      "Added missing medication column: rocuronium\n",
      "Added missing medication column: lorazepam\n",
      "Added missing labs category column: eosinophils_absolute\n",
      "Added missing labs category column: glucose_fingerstick\n",
      "Added missing labs category column: lymphocytes_absolute\n",
      "Added missing labs category column: monocytes_absolute\n",
      "Added missing labs category column: neutrophils_absolute\n",
      "Added missing labs category column: procalcitonin\n",
      "Added missing labs category column: troponin_i\n",
      "Added missing labs category column: wbc\n",
      "Added missing medication_admin_continuous category column: ketamine\n",
      "Added missing medication_admin_continuous category column: remifentanil\n",
      "Added missing medication_admin_continuous category column: pentobarbital\n",
      "Added missing respiratory_support category column: fio2\n"
     ]
    }
   ],
   "source": [
    "# Create wide dataset for cohort hospitalizations\n",
    "print(\"Creating wide dataset using pyCLIF...\")\n",
    "\n",
    "\n",
    "wide_df = clif.create_wide_dataset(\n",
    "    hospitalization_ids=cohort_ids,\n",
    "    optional_tables=['vitals', 'labs', 'patient_assessments', 'medication_admin_continuous', 'respiratory_support'],\n",
    "    category_filters=category_filters,\n",
    "    save_to_data_location=False  # Keep in memory for processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.to_csv(\"wide_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to 24-Hour Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to 24-hour windows...\n",
      "After merge with cohort: 0 records\n",
      "âœ… Filtered to 24-hour windows: 0 records\n",
      "Hospitalizations with data: 0\n",
      "\n",
      "Time window validation:\n",
      "All events within window: True\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTime window validation:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll events within window: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((wide_df_filtered[\u001b[33m'\u001b[39m\u001b[33mevent_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m>=\u001b[38;5;250m \u001b[39mwide_df_filtered[\u001b[33m'\u001b[39m\u001b[33mhour_24_start_dttm\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;250m \u001b[39m&\u001b[38;5;250m \u001b[39m(wide_df_filtered[\u001b[33m'\u001b[39m\u001b[33mevent_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m<=\u001b[38;5;250m \u001b[39mwide_df_filtered[\u001b[33m'\u001b[39m\u001b[33mhour_24_end_dttm\u001b[39m\u001b[33m'\u001b[39m])).all()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage records per hospitalization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwide_df_filtered\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m/\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mwide_df_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhospitalization_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "# Filter wide dataset to 24-hour windows\n",
    "print(\"Filtering to 24-hour windows...\")\n",
    "\n",
    "# Merge with cohort to get time windows\n",
    "wide_df_filtered = pd.merge(\n",
    "    wide_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(wide_df_filtered)} records\")\n",
    "\n",
    "# Filter events within 24-hour window\n",
    "wide_df_filtered = wide_df_filtered[\n",
    "    (wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) &\n",
    "    (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Filtered to 24-hour windows: {len(wide_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data: {wide_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation\n",
    "print(\"\\nTime window validation:\")\n",
    "print(f\"All events within window: {((wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) & (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(wide_df_filtered) / wide_df_filtered['hospitalization_id'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Respiratory Support One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare respiratory support for one-hot encoding\n",
    "print(\"Preparing respiratory support one-hot encoding...\")\n",
    "\n",
    "# Check for respiratory support categorical columns\n",
    "resp_categorical_cols = ['mode', 'device_category']\n",
    "resp_available_cols = [col for col in resp_categorical_cols if col in wide_df_filtered.columns]\n",
    "\n",
    "if resp_available_cols:\n",
    "    print(f\"Found respiratory support categorical columns: {resp_available_cols}\")\n",
    "    \n",
    "    # Show unique values for each categorical column\n",
    "    for col in resp_available_cols:\n",
    "        unique_vals = wide_df_filtered[col].dropna().unique()\n",
    "        print(f\"  {col}: {len(unique_vals)} unique values\")\n",
    "        print(f\"    Values: {list(unique_vals)[:10]}...\" if len(unique_vals) > 10 else f\"    Values: {list(unique_vals)}\")\n",
    "        \n",
    "        # Create one-hot encoded columns\n",
    "        for val in unique_vals:\n",
    "            if pd.notna(val) and val != '':\n",
    "                col_name = f\"{col}_{str(val).lower().replace(' ', '_').replace('/', '_').replace('-', '_')}\"\n",
    "                wide_df_filtered[col_name] = (wide_df_filtered[col] == val).astype(int)\n",
    "                print(f\"    Created: {col_name}\")\n",
    "    \n",
    "    print(f\"âœ… One-hot encoding completed for respiratory support\")\n",
    "else:\n",
    "    print(\"No respiratory support categorical columns found\")\n",
    "\n",
    "# Check total columns after one-hot encoding\n",
    "print(f\"\\nTotal columns after one-hot encoding: {len(wide_df_filtered.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hourly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hourly aggregation\n",
    "print(\"Configuring hourly aggregation...\")\n",
    "\n",
    "# Define columns for different aggregation methods\n",
    "continuous_cols = []\n",
    "categorical_cols = []\n",
    "boolean_cols = []\n",
    "one_hot_cols = []\n",
    "\n",
    "# Identify column types\n",
    "for col in wide_df_filtered.columns:\n",
    "    if col in ['hospitalization_id', 'patient_id', 'event_time', 'day_number', 'hosp_id_day_key', \n",
    "               'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']:\n",
    "        continue  # Skip meta columns\n",
    "    \n",
    "    # Check if it's a one-hot encoded column (from respiratory support)\n",
    "    if any(col.startswith(f\"{cat}_\") for cat in resp_available_cols):\n",
    "        one_hot_cols.append(col)\n",
    "    # Check if it's a medication (boolean)\n",
    "    elif col in category_filters.get('medication_admin_continuous', []):\n",
    "        boolean_cols.append(col)\n",
    "    # Check if it's categorical (original respiratory support columns)\n",
    "    elif col in resp_categorical_cols:\n",
    "        categorical_cols.append(col)\n",
    "    # Otherwise, treat as continuous\n",
    "    else:\n",
    "        continuous_cols.append(col)\n",
    "\n",
    "# Configure aggregation\n",
    "aggregation_config = {\n",
    "    'min': continuous_cols,\n",
    "    'max': continuous_cols,\n",
    "    'mean': continuous_cols,\n",
    "    'boolean': boolean_cols + one_hot_cols,  # Both medications and one-hot encoded columns\n",
    "    'last': categorical_cols  # For original categorical columns\n",
    "}\n",
    "\n",
    "print(f\"Aggregation configuration:\")\n",
    "for method, cols in aggregation_config.items():\n",
    "    if cols:\n",
    "        print(f\"  {method}: {len(cols)} columns\")\n",
    "        print(f\"    Examples: {cols[:5]}...\" if len(cols) > 5 else f\"    All: {cols}\")\n",
    "    else:\n",
    "        print(f\"  {method}: No columns\")\n",
    "\n",
    "print(f\"\\nâœ… Hourly aggregation configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Hourly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to hourly dataset\n",
    "print(\"Converting to hourly dataset...\")\n",
    "\n",
    "try:\n",
    "    hourly_df = convert_wide_to_hourly(wide_df_filtered, aggregation_config)\n",
    "    \n",
    "    print(f\"âœ… Hourly dataset created: {len(hourly_df)} rows, {len(hourly_df.columns)} columns\")\n",
    "    print(f\"Reduction: {len(wide_df_filtered)} â†’ {len(hourly_df)} rows ({(1-len(hourly_df)/len(wide_df_filtered))*100:.1f}% reduction)\")\n",
    "    print(f\"Hospitalizations: {hourly_df['hospitalization_id'].nunique()}\")\n",
    "    print(f\"Max nth_hour: {hourly_df['nth_hour'].max()} (â‰ˆ {hourly_df['nth_hour'].max():.1f} hours)\")\n",
    "    \n",
    "    # Show column naming convention\n",
    "    print(\"\\nColumn naming examples:\")\n",
    "    sample_cols = [col for col in hourly_df.columns if '_' in col and col not in ['hospitalization_id', 'patient_id', 'event_time']][:10]\n",
    "    for col in sample_cols:\n",
    "        print(f\"  {col}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error converting to hourly dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to Complete 24-Hour Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to encounters with complete 24-hour data\n",
    "print(\"Filtering to encounters with complete 24-hour data...\")\n",
    "\n",
    "# Count hours per hospitalization\n",
    "hours_per_hosp = hourly_df.groupby('hospitalization_id')['nth_hour'].nunique().reset_index()\n",
    "hours_per_hosp.columns = ['hospitalization_id', 'total_hours']\n",
    "\n",
    "print(f\"Hours per hospitalization distribution:\")\n",
    "print(hours_per_hosp['total_hours'].describe())\n",
    "\n",
    "# Filter to hospitalizations with at least 20 hours of data (allowing for some missing hours)\n",
    "min_hours_threshold = 20\n",
    "complete_hosp_ids = hours_per_hosp[hours_per_hosp['total_hours'] >= min_hours_threshold]['hospitalization_id'].unique()\n",
    "\n",
    "print(f\"\\nHospitalizations with â‰¥{min_hours_threshold} hours: {len(complete_hosp_ids)} / {len(hours_per_hosp)}\")\n",
    "\n",
    "# Filter hourly dataset\n",
    "hourly_df_complete = hourly_df[hourly_df['hospitalization_id'].isin(complete_hosp_ids)].copy()\n",
    "\n",
    "# Filter to first 24 hours only\n",
    "hourly_df_complete = hourly_df_complete[hourly_df_complete['nth_hour'] < 24].copy()\n",
    "\n",
    "print(f\"âœ… Final hourly dataset: {len(hourly_df_complete)} rows\")\n",
    "print(f\"Hospitalizations: {hourly_df_complete['hospitalization_id'].nunique()}\")\n",
    "print(f\"Hour range: {hourly_df_complete['nth_hour'].min()} to {hourly_df_complete['nth_hour'].max()}\")\n",
    "\n",
    "# Add outcome variable\n",
    "hourly_df_complete = pd.merge(\n",
    "    hourly_df_complete,\n",
    "    cohort_df[['hospitalization_id', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Mortality rate in final dataset: {hourly_df_complete['disposition'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature summary\n",
    "print(\"Creating feature summary...\")\n",
    "\n",
    "# Get feature columns (exclude meta columns)\n",
    "meta_cols = ['hospitalization_id', 'patient_id', 'nth_hour', 'event_time_hour', 'hour_bucket', 'disposition']\n",
    "feature_cols = [col for col in hourly_df_complete.columns if col not in meta_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Create feature summary\n",
    "feature_summary = []\n",
    "\n",
    "for col in feature_cols:\n",
    "    non_null_count = hourly_df_complete[col].notna().sum()\n",
    "    null_count = hourly_df_complete[col].isna().sum()\n",
    "    completeness = (non_null_count / len(hourly_df_complete)) * 100\n",
    "    \n",
    "    # Determine feature type\n",
    "    if col.endswith('_boolean'):\n",
    "        feature_type = 'boolean'\n",
    "        unique_vals = hourly_df_complete[col].dropna().unique()\n",
    "        stats = {'unique_values': len(unique_vals), 'positive_rate': hourly_df_complete[col].mean()}\n",
    "    elif col.endswith('_min') or col.endswith('_max') or col.endswith('_mean'):\n",
    "        feature_type = 'continuous'\n",
    "        stats = {\n",
    "            'mean': hourly_df_complete[col].mean(),\n",
    "            'std': hourly_df_complete[col].std(),\n",
    "            'min': hourly_df_complete[col].min(),\n",
    "            'max': hourly_df_complete[col].max()\n",
    "        }\n",
    "    elif col.endswith('_last'):\n",
    "        feature_type = 'categorical'\n",
    "        unique_vals = hourly_df_complete[col].dropna().unique()\n",
    "        stats = {'unique_values': len(unique_vals), 'most_common': hourly_df_complete[col].mode().iloc[0] if not hourly_df_complete[col].empty else None}\n",
    "    else:\n",
    "        feature_type = 'other'\n",
    "        stats = {}\n",
    "    \n",
    "    feature_summary.append({\n",
    "        'feature': col,\n",
    "        'type': feature_type,\n",
    "        'non_null_count': non_null_count,\n",
    "        'null_count': null_count,\n",
    "        'completeness_pct': completeness,\n",
    "        'stats': stats\n",
    "    })\n",
    "\n",
    "feature_summary_df = pd.DataFrame(feature_summary)\n",
    "\n",
    "print(f\"\\nFeature summary by type:\")\n",
    "print(feature_summary_df.groupby('type').size())\n",
    "\n",
    "print(f\"\\nTop 10 most complete features:\")\n",
    "top_complete = feature_summary_df.nlargest(10, 'completeness_pct')\n",
    "for _, row in top_complete.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['completeness_pct']:.1f}% ({row['type']})\")\n",
    "\n",
    "print(f\"\\nFeatures with >50% completeness: {(feature_summary_df['completeness_pct'] > 50).sum()}\")\n",
    "print(f\"Features with >80% completeness: {(feature_summary_df['completeness_pct'] > 80).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature Dataset and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hourly features dataset\n",
    "print(\"Saving feature dataset and summary...\")\n",
    "\n",
    "# Save main hourly dataset\n",
    "hourly_output_path = os.path.join('output', 'intermitted', 'hourly_features_24hr.csv')\n",
    "hourly_df_complete.to_csv(hourly_output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Hourly features saved to: {hourly_output_path}\")\n",
    "print(f\"File size: {os.path.getsize(hourly_output_path) / (1024*1024):.1f} MB\")\n",
    "print(f\"Shape: {hourly_df_complete.shape}\")\n",
    "\n",
    "# Save feature summary\n",
    "summary_output_path = os.path.join('output', 'intermitted', 'feature_summary.csv')\n",
    "feature_summary_df.to_csv(summary_output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Feature summary saved to: {summary_output_path}\")\n",
    "\n",
    "# Save high-level statistics\n",
    "stats = {\n",
    "    'total_hospitalizations': int(hourly_df_complete['hospitalization_id'].nunique()),\n",
    "    'total_hourly_records': int(len(hourly_df_complete)),\n",
    "    'total_features': len(feature_cols),\n",
    "    'mortality_rate': float(hourly_df_complete['disposition'].mean()),\n",
    "    'avg_records_per_hosp': float(len(hourly_df_complete) / hourly_df_complete['hospitalization_id'].nunique()),\n",
    "    'hour_range': [int(hourly_df_complete['nth_hour'].min()), int(hourly_df_complete['nth_hour'].max())],\n",
    "    'features_by_type': feature_summary_df.groupby('type').size().to_dict(),\n",
    "    'high_completeness_features': int((feature_summary_df['completeness_pct'] > 80).sum()),\n",
    "    'processing_info': {\n",
    "        'original_wide_records': len(wide_df_filtered),\n",
    "        'hourly_records': len(hourly_df_complete),\n",
    "        'reduction_pct': float((1 - len(hourly_df_complete) / len(wide_df_filtered)) * 100),\n",
    "        'min_hours_threshold': min_hours_threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_path = os.path.join('output', 'intermitted', 'feature_stats.json')\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Feature statistics saved to: {stats_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Feature engineering completed successfully!\")\n",
    "print(f\"\\nðŸ“Š Final Dataset Summary:\")\n",
    "print(f\"  - Hospitalizations: {stats['total_hospitalizations']:,}\")\n",
    "print(f\"  - Hourly records: {stats['total_hourly_records']:,}\")\n",
    "print(f\"  - Features: {stats['total_features']:,}\")\n",
    "print(f\"  - Mortality rate: {stats['mortality_rate']:.3f}\")\n",
    "print(f\"  - Hours per patient: {stats['avg_records_per_hosp']:.1f}\")\n",
    "print(f\"  - High-quality features (>80% complete): {stats['high_completeness_features']:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
