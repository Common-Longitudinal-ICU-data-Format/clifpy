{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>CLIFpy is a Python package that implements the Common Longitudinal ICU data Format (CLIF) specification. It provides a standardized interface for working with critical care data in the CLIF format, enabling healthcare researchers and data scientists to analyze ICU data across different healthcare systems.</p> <ul> <li> <p>\ud83d\ude80 Getting Started</p> <p>Install with <code>pip</code> and get started with CLIFpy with these tutorials.</p> </li> <li> <p>\ud83d\udcd6 User Guide</p> <p>In depth explanation and discussion of the concepts and working of different features available in CLIFpy.</p> </li> <li> <p>\ud83d\udd28 How-to Guides</p> <p>Practical guides to help you achieve specific goals. Take a look at these guides to learn how to use CLIFpy to solve real-world problems.</p> </li> <li> <p>\ud83d\udcc4 API Reference</p> <p>Technical descriptions of how CLIFpy classes and methods work.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>CLIFpy is released under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"BaseTable/","title":"BaseTable class for pyCLIF tables.","text":"<p>This module provides the base class that all pyCLIF table classes inherit from. It handles common functionality including data loading, validation, and reporting.</p>"},{"location":"BaseTable/#overview","title":"Overview","text":"<p>The <code>BaseTable</code> class is the foundational component of all table classes in clifpy. It implements the common functionality that every CLIF table needs, following an inheritance pattern where specific tables (patient, hospitalization, adt, etc.) inherit from it.</p>"},{"location":"BaseTable/#design-pattern","title":"Design Pattern","text":"<p>BaseTable implements the Template Method Pattern where: - Common behavior is defined in the base class - Specific behavior is implemented in child classes - Extensibility is provided through method overriding</p>"},{"location":"BaseTable/#core-responsibilities","title":"Core Responsibilities","text":""},{"location":"BaseTable/#1-data-management","title":"1. Data Management","text":"<pre><code># Stores the actual data\nself.df: pd.DataFrame = None\n\n# Configuration\nself.data_directory: str    # Path to data files\nself.filetype: str         # File format (parquet, csv, etc.)\nself.timezone: str         # Timezone for datetime conversions\nself.output_directory: str # Where to save validation outputs\n</code></pre>"},{"location":"BaseTable/#2-schema-management","title":"2. Schema Management","text":"<pre><code># Automatically loads YAML schema based on table name\nself.schema: Dict = None  # Loaded from clifpy/schemas/{table_name}_schema.yaml\n</code></pre>"},{"location":"BaseTable/#3-validation-system","title":"3. Validation System","text":"<pre><code>self.errors: List[Dict] = []  # Stores validation errors and warnings\n</code></pre>"},{"location":"BaseTable/#4-logging-system","title":"4. Logging System","text":"<pre><code>self.logger: logging.Logger  # Per-table logger that writes to output directory\n</code></pre>"},{"location":"BaseTable/#key-methods","title":"Key Methods","text":""},{"location":"BaseTable/#constructor-__init__","title":"Constructor (<code>__init__</code>)","text":"<pre><code>def __init__(\n    self,\n    data_directory: str,\n    filetype: str, \n    timezone: str,\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n)\n</code></pre> <p>Parameters: - <code>data_directory</code>: Path to directory containing data files - <code>filetype</code>: File format (\"parquet\", \"csv\", etc.) - <code>timezone</code>: Timezone for datetime columns (default: \"UTC\")  - <code>output_directory</code>: Where to save validation outputs (optional) - <code>data</code>: Pre-loaded DataFrame (optional)</p> <p>What it does: 1. Stores configuration parameters 2. Sets up output directory (creates if doesn't exist) 3. Determines table name from class name 4. Sets up logging system 5. Loads YAML schema for the table 6. If data provided, automatically runs validation</p>"},{"location":"BaseTable/#class-method-from_file","title":"Class Method: <code>from_file()</code>","text":"<pre><code>@classmethod\ndef from_file(\n    cls,\n    data_directory: str,\n    filetype: str,\n    timezone: str = \"UTC\", \n    output_directory: Optional[str] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n)\n</code></pre> <p>Alternative constructor that loads data from files with additional options: - <code>sample_size</code>: Limit number of rows to load - <code>columns</code>: Only load specific columns - <code>filters</code>: Apply filters during loading</p> <p>Example: <pre><code>patient_table = patient.from_file(\n    data_directory=\"/path/to/data\",\n    filetype=\"parquet\",\n    timezone=\"US/Eastern\",\n    sample_size=1000  # Load only first 1000 rows\n)\n</code></pre></p>"},{"location":"BaseTable/#validation-methods","title":"Validation Methods","text":""},{"location":"BaseTable/#validate","title":"<code>validate()</code>","text":"<p>Runs comprehensive validation on the loaded data: - Schema validation (required columns, data types, categories) - Enhanced validation (missing data, duplicates, statistics) - Table-specific validation (can be overridden by child classes)</p>"},{"location":"BaseTable/#isvalid-bool","title":"<code>isvalid() -&gt; bool</code>","text":"<p>Returns <code>True</code> if no errors were found in the last validation run.</p> <pre><code>if table.isvalid():\n    print(\"\u2705 Data passed all validations!\")\nelse:\n    print(f\"\u274c Found {len(table.errors)} validation issues\")\n</code></pre>"},{"location":"BaseTable/#how-tables-inherit-from-basetable","title":"How Tables Inherit from BaseTable","text":""},{"location":"BaseTable/#basic-inheritance-pattern","title":"Basic Inheritance Pattern","text":"<pre><code>class patient(BaseTable):\n    \"\"\"Patient table with demographic information.\"\"\"\n\n    def __init__(self, data_directory: str = None, filetype: str = None, \n                 timezone: str = \"UTC\", output_directory: Optional[str] = None,\n                 data: Optional[pd.DataFrame] = None):\n        # Handle backward compatibility\n        if data_directory is None and filetype is None and data is not None:\n            data_directory = \".\"\n            filetype = \"parquet\" \n\n        # Call parent constructor\n        super().__init__(\n            data_directory=data_directory,\n            filetype=filetype, \n            timezone=timezone,\n            output_directory=output_directory,\n            data=data\n        )\n\n    # Add patient-specific methods\n    def get_demographics_summary(self):\n        \"\"\"Return demographic breakdown of patients.\"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"BaseTable/#table-specific-methods","title":"Table-Specific Methods","text":"<p>Child classes can add methods specific to their domain:</p> <pre><code># hospitalization.py\nclass hospitalization(BaseTable):\n    def get_mortality_rate(self) -&gt; float:\n        \"\"\"Calculate in-hospital mortality rate.\"\"\"\n        if 'discharge_category' not in self.df.columns:\n            return 0.0\n        total = len(self.df)\n        expired = len(self.df[self.df['discharge_category'] == 'Expired'])\n        return (expired / total) * 100 if total &gt; 0 else 0.0\n\n    def calculate_length_of_stay(self) -&gt; pd.DataFrame:\n        \"\"\"Calculate length of stay for each hospitalization.\"\"\"\n        # Implementation here\n        pass\n\n# adt.py  \nclass adt(BaseTable):\n    def get_location_categories(self) -&gt; List[str]:\n        \"\"\"Return unique location categories.\"\"\"\n        if 'location_category' not in self.df.columns:\n            return []\n        return self.df['location_category'].dropna().unique().tolist()\n\n    def filter_by_location_category(self, location: str) -&gt; pd.DataFrame:\n        \"\"\"Filter records by location category (e.g., 'icu', 'ward').\"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"BaseTable/#validation-flow","title":"Validation Flow","text":"<p>When you create a table instance, BaseTable automatically:</p> <ol> <li>Schema Loading: Reads <code>{table_name}_schema.yaml</code> from the schemas directory</li> <li>Logging Setup: Creates log files in the output directory</li> <li>Data Validation (if data provided):</li> <li>\u2705 Required columns - Ensures all mandatory columns are present</li> <li>\u2705 Data types - Validates column data types match schema</li> <li>\u2705 Categorical values - Checks values against permitted categories</li> <li>\u2705 Datetime timezones - Validates timezone-aware datetime columns</li> <li>\u2705 Missing data analysis - Calculates missing data statistics</li> <li>\u2705 Duplicate detection - Checks composite keys for uniqueness</li> <li>\u2705 Statistical analysis - Generates summaries and skewness analysis</li> <li>\u2705 Unit validation - For tables like vitals/labs, validates measurement units</li> <li>\u2705 Numeric ranges - Checks values fall within expected clinical ranges</li> </ol>"},{"location":"BaseTable/#output-files-generated","title":"Output Files Generated","text":"<p>BaseTable creates several files during validation in the output directory:</p> File Type Example Purpose Log files <code>validation_log_patient.log</code> Detailed validation logs with timestamps Missing data <code>missing_data_stats_patient.csv</code> Missing value counts and percentages Statistics <code>summary_statistics_patient.csv</code> Q1, Q3, median for numeric columns Skewness <code>skewness_analysis_patient.csv</code> Distribution analysis for numeric columns Validation errors <code>validation_errors_patient.csv</code> Summary of all validation issues"},{"location":"BaseTable/#usage-examples","title":"Usage Examples","text":""},{"location":"BaseTable/#method-1-direct-instantiation-with-data","title":"Method 1: Direct Instantiation with Data","text":"<pre><code># When you already have a DataFrame\npatient_table = patient(\n    data_directory=\"./data\",      # Required for schema/logging\n    filetype=\"parquet\",           # Required for metadata\n    timezone=\"UTC\",               # Timezone for datetime columns  \n    output_directory=\"./output\",  # Where to save validation files\n    data=my_dataframe            # Your pre-loaded DataFrame\n)\n</code></pre>"},{"location":"BaseTable/#method-2-load-from-file","title":"Method 2: Load from File","text":"<pre><code># Load data from files\npatient_table = patient.from_file(\n    data_directory=\"./data\",\n    filetype=\"parquet\", \n    timezone=\"US/Eastern\",\n    columns=['patient_id', 'age_at_admission', 'sex_category'],  # Only load specific columns\n    sample_size=5000  # Only load first 5000 rows\n)\n</code></pre>"},{"location":"BaseTable/#method-3-demo-data-recommended-for-learning","title":"Method 3: Demo Data (Recommended for Learning)","text":"<pre><code># Use built-in demo datasets\nfrom clifpy.data import load_demo_patient\n\npatient_table = load_demo_patient()  # Uses Method 1 internally\n</code></pre>"},{"location":"BaseTable/#all-methods-result-in-same-capabilities","title":"All Methods Result in Same Capabilities:","text":"<pre><code># Check validation status\nprint(f\"Valid: {patient_table.isvalid()}\")\nprint(f\"Errors: {len(patient_table.errors)}\")  \nprint(f\"Records: {len(patient_table.df)}\")\n\n# Access the data\ndf = patient_table.df\nprint(df.head())\n\n# Use table-specific methods (if implemented)\nif hasattr(patient_table, 'get_demographics_summary'):\n    demographics = patient_table.get_demographics_summary()\n</code></pre>"},{"location":"BaseTable/#benefits-of-this-design","title":"Benefits of This Design","text":""},{"location":"BaseTable/#1-code-reuse","title":"1. Code Reuse","text":"<ul> <li>All tables get validation, logging, and schema loading automatically</li> <li>No duplicate code across table implementations</li> <li>Consistent behavior across all table types</li> </ul>"},{"location":"BaseTable/#2-consistency","title":"2. Consistency","text":"<ul> <li>Same API across all table types: <code>validate()</code>, <code>isvalid()</code>, <code>from_file()</code></li> <li>Standardized output file formats and naming conventions</li> <li>Uniform error handling and logging</li> </ul>"},{"location":"BaseTable/#3-extensibility","title":"3. Extensibility","text":"<ul> <li>Easy to add new tables by inheriting from BaseTable</li> <li>Can override specific methods for table-specific behavior</li> <li>Template method pattern allows customization while preserving structure</li> </ul>"},{"location":"BaseTable/#4-separation-of-concerns","title":"4. Separation of Concerns","text":"<ul> <li>BaseTable: Infrastructure (validation, logging, I/O, schema management)</li> <li>Child classes: Domain-specific methods and business logic</li> <li>Validator module: Reusable validation functions</li> <li>Schema files: Data structure definitions</li> </ul>"},{"location":"BaseTable/#5-maintainability","title":"5. Maintainability","text":"<ul> <li>Changes to validation logic automatically apply to all tables</li> <li>Schema changes are managed in separate YAML files  </li> <li>Logging and error handling centralized in one place</li> </ul>"},{"location":"BaseTable/#advanced-features","title":"Advanced Features","text":""},{"location":"BaseTable/#custom-validation","title":"Custom Validation","text":"<p>Tables can override validation methods for specific requirements:</p> <pre><code>class vitals(BaseTable):\n    def _run_table_specific_validations(self):\n        \"\"\"Add vitals-specific validation rules.\"\"\"\n        super()._run_table_specific_validations()\n\n        # Custom validation for vital signs ranges\n        if 'vital_category' in self.df.columns and 'vital_value' in self.df.columns:\n            # Check for physiologically impossible values\n            extreme_values = self.df[\n                (self.df['vital_category'] == 'heart_rate') &amp; \n                ((self.df['vital_value'] &lt; 0) | (self.df['vital_value'] &gt; 300))\n            ]\n            if not extreme_values.empty:\n                self.errors.append({\n                    \"type\": \"extreme_vital_values\",\n                    \"message\": f\"Found {len(extreme_values)} extreme heart rate values\",\n                    \"count\": len(extreme_values)\n                })\n</code></pre>"},{"location":"BaseTable/#custom-output-methods","title":"Custom Output Methods","text":"<pre><code>class hospitalization(BaseTable):\n    def save_mortality_report(self, filename: str = None):\n        \"\"\"Save detailed mortality analysis to file.\"\"\"\n        if filename is None:\n            filename = os.path.join(self.output_directory, f'mortality_report_{self.table_name}.csv')\n\n        mortality_data = self.analyze_mortality_by_demographics()\n        mortality_data.to_csv(filename, index=False)\n        self.logger.info(f\"Saved mortality report to {filename}\")\n</code></pre>"},{"location":"BaseTable/#best-practices","title":"Best Practices","text":""},{"location":"BaseTable/#1-when-to-inherit-from-basetable","title":"1. When to Inherit from BaseTable","text":"<ul> <li>\u2705 Always for CLIF table implementations</li> <li>\u2705 When you need validation and schema management</li> <li>\u2705 For tables that will be used in production workflows</li> </ul>"},{"location":"BaseTable/#2-method-naming-conventions","title":"2. Method Naming Conventions","text":"<ul> <li>Use descriptive names: <code>get_mortality_rate()</code> not <code>mortality()</code></li> <li>Follow existing patterns: <code>filter_by_*()</code>, <code>get_*()</code>, <code>calculate_*()</code></li> <li>Return appropriate types: DataFrames for subsets, numbers for metrics</li> </ul>"},{"location":"BaseTable/#3-error-handling","title":"3. Error Handling","text":"<pre><code>def custom_method(self):\n    \"\"\"Custom analysis method with proper error handling.\"\"\"\n    try:\n        if self.df is None or self.df.empty:\n            self.logger.warning(\"No data available for analysis\")\n            return None\n\n        # Your analysis here\n        result = self.df.groupby('category').mean()\n\n        self.logger.info(f\"Analysis completed successfully with {len(result)} groups\")\n        return result\n\n    except Exception as e:\n        self.logger.error(f\"Analysis failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"BaseTable/#future-enhancements","title":"Future Enhancements","text":"<p>The BaseTable design supports future enhancements such as:</p> <ul> <li>Caching: Store validation results to avoid re-computation</li> <li>Streaming: Handle large datasets that don't fit in memory  </li> <li>Parallel processing: Run validation checks in parallel</li> <li>Custom validators: Plugin system for domain-specific validation rules</li> <li>Data lineage: Track data transformations and sources</li> <li>Version control: Schema versioning and migration support</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to CLIFpy will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive documentation with MkDocs</li> <li>Enhanced docstrings for all modules and classes</li> <li>API reference documentation</li> <li>User guide and examples</li> </ul>"},{"location":"changelog/#001-2024-01-xx","title":"0.0.1 - 2024-01-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release of CLIFpy</li> <li>Core implementation of CLIF 2.0.0 specification</li> <li>All 9 CLIF table implementations:</li> <li>Patient demographics</li> <li>ADT (Admission, Discharge, Transfer)</li> <li>Hospitalization</li> <li>Laboratory results</li> <li>Vital signs</li> <li>Respiratory support</li> <li>Continuous medication administration</li> <li>Patient assessments</li> <li>Patient positioning</li> <li>Data validation against mCIDE schemas</li> <li>Timezone handling and conversion</li> <li>ClifOrchestrator for multi-table management</li> <li>Comprehensive test suite</li> <li>Demo dataset based on MIMIC-IV</li> <li>Example notebooks</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Load data from CSV or Parquet files</li> <li>Schema-based validation</li> <li>Advanced filtering and querying</li> <li>Clinical calculations</li> <li>Summary statistics and reporting</li> <li>Memory-efficient data loading options</li> </ul>"},{"location":"contributing/","title":"Contributing to CLIFpy","text":"<p>We welcome contributions to CLIFpy! This guide will help you get started.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/CLIFpy.git\ncd CLIFpy\n</code></pre></li> <li>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></li> <li>Install in development mode with all dependencies:    <pre><code>pip install -e \".[docs]\"\n</code></pre></li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a new branch for your feature or fix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and ensure:</p> </li> <li>Code follows the existing style</li> <li>All tests pass</li> <li>New features include tests</li> <li> <p>Documentation is updated</p> </li> <li> <p>Run tests:    <pre><code>pytest tests/\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"feat: add new feature\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add type hints where appropriate</li> <li>Include docstrings for all public functions and classes</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update docstrings for any API changes</li> <li>Add examples to docstrings where helpful</li> <li>Update user guide if adding new features</li> <li>Build docs locally to verify:   <pre><code>mkdocs serve\n</code></pre></li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure all tests pass before submitting PR</li> <li>Aim for high test coverage</li> <li>Use pytest fixtures for common test data</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits format: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation changes - <code>test:</code> - Test additions or changes - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance tasks</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue for bugs or feature requests</li> <li>Join discussions in existing issues</li> <li>Reach out to maintainers if you need help</li> </ul> <p>Thank you for contributing to CLIFpy!</p>"},{"location":"prompts/","title":"<code>utils</code>","text":""},{"location":"prompts/#unit_converter","title":"<code>unit_converter</code>","text":"<p>Implement the following tests in tests/tables/test_medication_admin_continuous.py following instructions in their docstrings: - test_acceptable_dose_unit_patterns - test_standardize_dose_unit_pattern - test_convert_dose_to_same_units</p> <p>Use pytest features to make the tests more readable and less verbose. If helpful, follow the pattern of existing tests in the same file.</p> <p>the overall pattern looks good but please improve on the following issues: 1. the test data are not easily human readable and verificable. Can you create them as .csv's under  tests/fixtures? the csv would look like the output df which contains the original df and the newly generated appended columns. e.g. for test_convert_dose_to_same_units, the csv would have the following columns: ['med_dose', 'med_dose_unit_clean', 'weight_kg', 'med_dose_converted', 'med_dose_unit_converted'] 2. within the same test function, we are testing multiple scenarios. Does it make sense to split them up into separate test functions? I'm not sure about it either way. I think splitting them up seems like a good pattern but I worry about code duplication and overcomplicating things. Can you advise and implement what you think would be a preferred strategy, esp. taking into account the first issue above?</p> <p>it looks good overall! but would it make sense to consolidate related test data into the same csv's and  just add a new column like 'case' to differentiate between the different scenarios? i.e. I think i'd make sense for me to be able to view the valid and invalid unit cases at the same time in the same csv, and we can just load the csv as a whole and then filter by 'case'. Of course, for tests that are rather distinct and would require different columns, they should be still in different csv's.</p> <ol> <li>remove dose_unit_patterns.csv as that test does not necessarily need a tabular structure and the  test data can be easily understood as two lists written in the .py file directly.</li> <li>rename the csv to be aligned with the test names</li> <li>modify column names in the csv's to be exactly aligned with the dataframes, e.g. 'expected_clean' should be 'med_dose_unit_clean'; 'expected_dose_converted' should just be 'med_dose_converted'</li> <li>remove unnecessary columns in the csv's such as 'med_category' which does not impact unit conversion at all. </li> </ol> <p>Based on these two files: @tests/tables/test_medication_admin_continuous.py                                            \u2502 @clifpy/tables/medication_admin_continuous.py 1. Add a test(s) for when no med_df was ever provided (self.df was not even populated), triggering the ValueError(\"No data provided\") 2. Flesh out the doc strings for all functions</p> <ol> <li>can you make the file called med-unit-conversion.md to avoid confusion\\</li> <li>can you create a table for all the acceptable units? the columns would be: unit class; unit subclass; _clean_unit; acceptable variations;_base_unit. and explain where you see fit what they mean, i.e. unit class = rate or amount; unit subclass = mass, volume, unit; _clean_unit = the format in which user need to write their preferred units; acceptable variations = all of 'mL/m' 'ml/min' 'milli-liter/minute' would be converted to the 'ml/min'. finally mention that the unit class and subclass are used to track if units are conversion to one each other. </li> <li>introduce how the _convert_status column should be used and how any failure in conversion would result in the original dose and _clean_unit being the *_converted output columns and the override = True option can be used to bypass detection of unacceptable conversion\\</li> <li>highlight both the public functions give two outputs: a df with converted dose and units and a df that lists the conversion status and count by med_category, med_unit, _base_unit, etc etc.</li> </ol>"},{"location":"wide_dataset/","title":"Wide Dataset Creation with pyCLIF","text":"<p>The pyCLIF library now includes powerful functionality for creating wide datasets by joining multiple CLIF tables with automatic pivoting of category-based data. This feature is designed to replicate and enhance the sophisticated wide dataset creation logic from the original notebook while making it reusable and configurable.</p>"},{"location":"wide_dataset/#overview","title":"Overview","text":"<p>The wide dataset functionality allows you to: - Automatically join multiple CLIF tables (patient, hospitalization, ADT, and optional tables) - Pivot category-based data from vitals, labs, medications, and assessments - Sample or filter hospitalizations for targeted analysis - Handle time-based alignment of events across different tables - Save results in multiple formats (DataFrame, CSV, Parquet)</p>"},{"location":"wide_dataset/#quick-start","title":"Quick Start","text":"<pre><code>from pyclif import CLIF\n\n# Initialize with your data\nclif = CLIF(\n    data_dir=\"/path/to/CLIF_data\",\n    filetype='parquet',\n    timezone=\"US/Eastern\"\n)\n\n# Create a wide dataset (tables auto-loaded as needed)\nwide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['map', 'heart_rate', 'spo2'],\n        'labs': ['hemoglobin', 'wbc', 'sodium']\n    },\n    sample=True  # 20 random hospitalizations\n)\n</code></pre>"},{"location":"wide_dataset/#core-functionality","title":"Core Functionality","text":""},{"location":"wide_dataset/#base-tables-always-included","title":"Base Tables (Always Included)","text":"<ul> <li>patient: Demographics and patient information</li> <li>hospitalization: Admission/discharge details</li> <li>adt: Admission, discharge, and transfer events</li> </ul>"},{"location":"wide_dataset/#optional-tables-user-specified","title":"Optional Tables (User-Specified)","text":"<ul> <li>vitals: Vital signs (pivoted by <code>vital_category</code>)</li> <li>labs: Laboratory results (pivoted by <code>lab_category</code>)</li> <li>medication_admin_continuous: Continuous medications (pivoted by <code>med_category</code>)</li> <li>patient_assessments: Clinical assessments (pivoted by <code>assessment_category</code>)</li> <li>respiratory_support: Respiratory support data</li> </ul>"},{"location":"wide_dataset/#parameters","title":"Parameters","text":""},{"location":"wide_dataset/#create_wide_dataset-parameters","title":"<code>create_wide_dataset()</code> Parameters","text":"Parameter Type Default Description <code>optional_tables</code> List[str] None List of optional tables to include <code>category_filters</code> Dict[str, List[str]] None Categories to pivot for each table <code>sample</code> bool False If True, randomly select 20 hospitalizations <code>hospitalization_ids</code> List[str] None Specific hospitalization IDs to include <code>output_format</code> str 'dataframe' Output format: 'dataframe', 'csv', 'parquet' <code>save_to_data_location</code> bool False Save output to data directory <code>output_filename</code> str None Custom filename (auto-generated if None) <code>auto_load</code> bool True Automatically load missing tables"},{"location":"wide_dataset/#usage-examples","title":"Usage Examples","text":""},{"location":"wide_dataset/#example-1-sample-mode","title":"Example 1: Sample Mode","text":"<p>Create a wide dataset with 20 random hospitalizations:</p> <pre><code>wide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['map', 'heart_rate', 'spo2', 'respiratory_rate'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium']\n    },\n    sample=True,\n    save_to_data_location=True,\n    output_format='parquet'\n)\n</code></pre>"},{"location":"wide_dataset/#example-2-specific-hospitalizations","title":"Example 2: Specific Hospitalizations","text":"<p>Target specific encounters for analysis:</p> <pre><code>target_ids = ['12345', '67890', '11111']\nwide_df = clif.create_wide_dataset(\n    hospitalization_ids=target_ids,\n    optional_tables=['medication_admin_continuous', 'patient_assessments'],\n    category_filters={\n        'medication_admin_continuous': ['norepinephrine', 'propofol', 'fentanyl'],\n        'patient_assessments': ['gcs_total', 'rass', 'sbt_delivery_pass_fail']\n    },\n    output_filename='targeted_encounters'\n)\n</code></pre>"},{"location":"wide_dataset/#example-3-comprehensive-dataset","title":"Example 3: Comprehensive Dataset","text":"<p>Create a full wide dataset with all optional tables:</p> <pre><code>wide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs', 'medication_admin_continuous', 'patient_assessments'],\n    category_filters={\n        'vitals': ['map', 'heart_rate', 'spo2', 'respiratory_rate', 'temp_c'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium', 'creatinine'],\n        'medication_admin_continuous': ['norepinephrine', 'epinephrine', 'propofol'],\n        'patient_assessments': ['gcs_total', 'rass', 'sbt_delivery_pass_fail']\n    },\n    save_to_data_location=True\n)\n</code></pre>"},{"location":"wide_dataset/#available-categories","title":"Available Categories","text":""},{"location":"wide_dataset/#vitals-categories","title":"Vitals Categories","text":"<p>Common vital sign categories include: - <code>map</code>, <code>heart_rate</code>, <code>sbp</code>, <code>dbp</code>, <code>spo2</code>, <code>respiratory_rate</code>, <code>temp_c</code>, <code>weight_kg</code>, <code>height_cm</code></p>"},{"location":"wide_dataset/#labs-categories","title":"Labs Categories","text":"<p>Common laboratory categories include: - <code>hemoglobin</code>, <code>wbc</code>, <code>sodium</code>, <code>potassium</code>, <code>creatinine</code>, <code>bun</code>, <code>glucose</code>, <code>lactate</code></p>"},{"location":"wide_dataset/#medication-categories","title":"Medication Categories","text":"<p>Common continuous medication categories include: - <code>norepinephrine</code>, <code>epinephrine</code>, <code>phenylephrine</code>, <code>vasopressin</code>, <code>dopamine</code> - <code>propofol</code>, <code>fentanyl</code>, <code>midazolam</code>, <code>lorazepam</code>, <code>morphine</code></p>"},{"location":"wide_dataset/#assessment-categories","title":"Assessment Categories","text":"<p>Common assessment categories include: - <code>gcs_total</code>, <code>rass</code>, <code>sbt_delivery_pass_fail</code>, <code>sat_delivery_pass_fail</code> - <code>sbt_screen_pass_fail</code>, <code>sat_screen_pass_fail</code></p>"},{"location":"wide_dataset/#output-structure","title":"Output Structure","text":"<p>The resulting wide dataset includes:</p>"},{"location":"wide_dataset/#core-columns","title":"Core Columns","text":"<ul> <li>Patient demographics (<code>patient_id</code>, <code>sex_category</code>, <code>race_category</code>, etc.)</li> <li>Hospitalization details (<code>hospitalization_id</code>, <code>admission_dttm</code>, <code>discharge_dttm</code>, etc.)</li> <li>Event timing (<code>event_time</code>, <code>day_number</code>, <code>hosp_id_day_key</code>)</li> <li>Location information (from ADT table)</li> </ul>"},{"location":"wide_dataset/#pivoted-columns","title":"Pivoted Columns","text":"<ul> <li>Individual columns for each specified category (e.g., <code>map</code>, <code>heart_rate</code>, <code>norepinephrine</code>)</li> <li>Values aligned by timestamp and hospitalization</li> </ul>"},{"location":"wide_dataset/#time-based-features","title":"Time-Based Features","text":"<ul> <li><code>day_number</code>: Sequential day number within each hospitalization</li> <li><code>hosp_id_day_key</code>: Unique identifier combining hospitalization and day</li> <li><code>event_time</code>: Timestamp for each record</li> </ul>"},{"location":"wide_dataset/#auto-loading-feature","title":"Auto-Loading Feature","text":"<p>The function automatically loads required tables if they haven't been loaded yet:</p> <pre><code># No need to manually load tables\nclif = CLIF(data_dir=\"/path/to/data\", filetype='parquet')\n\n# Tables will be auto-loaded as needed\nwide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs']  # These will be loaded automatically\n)\n</code></pre>"},{"location":"wide_dataset/#error-handling","title":"Error Handling","text":"<p>The function includes robust error handling:</p> <ul> <li>Missing tables: Warns and skips if optional tables aren't available</li> <li>Missing columns: Handles alternative timestamp column names</li> <li>Missing categories: Adds NaN columns for standard assessments/medications</li> <li>Empty data: Gracefully handles cases where no data remains after filtering</li> </ul>"},{"location":"wide_dataset/#performance-considerations","title":"Performance Considerations","text":""},{"location":"wide_dataset/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Use <code>sample=True</code> for testing and development</li> <li>Specify <code>hospitalization_ids</code> for targeted analysis</li> <li>Use <code>category_filters</code> to limit pivoted columns</li> </ul>"},{"location":"wide_dataset/#output-management","title":"Output Management","text":"<ul> <li>Use <code>save_to_data_location=True</code> for large datasets</li> <li>Choose <code>output_format='parquet'</code> for better compression</li> <li>Set <code>output_format='dataframe'</code> only for immediate analysis</li> </ul>"},{"location":"wide_dataset/#implementation-details","title":"Implementation Details","text":""},{"location":"wide_dataset/#temporal-alignment","title":"Temporal Alignment","text":"<p>The function creates a unified timeline by: 1. Collecting all unique timestamps from included tables 2. Creating a cartesian product of hospitalizations \u00d7 timestamps 3. Joining table-specific data based on matching timestamps</p>"},{"location":"wide_dataset/#pivoting-logic","title":"Pivoting Logic","text":"<p>Category-based tables are pivoted using DuckDB for performance: - Creates unique combination IDs (<code>hospitalization_id_YYYYMMDDHHMM</code>) - Pivots on category columns using <code>PIVOT</code> SQL operation - Handles missing values and duplicate timestamps</p>"},{"location":"wide_dataset/#data-integration","title":"Data Integration","text":"<p>Tables are joined using a combination of: - Hospitalization IDs for patient-level data - Timestamp-based combo IDs for time-series data - Left joins to preserve all timestamps</p>"},{"location":"wide_dataset/#best-practices","title":"Best Practices","text":"<ol> <li>Start Small: Use <code>sample=True</code> for initial testing</li> <li>Filter Categories: Specify only needed categories to reduce memory usage</li> <li>Save Large Datasets: Use file output for datasets &gt; 1GB</li> <li>Check Data Quality: Validate timestamp alignment and missing values</li> <li>Document Choices: Record which categories and filters were used</li> </ol>"},{"location":"wide_dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"wide_dataset/#common-issues","title":"Common Issues","text":"<p>Memory Errors <pre><code># Use sampling or filtering\nwide_df = clif.create_wide_dataset(sample=True)  # or\nwide_df = clif.create_wide_dataset(hospitalization_ids=small_list)\n</code></pre></p> <p>Missing Columns <pre><code># Check available categories in your data first\nprint(clif.vitals.df['vital_category'].unique())\n</code></pre></p> <p>Empty Results <pre><code># Verify data exists for your filters\nprint(clif.hospitalization.df['hospitalization_id'].nunique())\n</code></pre></p>"},{"location":"wide_dataset/#integration-with-existing-workflow","title":"Integration with Existing Workflow","text":"<p>The wide dataset function is designed to integrate seamlessly with existing pyCLIF workflows:</p> <pre><code># Traditional approach\nclif = CLIF(data_dir=\"/path/to/data\")\nclif.initialize(['patient', 'vitals', 'labs'])\n\n# Enhanced with wide dataset\nwide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs'],\n    category_filters={'vitals': ['map'], 'labs': ['hemoglobin']}\n)\n\n# Continue with analysis\nanalysis_results = analyze_wide_dataset(wide_df)\n</code></pre> <p>This functionality brings the power of the original notebook's wide dataset creation into a reusable, configurable, and robust function that can be easily integrated into any pyCLIF workflow.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the complete API documentation for CLIFpy, automatically generated from the source code docstrings.</p>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#cliforchestrator","title":"ClifOrchestrator","text":"<p>The main orchestration class for managing multiple CLIF tables with consistent configuration.</p>"},{"location":"api/#basetable","title":"BaseTable","text":"<p>The base class that all CLIF table implementations inherit from, providing common functionality for data loading, validation, and reporting.</p>"},{"location":"api/#table-classes","title":"Table Classes","text":""},{"location":"api/#tables-overview","title":"Tables Overview","text":"<p>Complete API documentation for all CLIF table implementations:</p> <ul> <li>Patient - Patient demographics and identification</li> <li>Adt - Admission, discharge, and transfer events  </li> <li>Hospitalization - Hospital stay information</li> <li>Labs - Laboratory test results</li> <li>Vitals - Vital signs measurements</li> <li>RespiratorySupport - Ventilation and oxygen therapy</li> <li>MedicationAdminContinuous - Continuous medication infusions</li> <li>PatientAssessments - Clinical assessment scores</li> <li>Position - Patient positioning data</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":""},{"location":"api/#utility-functions","title":"Utility Functions","text":"<p>Helper functions for data processing, validation, and specialized operations:</p> <ul> <li>stitch_encounters - Link related hospitalizations within time windows</li> <li>process_resp_support_waterfall - Respiratory support waterfall algorithm</li> <li>io - Data loading and sample creation utilities</li> <li>config - Configuration management functions</li> <li>validator - Data validation functions</li> <li>outlier_handler - Outlier detection and handling</li> <li>wide_dataset - Wide dataset creation utilities</li> </ul>"},{"location":"api/#quick-links","title":"Quick Links","text":"<ul> <li>ClifOrchestrator API - Multi-table management</li> <li>BaseTable API - Common table functionality</li> <li>Table Classes API - Individual table implementations</li> <li>Utilities API - Helper functions</li> </ul>"},{"location":"api/#usage-example","title":"Usage Example","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\nfrom clifpy.tables import Patient, Labs, Vitals\n\n# Using the orchestrator\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Using individual tables\npatient = Patient.from_file('/path/to/data', 'parquet')\npatient.validate()\n</code></pre>"},{"location":"api/base-table/","title":"BaseTable","text":""},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable","title":"clifpy.tables.base_table.BaseTable","text":"<pre><code>BaseTable(\n    data_directory,\n    filetype,\n    timezone,\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>Base class for all pyCLIF table classes.</p> <p>Provides common functionality for loading data, running validations, and generating reports. All table-specific classes should inherit from this.</p> <p>Attributes:</p> Name Type Description <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>table_name</code> <code>str</code> <p>Name of the table (from class name)</p> <code>df</code> <code>DataFrame</code> <p>The loaded data</p> <code>schema</code> <code>dict</code> <p>The YAML schema for this table</p> <code>errors</code> <code>List[dict]</code> <p>Validation errors from last validation run</p> <code>logger</code> <code>Logger</code> <p>Logger for this table</p> <p>Initialize the BaseTable.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> required <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> required <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> required <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not provided, creates an 'output' directory in the current working directory.</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def __init__(\n    self, \n    data_directory: str,\n    filetype: str,\n    timezone: str,\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the BaseTable.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs.\n            If not provided, creates an 'output' directory in the current working directory.\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Store configuration\n    self.data_directory = data_directory\n    self.filetype = filetype\n    self.timezone = timezone\n\n    # Set output directory\n    if output_directory is None:\n        output_directory = os.path.join(os.getcwd(), 'output')\n    self.output_directory = output_directory\n    os.makedirs(self.output_directory, exist_ok=True)\n\n    # Derive snake_case table name from PascalCase class name\n    # Example: Adt -&gt; adt, RespiratorySupport -&gt; respiratory_support\n    self.table_name = ''.join(['_' + c.lower() if c.isupper() else c for c in self.__class__.__name__]).lstrip('_')\n\n    # Initialize data and validation state\n    self.df: Optional[pd.DataFrame] = data\n    self.errors: List[Dict[str, Any]] = []\n    self.schema: Optional[Dict[str, Any]] = None\n    self._validated: bool = False\n\n    # Setup logging\n    self._setup_logging()\n\n    # Load schema\n    self._load_schema()\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    data_directory=None,\n    filetype=None,\n    timezone=None,\n    config_path=None,\n    output_directory=None,\n    sample_size=None,\n    columns=None,\n    filters=None,\n)\n</code></pre> <p>Load data from file and create a table instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>None</code> <code>config_path</code> <code>str</code> <p>Path to configuration JSON file</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of rows to load</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>Specific columns to load</p> <code>None</code> <code>filters</code> <code>Dict</code> <p>Filters to apply when loading</p> <code>None</code> Loading priority <ol> <li>If all required params provided \u2192 use them</li> <li>If config_path provided \u2192 load from that path, allow param overrides</li> <li>If no params and no config_path \u2192 auto-detect clif_config.json</li> <li>Parameters override config file values when both are provided</li> </ol> <p>Returns:</p> Type Description <p>Instance of the table class with loaded data</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, \n    data_directory: Optional[str] = None,\n    filetype: Optional[str] = None,\n    timezone: Optional[str] = None,\n    config_path: Optional[str] = None,\n    output_directory: Optional[str] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n):\n    \"\"\"\n    Load data from file and create a table instance.\n\n    Parameters:\n        data_directory (str, optional): Path to the directory containing data files\n        filetype (str, optional): Type of data file (csv, parquet, etc.)\n        timezone (str, optional): Timezone for datetime columns\n        config_path (str, optional): Path to configuration JSON file\n        output_directory (str, optional): Directory for saving output files and logs\n        sample_size (int, optional): Number of rows to load\n        columns (List[str], optional): Specific columns to load\n        filters (Dict, optional): Filters to apply when loading\n\n    Loading priority:\n        1. If all required params provided \u2192 use them\n        2. If config_path provided \u2192 load from that path, allow param overrides\n        3. If no params and no config_path \u2192 auto-detect clif_config.json\n        4. Parameters override config file values when both are provided\n\n    Returns:\n        Instance of the table class with loaded data\n    \"\"\"\n    # Get configuration from config file or parameters\n    config = get_config_or_params(\n        config_path=config_path,\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory\n    )\n\n    # Derive snake_case table name from PascalCase class name\n    table_name = ''.join(['_' + c.lower() if c.isupper() else c for c in cls.__name__]).lstrip('_')\n\n    # Load data using existing io utility\n    data = load_data(\n        table_name, \n        config['data_directory'], \n        config['filetype'], \n        sample_size=sample_size,\n        columns=columns,\n        filters=filters,\n        site_tz=config['timezone']\n    )\n\n    # Create instance with loaded data\n    return cls(\n        data_directory=config['data_directory'],\n        filetype=config['filetype'],\n        timezone=config['timezone'],\n        output_directory=config.get('output_directory', output_directory),\n        data=data\n    )\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.get_summary","title":"get_summary","text":"<pre><code>get_summary()\n</code></pre> <p>Get a summary of the table data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Summary statistics and information about the table</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get a summary of the table data.\n\n    Returns:\n        dict: Summary statistics and information about the table\n    \"\"\"\n    if self.df is None:\n        return {\"status\": \"No data loaded\"}\n\n    summary = {\n        \"table_name\": self.table_name,\n        \"num_rows\": len(self.df),\n        \"num_columns\": len(self.df.columns),\n        \"columns\": list(self.df.columns),\n        \"memory_usage_mb\": self.df.memory_usage(deep=True).sum() / 1024 / 1024,\n        \"validation_run\": self._validated,\n        \"validation_errors\": len(self.errors) if self._validated else None,\n        \"is_valid\": self.isvalid()\n    }\n\n    # Add basic statistics for numeric columns\n    numeric_cols = self.df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) &gt; 0:\n        summary[\"numeric_columns\"] = list(numeric_cols)\n        summary[\"numeric_stats\"] = self.df[numeric_cols].describe().to_dict()\n\n    # Add missing data summary\n    missing_counts = self.df.isnull().sum()\n    if missing_counts.any():\n        summary[\"missing_data\"] = missing_counts[missing_counts &gt; 0].to_dict()\n\n    return summary\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Check if the data is valid based on the last validation run.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if validation has been run and no errors were found,   False if validation found errors or hasn't been run yet</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"\n    Check if the data is valid based on the last validation run.\n\n    Returns:\n        bool: True if validation has been run and no errors were found,\n              False if validation found errors or hasn't been run yet\n    \"\"\"\n    if not self._validated:\n        print(\"Validation has not been run yet. Please call validate() first.\")\n        return False\n    return not self.errors\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.save_summary","title":"save_summary","text":"<pre><code>save_summary()\n</code></pre> <p>Save table summary to a JSON file.</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def save_summary(self):\n    \"\"\"Save table summary to a JSON file.\"\"\"\n    try:\n        import json\n\n        summary = self.get_summary()\n\n        # Save to JSON\n        summary_file = os.path.join(\n            self.output_directory,\n            f'summary_{self.table_name}.json'\n        )\n\n        with open(summary_file, 'w') as f:\n            json.dump(summary, f, indent=2, default=str)\n\n        self.logger.info(f\"Saved summary to {summary_file}\")\n\n    except Exception as e:\n        self.logger.error(f\"Error saving summary: {str(e)}\")\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Run comprehensive validation on the data.</p> <p>This method runs all validation checks including: - Schema validation (required columns, data types, categories) - Missing data analysis - Duplicate checking - Statistical analysis - Table-specific validations (if overridden in child class)</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Run comprehensive validation on the data.\n\n    This method runs all validation checks including:\n    - Schema validation (required columns, data types, categories)\n    - Missing data analysis\n    - Duplicate checking\n    - Statistical analysis\n    - Table-specific validations (if overridden in child class)\n    \"\"\"\n    if self.df is None:\n        self.logger.warning(\"No dataframe to validate\")\n        print(\"No dataframe to validate.\")\n        return\n\n    self.logger.info(\"Starting validation\")\n    self.errors = []\n    self._validated = True\n\n    try:\n        # Run basic schema validation\n        if self.schema:\n            self.logger.info(\"Running schema validation\")\n            schema_errors = validator.validate_dataframe(self.df, self.schema)\n            self.errors.extend(schema_errors)\n\n            if schema_errors:\n                self.logger.warning(f\"Schema validation found {len(schema_errors)} errors\")\n            else:\n                self.logger.info(\"Schema validation passed\")\n\n        # Run enhanced validations (these will be implemented in Phase 3)\n        self._run_enhanced_validations()\n\n        # Run table-specific validations (can be overridden in child classes)\n        self._run_table_specific_validations()\n\n        # Log validation results\n        if not self.errors:\n            self.logger.info(\"Validation completed successfully\")\n            print(\"Validation completed successfully.\")\n        else:\n            self.logger.warning(f\"Validation completed with {len(self.errors)} error(s)\")\n            print(f\"Validation completed with {len(self.errors)} error(s). See `errors` attribute.\")\n\n            # Save errors to CSV\n            self._save_validation_errors()\n\n    except Exception as e:\n        self.logger.error(f\"Error during validation: {str(e)}\")\n        self.errors.append({\n            \"type\": \"validation_error\",\n            \"message\": str(e)\n        })\n</code></pre>"},{"location":"api/orchestrator/","title":"ClifOrchestrator","text":""},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator","title":"clifpy.clif_orchestrator.ClifOrchestrator","text":"<pre><code>ClifOrchestrator(\n    config_path=None,\n    data_directory=None,\n    filetype=None,\n    timezone=None,\n    output_directory=None,\n    stitch_encounter=False,\n    stitch_time_interval=6,\n)\n</code></pre> <p>Orchestrator class for managing multiple CLIF table objects.</p> <p>This class provides a centralized interface for loading, managing, and validating multiple CLIF tables with consistent configuration.</p> <p>Attributes:</p> Name Type Description <code>config_path</code> <code>str</code> <p>Path to configuration JSON file</p> <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>stitch_encounter</code> <code>bool</code> <p>Whether to stitch encounters within time interval</p> <code>stitch_time_interval</code> <code>int</code> <p>Hours between discharge and next admission to consider encounters linked</p> <code>encounter_mapping</code> <code>DataFrame</code> <p>Mapping of hospitalization_id to encounter_block (after stitching)</p> <code>patient</code> <code>Patient</code> <p>Patient table object</p> <code>hospitalization</code> <code>Hospitalization</code> <p>Hospitalization table object</p> <code>adt</code> <code>Adt</code> <p>ADT table object</p> <code>labs</code> <code>Labs</code> <p>Labs table object</p> <code>vitals</code> <code>Vitals</code> <p>Vitals table object</p> <code>medication_admin_continuous</code> <code>MedicationAdminContinuous</code> <p>Medication administration continuous table object</p> <code>medication_admin_intermittent</code> <code>MedicationAdminIntermittent</code> <p>Medication administration intermittent table object</p> <code>patient_assessments</code> <code>PatientAssessments</code> <p>Patient assessments table object</p> <code>respiratory_support</code> <code>RespiratorySupport</code> <p>Respiratory support table object</p> <code>position</code> <code>Position</code> <p>Position table object</p> <code>wide_df</code> <code>DataFrame</code> <p>Wide dataset with time-series data (populated by create_wide_dataset)</p> <p>Initialize the ClifOrchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to configuration JSON file</p> <code>None</code> <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not provided, creates an 'output' directory in the current working directory.</p> <code>None</code> <code>stitch_encounter</code> <code>bool</code> <p>Whether to stitch encounters within time interval. Default False.</p> <code>False</code> <code>stitch_time_interval</code> <code>int</code> <p>Hours between discharge and next admission to consider  encounters linked. Default 6 hours.</p> <code>6</code> Loading priority <ol> <li>If all required params provided \u2192 use them</li> <li>If config_path provided \u2192 load from that path, allow param overrides</li> <li>If no params and no config_path \u2192 auto-detect config.json</li> <li>Parameters override config file values when both are provided</li> </ol> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def __init__(\n    self,\n    config_path: Optional[str] = None,\n    data_directory: Optional[str] = None,\n    filetype: Optional[str] = None,\n    timezone: Optional[str] = None,\n    output_directory: Optional[str] = None,\n    stitch_encounter: bool = False,\n    stitch_time_interval: int = 6\n):\n    \"\"\"\n    Initialize the ClifOrchestrator.\n\n    Parameters:\n        config_path (str, optional): Path to configuration JSON file\n        data_directory (str, optional): Path to the directory containing data files\n        filetype (str, optional): Type of data file (csv, parquet, etc.)\n        timezone (str, optional): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs.\n            If not provided, creates an 'output' directory in the current working directory.\n        stitch_encounter (bool, optional): Whether to stitch encounters within time interval. Default False.\n        stitch_time_interval (int, optional): Hours between discharge and next admission to consider \n            encounters linked. Default 6 hours.\n\n    Loading priority:\n        1. If all required params provided \u2192 use them\n        2. If config_path provided \u2192 load from that path, allow param overrides\n        3. If no params and no config_path \u2192 auto-detect config.json\n        4. Parameters override config file values when both are provided\n    \"\"\"\n    # Get configuration from config file or parameters\n    config = get_config_or_params(\n        config_path=config_path,\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory\n    )\n\n    self.data_directory = config['data_directory']\n    self.filetype = config['filetype']\n    self.timezone = config['timezone']\n\n    # Set output directory\n    self.output_directory = config.get('output_directory')\n    if self.output_directory is None:\n        self.output_directory = os.path.join(os.getcwd(), 'output')\n    os.makedirs(self.output_directory, exist_ok=True)\n\n    # Initialize logger\n    self.logger = logging.getLogger('pyclif.ClifOrchestrator')\n\n\n    # Set stitching parameters\n    self.stitch_encounter = stitch_encounter\n    self.stitch_time_interval = stitch_time_interval\n    self.encounter_mapping = None\n\n    # Initialize all table attributes to None\n    self.patient: Patient = None\n    self.hospitalization: Hospitalization = None\n    self.adt: Adt = None\n    self.labs: Labs = None\n    self.vitals: Vitals = None\n    self.medication_admin_continuous: MedicationAdminContinuous = None\n    self.medication_admin_intermittent: MedicationAdminIntermittent = None\n    self.patient_assessments: PatientAssessments = None\n    self.respiratory_support: RespiratorySupport = None\n    self.position: Position = None\n\n    # Initialize wide dataset property\n    self.wide_df: Optional[pd.DataFrame] = None\n\n    print('ClifOrchestrator initialized.')\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.compute_sofa_scores","title":"compute_sofa_scores","text":"<pre><code>compute_sofa_scores(\n    wide_df=None,\n    cohort_df=None,\n    extremal_type=\"worst\",\n    id_name=\"encounter_block\",\n    fill_na_scores_with_zero=True,\n)\n</code></pre> <p>Compute SOFA (Sequential Organ Failure Assessment) scores.</p> <p>Parameters:</p> Name Type Description Default <code>wide_df</code> <code>Optional[DataFrame]</code> <p>Optional wide dataset. If not provided, uses self.wide_df or creates one</p> <code>None</code> <code>cohort_df</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with columns [id_name, 'start_time', 'end_time']       to further filter observations by time windows</p> <code>None</code> <code>extremal_type</code> <code>str</code> <p>'worst' (default) or 'latest' (future feature)</p> <code>'worst'</code> <code>id_name</code> <code>str</code> <p>Column name for grouping (default: 'encounter_block')     - 'encounter_block': Groups related hospitalizations (requires encounter stitching)     - 'hospitalization_id': Individual hospitalizations</p> <code>'encounter_block'</code> <code>fill_na_scores_with_zero</code> <code>bool</code> <p>If True, missing component scores default to 0</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with SOFA component scores and total score for each ID.</p> <code>DataFrame</code> <p>Results are stored in self.sofa_df.</p> Notes <ul> <li>Medication units should be pre-converted (e.g., 'norepinephrine_mcg_kg_min')</li> <li>When id_name='encounter_block' and encounter mapping doesn't exist,   it will be created automatically via run_stitch_encounters()</li> <li>Missing data defaults to score of 0 (normal organ function)</li> </ul> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; co = ClifOrchestrator(config_path='config/config.yaml')\n&gt;&gt;&gt; sofa_scores = co.compute_sofa_scores()\n</code></pre> <p>Per hospitalization instead of encounter:</p> <pre><code>&gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(id_name='hospitalization_id')\n</code></pre> <p>With time filtering:</p> <pre><code>&gt;&gt;&gt; cohort_df = pd.DataFrame({\n...     'encounter_block': ['E001', 'E002'],\n...     'start_time': pd.to_datetime(['2024-01-01', '2024-01-02']),\n...     'end_time': pd.to_datetime(['2024-01-03', '2024-01-04'])\n... })\n&gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(cohort_df=cohort_df)\n</code></pre> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def compute_sofa_scores(\n    self,\n    wide_df: Optional[pd.DataFrame] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    extremal_type: str = 'worst',\n    id_name: str = 'encounter_block',\n    fill_na_scores_with_zero: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute SOFA (Sequential Organ Failure Assessment) scores.\n\n    Parameters:\n        wide_df: Optional wide dataset. If not provided, uses self.wide_df or creates one\n        cohort_df: Optional DataFrame with columns [id_name, 'start_time', 'end_time']\n                  to further filter observations by time windows\n        extremal_type: 'worst' (default) or 'latest' (future feature)\n        id_name: Column name for grouping (default: 'encounter_block')\n                - 'encounter_block': Groups related hospitalizations (requires encounter stitching)\n                - 'hospitalization_id': Individual hospitalizations\n        fill_na_scores_with_zero: If True, missing component scores default to 0\n\n    Returns:\n        DataFrame with SOFA component scores and total score for each ID.\n        Results are stored in self.sofa_df.\n\n    Notes:\n        - Medication units should be pre-converted (e.g., 'norepinephrine_mcg_kg_min')\n        - When id_name='encounter_block' and encounter mapping doesn't exist,\n          it will be created automatically via run_stitch_encounters()\n        - Missing data defaults to score of 0 (normal organ function)\n\n    Examples:\n        Basic usage:\n        &gt;&gt;&gt; co = ClifOrchestrator(config_path='config/config.yaml')\n        &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores()\n\n        Per hospitalization instead of encounter:\n        &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(id_name='hospitalization_id')\n\n        With time filtering:\n        &gt;&gt;&gt; cohort_df = pd.DataFrame({\n        ...     'encounter_block': ['E001', 'E002'],\n        ...     'start_time': pd.to_datetime(['2024-01-01', '2024-01-02']),\n        ...     'end_time': pd.to_datetime(['2024-01-03', '2024-01-04'])\n        ... })\n        &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(cohort_df=cohort_df)\n    \"\"\"\n    from .utils.sofa import compute_sofa, REQUIRED_SOFA_CATEGORIES_BY_TABLE\n\n    self.logger.info(f\"Computing SOFA scores with extremal_type='{extremal_type}', id_name='{id_name}'\")\n\n    if (cohort_df is not None) and (id_name not in cohort_df.columns):\n        raise ValueError(f\"id_name '{id_name}' not found in cohort_df columns\")\n\n    # Determine which wide_df to use\n    if wide_df is not None:\n        self.logger.debug(\"Using provided wide_df\")\n        df = wide_df\n    elif hasattr(self, 'wide_df') and self.wide_df is not None:\n        self.logger.debug(\"Using existing self.wide_df\")\n        df = self.wide_df\n    else:\n        self.logger.info(\"No wide dataset available, creating one...\")\n        # Create wide dataset with required categories for SOFA\n\n        self.create_wide_dataset(\n            tables_to_load=list(REQUIRED_SOFA_CATEGORIES_BY_TABLE.keys()),\n            category_filters=REQUIRED_SOFA_CATEGORIES_BY_TABLE,\n            cohort_df=cohort_df\n        )\n        df = self.wide_df\n        self.logger.debug(f\"Created wide dataset with shape: {df.shape}\")\n\n    if id_name not in df.columns:\n        if self.encounter_mapping is None:\n            try:\n                self.run_stitch_encounters()\n            except Exception as e:\n                self.logger.error(f\"Error during encounter stitching: {e}\")\n                raise ValueError(\"Encounter stitching failed. Please run stitch_encounters() manually.\")\n        df = df.merge(self.encounter_mapping, on='hospitalization_id', how='left')   \n        self.wide_df = df\n        self.logger.debug(f\"Mapped {id_name} to wide_df via encounter_mapping, with shape: {df.shape}\")\n\n    # Compute SOFA scores\n    self.logger.debug(\"Calling compute_sofa function\")\n    sofa_scores = compute_sofa(\n        wide_df=df,\n        cohort_df=cohort_df,\n        extremal_type=extremal_type,\n        id_name=id_name,\n        fill_na_scores_with_zero=fill_na_scores_with_zero\n    )\n\n    # Store results in orchestrator\n    self.sofa_df = sofa_scores\n    self.logger.info(f\"SOFA computation completed. Results stored in self.sofa_df with shape: {sofa_scores.shape}\")\n\n    return sofa_scores\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_dose_units_for_continuous_meds","title":"convert_dose_units_for_continuous_meds","text":"<pre><code>convert_dose_units_for_continuous_meds(\n    preferred_units,\n    vitals_df=None,\n    show_intermediate=False,\n    override=False,\n    save_to_table=True,\n)\n</code></pre> <p>Convert dose units for continuous medication data.</p> <p>Parameters:</p> Name Type Description Default <code>preferred_units</code> <code>Dict[str, str]</code> <p>Dict of preferred units for each medication category</p> required <code>vitals_df</code> <code>DataFrame</code> <p>Vitals DataFrame for extracting patient weights (optional)</p> <code>None</code> <code>show_intermediate</code> <code>bool</code> <p>If True, includes intermediate calculation columns in output</p> <code>False</code> <code>override</code> <code>bool</code> <p>If True, continues processing with warnings for unacceptable units</p> <code>False</code> <code>save_to_table</code> <code>bool</code> <p>If True, saves the converted DataFrame to the table's df_converted property and stores conversion_counts as a table property. If False, returns the converted data without updating the table.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Tuple[DataFrame, DataFrame]]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: (converted_df, counts_df) when save_to_table=False</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_dose_units_for_continuous_meds(\n    self,\n    preferred_units: Dict[str, str],\n    vitals_df: pd.DataFrame = None,\n    show_intermediate: bool = False,\n    override: bool = False,\n    save_to_table: bool = True\n) -&gt; Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"\n    Convert dose units for continuous medication data.\n\n    Parameters:\n        preferred_units: Dict of preferred units for each medication category\n        vitals_df: Vitals DataFrame for extracting patient weights (optional)\n        show_intermediate: If True, includes intermediate calculation columns in output\n        override: If True, continues processing with warnings for unacceptable units\n        save_to_table: If True, saves the converted DataFrame to the table's df_converted\n            property and stores conversion_counts as a table property. If False,\n            returns the converted data without updating the table.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: (converted_df, counts_df) when save_to_table=False\n    \"\"\"\n    from .utils.unit_converter import convert_dose_units_by_med_category\n\n    # Log function entry with parameters\n    self.logger.info(f\"Starting dose unit conversion for continuous medications with parameters: \"\n                    f\"preferred_units={preferred_units}, show_intermediate={show_intermediate}, \"\n                    f\"override={override}, overwrite_table_df={save_to_table}\")\n\n    # use the vitals df loaded to the table instance if no stand-alone vitals_df is provided\n    if vitals_df is None:\n        self.logger.debug(\"No vitals_df provided, checking existing vitals table\")\n        if (self.vitals is None) or (self.vitals.df is None):\n            self.logger.info(\"Loading vitals table...\")\n            self.load_table('vitals')\n        vitals_df = self.vitals.df\n        self.logger.debug(f\"Using vitals data with shape: {vitals_df.shape}\")\n    else:\n        self.logger.debug(f\"Using provided vitals_df with shape: {vitals_df.shape}\")\n\n    if self.medication_admin_continuous is None:\n        self.logger.info(\"Loading medication_admin_continuous table...\")\n        self.load_table('medication_admin_continuous')\n        self.logger.debug(\"medication_admin_continuous table loaded successfully\")\n\n    # Call the conversion function with all parameters\n    self.logger.info(\"Starting dose unit conversion\")\n    self.logger.debug(f\"Input DataFrame shape: {self.medication_admin_continuous.df.shape}\")\n\n    converted_df, counts_df = convert_dose_units_by_med_category(\n        self.medication_admin_continuous.df,\n        vitals_df=vitals_df,\n        preferred_units=preferred_units,\n        show_intermediate=show_intermediate,\n        override=override\n    )\n\n    self.logger.info(\"Dose unit conversion completed\")\n    self.logger.debug(f\"Output DataFrame shape: {converted_df.shape}\")\n    self.logger.debug(f\"Conversion counts summary: {len(counts_df)} conversions tracked\")\n\n    # If overwrite_raw_df is True, update the table's df and store conversion_counts\n    if save_to_table:\n        self.logger.info(\"Updating medication_admin_continuous table with converted data\")\n        self.medication_admin_continuous.df_converted = converted_df\n        self.medication_admin_continuous.conversion_counts = counts_df\n        self.logger.debug(\"Conversion counts stored as table property\")\n    else:\n        self.logger.info(\"Returning converted data without updating table\")\n        return converted_df, counts_df\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_dose_units_for_intermittent_meds","title":"convert_dose_units_for_intermittent_meds","text":"<pre><code>convert_dose_units_for_intermittent_meds(\n    preferred_units,\n    vitals_df=None,\n    show_intermediate=False,\n    override=False,\n    save_to_table=True,\n)\n</code></pre> <p>Convert dose units for intermittent medication data.</p> <p>Parameters:</p> Name Type Description Default <code>preferred_units</code> <code>Dict[str, str]</code> <p>Dict of preferred units for each medication category</p> required <code>vitals_df</code> <code>DataFrame</code> <p>Vitals DataFrame for extracting patient weights (optional)</p> <code>None</code> <code>show_intermediate</code> <code>bool</code> <p>If True, includes intermediate calculation columns in output</p> <code>False</code> <code>override</code> <code>bool</code> <p>If True, continues processing with warnings for unacceptable units</p> <code>False</code> <code>save_to_table</code> <code>bool</code> <p>If True, saves the converted DataFrame to the table's df_converted property and stores conversion_counts as a table property. If False, returns the converted data without updating the table.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Tuple[DataFrame, DataFrame]]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: (converted_df, counts_df) when save_to_table=False</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_dose_units_for_intermittent_meds(\n    self,\n    preferred_units: Dict[str, str],\n    vitals_df: pd.DataFrame = None,\n    show_intermediate: bool = False,\n    override: bool = False,\n    save_to_table: bool = True\n) -&gt; Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"\n    Convert dose units for intermittent medication data.\n\n    Parameters:\n        preferred_units: Dict of preferred units for each medication category\n        vitals_df: Vitals DataFrame for extracting patient weights (optional)\n        show_intermediate: If True, includes intermediate calculation columns in output\n        override: If True, continues processing with warnings for unacceptable units\n        save_to_table: If True, saves the converted DataFrame to the table's df_converted\n            property and stores conversion_counts as a table property. If False,\n            returns the converted data without updating the table.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: (converted_df, counts_df) when save_to_table=False\n    \"\"\"\n    from .utils.unit_converter import convert_dose_units_by_med_category\n\n    # Log function entry with parameters\n    self.logger.info(f\"Starting dose unit conversion for intermittent medications with parameters: \"\n                    f\"preferred_units={preferred_units}, show_intermediate={show_intermediate}, \"\n                    f\"override={override}, save_to_table={save_to_table}\")\n\n    # use the vitals df loaded to the table instance if no stand-alone vitals_df is provided\n    if vitals_df is None:\n        self.logger.debug(\"No vitals_df provided, checking existing vitals table\")\n        if (self.vitals is None) or (self.vitals.df is None):\n            self.logger.info(\"Loading vitals table...\")\n            self.load_table('vitals')\n        vitals_df = self.vitals.df\n        self.logger.debug(f\"Using vitals data with shape: {vitals_df.shape}\")\n    else:\n        self.logger.debug(f\"Using provided vitals_df with shape: {vitals_df.shape}\")\n\n    if self.medication_admin_intermittent is None:\n        self.logger.info(\"Loading medication_admin_intermittent table...\")\n        self.load_table('medication_admin_intermittent')\n        self.logger.debug(\"medication_admin_intermittent table loaded successfully\")\n\n    # Call the conversion function with all parameters\n    self.logger.info(\"Starting dose unit conversion\")\n    self.logger.debug(f\"Input DataFrame shape: {self.medication_admin_intermittent.df.shape}\")\n\n    converted_df, counts_df = convert_dose_units_by_med_category(\n        self.medication_admin_intermittent.df,\n        vitals_df=vitals_df,\n        preferred_units=preferred_units,\n        show_intermediate=show_intermediate,\n        override=override\n    )\n\n    self.logger.info(\"Dose unit conversion completed\")\n    self.logger.debug(f\"Output DataFrame shape: {converted_df.shape}\")\n    self.logger.debug(f\"Conversion counts summary: {len(counts_df)} conversions tracked\")\n\n    # If save_to_table is True, update the table's df_converted and store conversion_counts\n    if save_to_table:\n        self.logger.info(\"Updating medication_admin_intermittent table with converted data\")\n        self.medication_admin_intermittent.df_converted = converted_df\n        self.medication_admin_intermittent.conversion_counts = counts_df\n        self.logger.debug(\"Conversion counts stored as table property\")\n    else:\n        self.logger.info(\"Returning converted data without updating table\")\n        return converted_df, counts_df\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_wide_to_hourly","title":"convert_wide_to_hourly","text":"<pre><code>convert_wide_to_hourly(\n    aggregation_config,\n    wide_df=None,\n    memory_limit=\"4GB\",\n    temp_directory=None,\n    batch_size=None,\n)\n</code></pre> <p>Convert wide dataset to hourly aggregation using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>wide_df</code> <code>Optional[DataFrame]</code> <p>Wide dataset DataFrame. If None, uses the stored wide_df from create_wide_dataset()</p> <code>None</code> <code>aggregation_config</code> <code>Dict[str, List[str]]</code> <p>Dict mapping aggregation methods to columns Example: {     'mean': ['heart_rate', 'sbp'],     'max': ['spo2'],     'min': ['map'],     'median': ['glucose'],     'first': ['gcs_total'],     'last': ['assessment_value'],     'boolean': ['norepinephrine'],     'one_hot_encode': ['device_category'] }</p> required <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '4GB', '8GB')</p> <code>'4GB'</code> <code>temp_directory</code> <code>Optional[str]</code> <p>Directory for DuckDB temp files</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Process in batches if specified</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Hourly aggregated DataFrame with nth_hour column</p> <p>Examples:</p>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_wide_to_hourly--using-stored-wide_df-after-create_wide_dataset","title":"Using stored wide_df (after create_wide_dataset())","text":"<p>co.create_wide_dataset(...) hourly_df = co.convert_wide_to_hourly(aggregation_config=config)</p>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_wide_to_hourly--using-explicit-wide_df-parameter","title":"Using explicit wide_df parameter","text":"<p>hourly_df = co.convert_wide_to_hourly(wide_df=my_df, aggregation_config=config)</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_wide_to_hourly(\n    self,\n    aggregation_config: Dict[str, List[str]],\n    wide_df: Optional[pd.DataFrame] = None,\n    memory_limit: str = '4GB',\n    temp_directory: Optional[str] = None,\n    batch_size: Optional[int] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert wide dataset to hourly aggregation using DuckDB.\n\n    Parameters:\n        wide_df: Wide dataset DataFrame. If None, uses the stored wide_df from create_wide_dataset()\n        aggregation_config: Dict mapping aggregation methods to columns\n            Example: {\n                'mean': ['heart_rate', 'sbp'],\n                'max': ['spo2'],\n                'min': ['map'],\n                'median': ['glucose'],\n                'first': ['gcs_total'],\n                'last': ['assessment_value'],\n                'boolean': ['norepinephrine'],\n                'one_hot_encode': ['device_category']\n            }\n        memory_limit: DuckDB memory limit (e.g., '4GB', '8GB')\n        temp_directory: Directory for DuckDB temp files\n        batch_size: Process in batches if specified\n\n    Returns:\n        Hourly aggregated DataFrame with nth_hour column\n\n    Examples:\n        # Using stored wide_df (after create_wide_dataset())\n        co.create_wide_dataset(...)\n        hourly_df = co.convert_wide_to_hourly(aggregation_config=config)\n\n        # Using explicit wide_df parameter\n        hourly_df = co.convert_wide_to_hourly(wide_df=my_df, aggregation_config=config)\n    \"\"\"\n    from clifpy.utils.wide_dataset import convert_wide_to_hourly\n\n    # Use provided wide_df or fall back to stored one\n    if wide_df is None:\n        if self.wide_df is None:\n            raise ValueError(\n                \"No wide dataset found. Please either:\\n\"\n                \"1. Run create_wide_dataset() first, OR\\n\"\n                \"2. Provide a wide_df parameter\"\n            )\n        wide_df = self.wide_df\n\n    return convert_wide_to_hourly(\n        wide_df=wide_df,\n        aggregation_config=aggregation_config,\n        memory_limit=memory_limit,\n        temp_directory=temp_directory,\n        batch_size=batch_size\n    )\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.create_wide_dataset","title":"create_wide_dataset","text":"<pre><code>create_wide_dataset(\n    tables_to_load=None,\n    category_filters=None,\n    sample=False,\n    hospitalization_ids=None,\n    encounter_blocks=None,\n    cohort_df=None,\n    output_format=\"dataframe\",\n    save_to_data_location=False,\n    output_filename=None,\n    return_dataframe=True,\n    batch_size=1000,\n    memory_limit=None,\n    threads=None,\n    show_progress=True,\n)\n</code></pre> <p>Create wide time-series dataset using DuckDB for high performance.</p>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.create_wide_dataset--parameters","title":"Parameters","text":"<p>tables_to_load : List[str], optional     List of table names to include in the wide dataset (e.g., ['vitals', 'labs', 'respiratory_support']).     If None, only base tables (patient, hospitalization, adt) are loaded. category_filters : Dict[str, List[str]], optional     Dictionary mapping table names to lists of categories to pivot into columns.     Example: {         'vitals': ['heart_rate', 'sbp', 'spo2'],         'labs': ['hemoglobin', 'sodium'],         'respiratory_support': ['device_category']     } sample : bool, default=False     If True, randomly sample 20 hospitalizations for testing purposes. hospitalization_ids : List[str], optional     List of specific hospitalization IDs to include. When provided, only data for these     hospitalizations will be loaded, improving performance for large datasets. encounter_blocks : List[int], optional     List of encounter block IDs to include when encounter stitching has been performed.     Automatically converts encounter blocks to their corresponding hospitalization IDs.     Only used when encounter stitching is enabled and encounter mapping exists. cohort_df : pd.DataFrame, optional     DataFrame containing cohort definitions with columns:     - 'patient_id': Patient identifier     - 'start_time': Start of time window (datetime)     - 'end_time': End of time window (datetime)     When encounter stitching is enabled, can also include 'encounter_block' column.     Used to filter data to specific time windows per patient. output_format : str, default='dataframe'     Format for output data. Options: 'dataframe', 'csv', 'parquet'. save_to_data_location : bool, default=False     If True, save output file to the data directory specified in orchestrator config. output_filename : str, optional     Custom filename for saved output. If None, auto-generates filename with timestamp. return_dataframe : bool, default=True     If True, return DataFrame even when saving to file. If False and saving,     returns None to save memory. batch_size : int, default=1000     Number of hospitalizations to process per batch. Lower values use less memory. memory_limit : str, optional     DuckDB memory limit (e.g., '8GB', '16GB'). If None, uses DuckDB default. threads : int, optional     Number of threads for DuckDB to use. If None, uses all available cores. show_progress : bool, default=True     If True, display progress bars during processing.</p>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.create_wide_dataset--returns","title":"Returns","text":"<p>None     The wide dataset is stored in the <code>wide_df</code> property of the orchestrator instance.     Access the result via <code>orchestrator.wide_df</code> after calling this method.</p>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.create_wide_dataset--notes","title":"Notes","text":"<ul> <li>When hospitalization_ids is provided, the function efficiently loads only the   specified hospitalizations from all tables, significantly reducing memory usage   and processing time for targeted analyses.</li> <li>The wide dataset will have one row per hospitalization per time point, with   columns for each category value specified in category_filters.</li> </ul> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def create_wide_dataset(\n    self,\n    tables_to_load: Optional[List[str]] = None,\n    category_filters: Optional[Dict[str, List[str]]] = None,\n    sample: bool = False,\n    hospitalization_ids: Optional[List[str]] = None,\n    encounter_blocks: Optional[List[int]] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    output_format: str = 'dataframe',\n    save_to_data_location: bool = False,\n    output_filename: Optional[str] = None,\n    return_dataframe: bool = True,\n    batch_size: int = 1000,\n    memory_limit: Optional[str] = None,\n    threads: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; None:\n    \"\"\"\n    Create wide time-series dataset using DuckDB for high performance.\n\n    Parameters\n    ----------\n    tables_to_load : List[str], optional\n        List of table names to include in the wide dataset (e.g., ['vitals', 'labs', 'respiratory_support']).\n        If None, only base tables (patient, hospitalization, adt) are loaded.\n    category_filters : Dict[str, List[str]], optional\n        Dictionary mapping table names to lists of categories to pivot into columns.\n        Example: {\n            'vitals': ['heart_rate', 'sbp', 'spo2'],\n            'labs': ['hemoglobin', 'sodium'],\n            'respiratory_support': ['device_category']\n        }\n    sample : bool, default=False\n        If True, randomly sample 20 hospitalizations for testing purposes.\n    hospitalization_ids : List[str], optional\n        List of specific hospitalization IDs to include. When provided, only data for these\n        hospitalizations will be loaded, improving performance for large datasets.\n    encounter_blocks : List[int], optional\n        List of encounter block IDs to include when encounter stitching has been performed.\n        Automatically converts encounter blocks to their corresponding hospitalization IDs.\n        Only used when encounter stitching is enabled and encounter mapping exists.\n    cohort_df : pd.DataFrame, optional\n        DataFrame containing cohort definitions with columns:\n        - 'patient_id': Patient identifier\n        - 'start_time': Start of time window (datetime)\n        - 'end_time': End of time window (datetime)\n        When encounter stitching is enabled, can also include 'encounter_block' column.\n        Used to filter data to specific time windows per patient.\n    output_format : str, default='dataframe'\n        Format for output data. Options: 'dataframe', 'csv', 'parquet'.\n    save_to_data_location : bool, default=False\n        If True, save output file to the data directory specified in orchestrator config.\n    output_filename : str, optional\n        Custom filename for saved output. If None, auto-generates filename with timestamp.\n    return_dataframe : bool, default=True\n        If True, return DataFrame even when saving to file. If False and saving,\n        returns None to save memory.\n    batch_size : int, default=1000\n        Number of hospitalizations to process per batch. Lower values use less memory.\n    memory_limit : str, optional\n        DuckDB memory limit (e.g., '8GB', '16GB'). If None, uses DuckDB default.\n    threads : int, optional\n        Number of threads for DuckDB to use. If None, uses all available cores.\n    show_progress : bool, default=True\n        If True, display progress bars during processing.\n\n    Returns\n    -------\n    None\n        The wide dataset is stored in the `wide_df` property of the orchestrator instance.\n        Access the result via `orchestrator.wide_df` after calling this method.\n\n    Notes\n    -----\n    - When hospitalization_ids is provided, the function efficiently loads only the\n      specified hospitalizations from all tables, significantly reducing memory usage\n      and processing time for targeted analyses.\n    - The wide dataset will have one row per hospitalization per time point, with\n      columns for each category value specified in category_filters.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"=== WIDE DATASET CREATION STARTED ===\")\n    print(\"=\" * 50)\n\n    print(\"\\nPhase 1: Initialization\")\n    print(\"  1.1: Validating parameters\")\n\n    # Import the utility function\n    from clifpy.utils.wide_dataset import create_wide_dataset as _create_wide\n\n    # Handle encounter stitching scenarios\n    if self.encounter_mapping is not None:\n        print(\"  1.2: Configuring encounter stitching (enabled)\")\n    else:\n        print(\"  1.2: Encounter stitching (disabled)\")\n\n    print(\"\\nPhase 2: Encounter Processing\")\n\n    if self.encounter_mapping is not None:\n        print(\"  2.1: === SPECIAL: ENCOUNTER STITCHING ===\")\n        # Handle cohort_df with encounter_block column\n        if cohort_df is not None:\n            if 'encounter_block' in cohort_df.columns:\n                print(\"       - Detected encounter_block column in cohort_df\")\n                print(\"       - Mapping encounter blocks to hospitalization IDs...\")\n                # Merge cohort_df with encounter_mapping to get hospitalization_ids\n                cohort_df = pd.merge(\n                    cohort_df,\n                    self.encounter_mapping[['hospitalization_id', 'encounter_block']],\n                    on='encounter_block',\n                    how='inner',\n                    suffixes=('_orig', '')\n                )\n                # If hospitalization_id_orig exists (cohort had both), use the mapping version\n                if 'hospitalization_id_orig' in cohort_df.columns:\n                    cohort_df = cohort_df.drop(columns=['hospitalization_id_orig'])\n                print(f\"       - Processing {cohort_df['encounter_block'].nunique()} encounter blocks from cohort_df\")\n            elif 'hospitalization_id' in cohort_df.columns:\n                print(\"Info: Encounter stitching has been performed. Your cohort_df uses hospitalization_id. \" +\n                      \"Consider using 'encounter_block' column instead for cleaner encounter-level filtering.\")\n            else:\n                print(\"Warning: cohort_df must contain either 'hospitalization_id' or 'encounter_block' column.\")\n\n        # Handle encounter_blocks parameter\n        if encounter_blocks is not None:\n            print(\"       - Processing encounter_blocks parameter\")\n            if len(encounter_blocks) == 0:\n                print(\"       - Warning: Empty encounter_blocks list provided. Processing all encounter blocks.\")\n                encounter_blocks = None\n            else:\n                # Validate that provided encounter_blocks exist in mapping\n                invalid_blocks = [b for b in encounter_blocks if b not in self.encounter_mapping['encounter_block'].values]\n                if invalid_blocks:\n                    print(f\"       - Warning: Invalid encounter blocks found: {invalid_blocks}\")\n                    encounter_blocks = [b for b in encounter_blocks if b in self.encounter_mapping['encounter_block'].values]\n\n                if encounter_blocks:  # Only if valid blocks remain\n                    hospitalization_ids = self.encounter_mapping[\n                        self.encounter_mapping['encounter_block'].isin(encounter_blocks)\n                    ]['hospitalization_id'].tolist()\n                    print(f\"       - Converting {len(encounter_blocks)} encounter blocks to {len(hospitalization_ids)} hospitalizations\")\n                else:\n                    print(\"       - No valid encounter blocks found. Processing all data.\")\n                    encounter_blocks = None\n\n        # If no filters provided after stitching\n        elif hospitalization_ids is None and cohort_df is None:\n            print(\"       - No encounter_blocks provided - processing all encounter blocks\")\n    else:\n        print(\"  2.1: No encounter stitching performed\")\n\n    filters = None\n    if hospitalization_ids:\n        filters = {'hospitalization_id': hospitalization_ids}\n\n    print(\"\\nPhase 3: Table Loading\")\n\n    print(\"  3.1: Auto-loading base tables\")\n    # Auto-load base tables if not loaded\n    if self.patient is None:\n        print(\"       - Loading patient table...\")\n        self.load_table('patient')  # Patient doesn't need filters\n    if self.hospitalization is None:\n        print(\"       - Loading hospitalization table...\")\n        self.load_table('hospitalization', filters=filters)\n    if self.adt is None:\n        print(\"       - Loading adt table...\")\n        self.load_table('adt', filters=filters)\n\n    # Load optional tables only if not already loaded\n    print(f\"  3.2: Loading optional tables: {tables_to_load or 'None'}\")\n    if tables_to_load:\n        for table_name in tables_to_load:\n            if getattr(self, table_name, None) is None:\n                print(f\"       - Loading {table_name} table...\")\n                try:\n                    self.load_table(table_name)\n                except Exception as e:\n                    print(f\"       - Warning: Could not load {table_name}: {e}\")\n            else:\n                print(f\"       - {table_name} table already loaded\")\n\n    # Check if patient_assessments needs assessment_value column\n    if (tables_to_load and 'patient_assessments' in tables_to_load) or \\\n       (category_filters and 'patient_assessments' in category_filters):\n        if self.patient_assessments is not None and hasattr(self.patient_assessments, 'df'):\n            df = self.patient_assessments.df\n            if 'numerical_value' in df.columns and 'categorical_value' in df.columns:\n                if 'assessment_value' not in df.columns:\n\n                    print(\"  === SPECIAL: PATIENT ASSESSMENTS PROCESSING ===\")\n                    print(\"       - Merging numerical_value and categorical_value columns\")\n                    try:\n                        import polars as pl\n                        print(\"       - Using Polars for performance optimization\")\n\n                        # Convert to Polars for efficient processing\n                        df_pl = pl.from_pandas(df)\n\n                        # Check data integrity using Polars\n                        both_filled = df_pl.filter(\n                            (pl.col('numerical_value').is_not_null()) &amp;\n                            (pl.col('categorical_value').is_not_null())\n                        )\n                        both_filled_count = len(both_filled)\n\n                        if both_filled_count &gt; 0:\n                            print(f\"       - WARNING: Found {both_filled_count} rows with both values!\")\n                            print(f\"       -          Numerical values will take precedence\")\n\n                        # Create assessment_value using Polars coalesce (much faster than pandas fillna)\n                        df_pl = df_pl.with_columns(\n                            pl.coalesce([\n                                pl.col('numerical_value'),\n                                pl.col('categorical_value')\n                            ]).cast(pl.Utf8).alias('assessment_value')\n                        )\n\n                        # Calculate statistics efficiently with Polars\n                        num_count = df_pl.select(pl.col('numerical_value').is_not_null().sum()).item()\n                        cat_count = df_pl.select(pl.col('categorical_value').is_not_null().sum()).item()\n                        total_count = df_pl.select(pl.col('assessment_value').is_not_null().sum()).item()\n\n                        # Convert back to pandas for compatibility\n                        self.patient_assessments.df = df_pl.to_pandas()\n\n                        print(f\"       - Created assessment_value column:\")\n                        print(f\"       -   {num_count} numerical values (takes precedence)\")\n                        print(f\"       -   {cat_count} categorical values (used when numerical is null)\")\n                        print(f\"       -   Total non-null assessment values: {total_count}\")\n                        print(f\"       -   Stored as string type for processing compatibility\")\n\n                    except ImportError:\n                        print(\"       - Warning: Polars not installed. Using pandas (slower)...\")\n                        # Fallback to pandas\n                        both_filled = df[(df['numerical_value'].notna()) &amp;\n                                        (df['categorical_value'].notna())]\n                        if len(both_filled) &gt; 0:\n                            print(f\"       - WARNING: Found {len(both_filled)} rows with both values!\")\n\n                        df['assessment_value'] = df['numerical_value'].fillna(df['categorical_value'])\n                        df['assessment_value'] = df['assessment_value'].astype(str)\n\n                        num_count = df['numerical_value'].notna().sum()\n                        cat_count = df['categorical_value'].notna().sum()\n                        total_count = df['assessment_value'].notna().sum()\n\n                        print(f\"       - Created assessment_value column:\")\n                        print(f\"       -   {num_count} numerical values (takes precedence)\")\n                        print(f\"       -   {cat_count} categorical values (used when numerical is null)\")\n                        print(f\"       -   Total non-null assessment values: {total_count}\")\n\n    print(\"\\nPhase 4: Calling Wide Dataset Utility\")\n\n    print(f\"  4.1: Passing to wide_dataset.create_wide_dataset()\")\n    print(f\"       - Tables: {tables_to_load or 'None'}\")\n    print(f\"       - Category filters: {list(category_filters.keys()) if category_filters else 'None'}\")\n    print(f\"       - Batch size: {batch_size}\")\n    print(f\"       - Memory limit: {memory_limit}\")\n    print(f\"       - Show progress: {show_progress}\")\n\n    # Call utility function with self as clif_instance and store result in wide_df property\n    self.wide_df = _create_wide(\n        clif_instance=self,\n        optional_tables=tables_to_load,\n        category_filters=category_filters,\n        sample=sample,\n        hospitalization_ids=hospitalization_ids,\n        cohort_df=cohort_df,\n        output_format=output_format,\n        save_to_data_location=save_to_data_location,\n        output_filename=output_filename,\n        return_dataframe=return_dataframe,\n        batch_size=batch_size,\n        memory_limit=memory_limit,\n        threads=threads,\n        show_progress=show_progress\n    )\n\n    print(\"\\nPhase 5: Post-Processing\")\n\n    # Add encounter_block column if encounter mapping exists and not already present\n    if self.encounter_mapping is not None and self.wide_df is not None:\n\n        print(\"  5.1: === SPECIAL: ADDING ENCOUNTER BLOCKS ===\")\n        if 'encounter_block' not in self.wide_df.columns:\n            print(\"       - Adding encounter_block column from encounter mapping...\")\n            self.wide_df = pd.merge(\n                self.wide_df,\n                self.encounter_mapping[['hospitalization_id', 'encounter_block']],\n                on='hospitalization_id',\n                how='left'\n            )\n            print(f\"       - Added encounter_block column - {self.wide_df['encounter_block'].nunique()} unique encounter blocks\")\n        else:\n            print(f\"       - Encounter_block column already present - {self.wide_df['encounter_block'].nunique()} unique encounter blocks\")\n    else:\n        print(\"  5.1: No encounter block mapping to add\")\n\n    # Optimize data types for assessment columns using Polars for performance\n    if self.wide_df is not None and ((tables_to_load and 'patient_assessments' in tables_to_load) or \\\n       (category_filters and 'patient_assessments' in category_filters)):\n\n        print(\"  5.2: === SPECIAL: ASSESSMENT TYPE OPTIMIZATION ===\")\n        try:\n            import polars as pl\n            print(\"       - Using Polars for performance optimization\")\n\n            # Determine which assessment columns to check\n            assessment_columns = []\n\n            if category_filters and 'patient_assessments' in category_filters:\n                # If specific categories were requested, use those\n                assessment_columns = [col for col in category_filters['patient_assessments']\n                                     if col in self.wide_df.columns]\n            else:\n                # Get all possible assessment categories from the schema\n                if self.patient_assessments and hasattr(self.patient_assessments, 'schema'):\n                    schema = self.patient_assessments.schema\n                    if 'columns' in schema:\n                        for col_def in schema['columns']:\n                            if col_def.get('name') == 'assessment_category':\n                                assessment_columns = col_def.get('permissible_values', [])\n                                break\n\n                # Filter to only columns that exist in wide_df\n                assessment_columns = [col for col in assessment_columns if col in self.wide_df.columns]\n\n            if assessment_columns:\n                print(f\"       - Analyzing {len(assessment_columns)} assessment columns\")\n\n                # Convert to Polars for efficient processing\n                df_pl = pl.from_pandas(self.wide_df)\n\n                numeric_conversions = []\n                string_kept = []\n\n                # Process all columns in one go for better performance\n                for col in assessment_columns:\n                    try:\n                        # Create a temporary column with numeric conversion attempt\n                        temp_col = f\"{col}_numeric_test\"\n                        df_pl = df_pl.with_columns(\n                            pl.col(col).cast(pl.Float64, strict=False).alias(temp_col)\n                        )\n\n                        # Check conversion success rate\n                        stats = df_pl.select([\n                            pl.col(col).is_not_null().sum().alias('original_count'),\n                            pl.col(temp_col).is_not_null().sum().alias('converted_count')\n                        ]).row(0)\n\n                        if stats[0] &gt; 0:  # If there are non-null values\n                            conversion_rate = stats[1] / stats[0]\n\n                            if conversion_rate &gt;= 0.95:  # 95% or more are numeric\n                                # Replace original with converted\n                                df_pl = df_pl.drop(col).rename({temp_col: col})\n                                numeric_conversions.append(col)\n                            else:\n                                # Keep original, drop temp\n                                df_pl = df_pl.drop(temp_col)\n                                string_kept.append(col)\n                        else:\n                            # No data, just drop temp\n                            df_pl = df_pl.drop(temp_col)\n\n                    except Exception:\n                        # Keep as string if any error, clean up temp column if it exists\n                        if f\"{col}_numeric_test\" in df_pl.columns:\n                            df_pl = df_pl.drop(f\"{col}_numeric_test\")\n                        string_kept.append(col)\n\n                # Convert back to pandas\n                self.wide_df = df_pl.to_pandas()\n\n                # Report conversions\n                if numeric_conversions:\n                    print(f\"       - Converted to numeric ({len(numeric_conversions)} columns):\")\n                    for col in numeric_conversions[:5]:  # Show first 5 examples\n                        print(f\"       -   {col}\")\n                    if len(numeric_conversions) &gt; 5:\n                        print(f\"       -   ... and {len(numeric_conversions) - 5} more\")\n\n                if string_kept:\n                    print(f\"       - Kept as string ({len(string_kept)} columns with mixed/text values):\")\n                    for col in string_kept[:5]:  # Show first 5 examples\n                        print(f\"       -   {col}\")\n                    if len(string_kept) &gt; 5:\n                        print(f\"       -   ... and {len(string_kept) - 5} more\")\n            else:\n                print(\"       - No assessment columns found to optimize\")\n\n        except ImportError:\n            print(\"       - Warning: Polars not installed. Skipping type optimization...\")\n            print(\"       - Install polars for better performance: pip install polars\")\n    else:\n        print(\"  5.2: No assessment type optimization needed\")\n\n    print(\"\\nPhase 6: Completion\")\n\n    if self.wide_df is not None:\n        print(f\"  6.1: Wide dataset stored in self.wide_df\")\n        print(f\"  6.2: Dataset shape: {self.wide_df.shape[0]} rows x {self.wide_df.shape[1]} columns\")\n    else:\n        print(\"  6.1: Warning: No wide dataset was created\")\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"=== WIDE DATASET CREATION COMPLETED ===\")\n    print(\"=\" * 50)\n    print(\"\")\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config_path='./config.json')\n</code></pre> <p>Create a ClifOrchestrator instance from a configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration JSON file</p> <code>'./config.json'</code> <p>Returns:</p> Name Type Description <code>ClifOrchestrator</code> <code>ClifOrchestrator</code> <p>Configured instance</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>@classmethod\ndef from_config(cls, config_path: str = \"./config.json\") -&gt; 'ClifOrchestrator':\n    \"\"\"\n    Create a ClifOrchestrator instance from a configuration file.\n\n    Parameters:\n        config_path (str): Path to the configuration JSON file\n\n    Returns:\n        ClifOrchestrator: Configured instance\n    \"\"\"\n    return cls(config_path=config_path)\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_encounter_mapping","title":"get_encounter_mapping","text":"<pre><code>get_encounter_mapping()\n</code></pre> <p>Return the encounter mapping DataFrame if encounter stitching was performed.</p> <p>Returns:</p> Name Type Description <code>Optional[DataFrame]</code> <p>pd.DataFrame: Mapping of hospitalization_id to encounter_block if stitching was performed.</p> <code>None</code> <code>Optional[DataFrame]</code> <p>If stitching was not performed or failed.</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_encounter_mapping(self) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Return the encounter mapping DataFrame if encounter stitching was performed.\n\n    Returns:\n        pd.DataFrame: Mapping of hospitalization_id to encounter_block if stitching was performed.\n        None: If stitching was not performed or failed.\n    \"\"\"\n    if self.encounter_mapping is None:\n        self.run_stitch_encounters()\n    return self.encounter_mapping\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_loaded_tables","title":"get_loaded_tables","text":"<pre><code>get_loaded_tables()\n</code></pre> <p>Return list of currently loaded table names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of loaded table names</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_loaded_tables(self) -&gt; List[str]:\n    \"\"\"\n    Return list of currently loaded table names.\n\n    Returns:\n        List[str]: List of loaded table names\n    \"\"\"\n    loaded = []\n    for table_name in ['patient', 'hospitalization', 'adt', 'labs', 'vitals',\n                      'medication_admin_continuous', 'medication_admin_intermittent',\n                      'patient_assessments', 'respiratory_support', 'position']:\n        if getattr(self, table_name) is not None:\n            loaded.append(table_name)\n    return loaded\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_sys_resource_info","title":"get_sys_resource_info","text":"<pre><code>get_sys_resource_info(print_summary=True)\n</code></pre> <p>Get system resource information including CPU, memory, and practical thread limits.</p> <p>Parameters:</p> Name Type Description Default <code>print_summary</code> <code>bool</code> <p>Whether to print a formatted summary</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing system resource information:</p> <code>Dict[str, Any]</code> <ul> <li>cpu_count_physical: Number of physical CPU cores</li> </ul> <code>Dict[str, Any]</code> <ul> <li>cpu_count_logical: Number of logical CPU cores</li> </ul> <code>Dict[str, Any]</code> <ul> <li>cpu_usage_percent: Current CPU usage percentage</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_total_gb: Total RAM in GB</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_available_gb: Available RAM in GB</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_used_gb: Used RAM in GB</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_usage_percent: Memory usage percentage</li> </ul> <code>Dict[str, Any]</code> <ul> <li>process_threads: Number of threads used by current process</li> </ul> <code>Dict[str, Any]</code> <ul> <li>max_recommended_threads: Recommended max threads for optimal performance</li> </ul> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_sys_resource_info(self, print_summary: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get system resource information including CPU, memory, and practical thread limits.\n\n    Parameters:\n        print_summary (bool): Whether to print a formatted summary\n\n    Returns:\n        Dict containing system resource information:\n        - cpu_count_physical: Number of physical CPU cores\n        - cpu_count_logical: Number of logical CPU cores\n        - cpu_usage_percent: Current CPU usage percentage\n        - memory_total_gb: Total RAM in GB\n        - memory_available_gb: Available RAM in GB\n        - memory_used_gb: Used RAM in GB\n        - memory_usage_percent: Memory usage percentage\n        - process_threads: Number of threads used by current process\n        - max_recommended_threads: Recommended max threads for optimal performance\n    \"\"\"\n    # Get current process\n    current_process = psutil.Process()\n\n    # CPU information\n    cpu_count_physical = psutil.cpu_count(logical=False)\n    cpu_count_logical = psutil.cpu_count(logical=True)\n    cpu_usage_percent = psutil.cpu_percent(interval=1)\n\n    # Memory information\n    memory = psutil.virtual_memory()\n    memory_total_gb = memory.total / (1024**3)\n    memory_available_gb = memory.available / (1024**3)\n    memory_used_gb = memory.used / (1024**3)\n    memory_usage_percent = memory.percent\n\n    # Thread information\n    process_threads = current_process.num_threads()\n    max_recommended_threads = cpu_count_physical  # Conservative recommendation\n\n    resource_info = {\n        'cpu_count_physical': cpu_count_physical,\n        'cpu_count_logical': cpu_count_logical,\n        'cpu_usage_percent': cpu_usage_percent,\n        'memory_total_gb': memory_total_gb,\n        'memory_available_gb': memory_available_gb,\n        'memory_used_gb': memory_used_gb,\n        'memory_usage_percent': memory_usage_percent,\n        'process_threads': process_threads,\n        'max_recommended_threads': max_recommended_threads\n    }\n\n    if print_summary:\n        print(\"=\" * 50)\n        print(\"SYSTEM RESOURCES\")\n        print(\"=\" * 50)\n        print(f\"CPU Cores (Physical): {cpu_count_physical}\")\n        print(f\"CPU Cores (Logical):  {cpu_count_logical}\")\n        print(f\"CPU Usage:            {cpu_usage_percent:.1f}%\")\n        print(\"-\" * 50)\n        print(f\"Total RAM:            {memory_total_gb:.1f} GB\")\n        print(f\"Available RAM:        {memory_available_gb:.1f} GB\")\n        print(f\"Used RAM:             {memory_used_gb:.1f} GB\")\n        print(f\"Memory Usage:         {memory_usage_percent:.1f}%\")\n        print(\"-\" * 50)\n        print(f\"Process Threads:      {process_threads}\")\n        print(f\"Max Recommended:      {max_recommended_threads} threads\")\n        print(\"-\" * 50)\n        print(f\"RECOMMENDATION: Use {max(1, cpu_count_physical-2)}-{cpu_count_physical} threads for optimal performance\")\n        print(f\"(Based on {cpu_count_physical} physical CPU cores)\")\n        print(\"=\" * 50)\n\n    return resource_info\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_tables_obj_list","title":"get_tables_obj_list","text":"<pre><code>get_tables_obj_list()\n</code></pre> <p>Return list of loaded table objects.</p> <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of loaded table objects</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_tables_obj_list(self) -&gt; List:\n    \"\"\"\n    Return list of loaded table objects.\n\n    Returns:\n        List: List of loaded table objects\n    \"\"\"\n    table_objects = []\n    for table_name in ['patient', 'hospitalization', 'adt', 'labs', 'vitals',\n                      'medication_admin_continuous', 'medication_admin_intermittent',\n                      'patient_assessments', 'respiratory_support', 'position']:\n        table_obj = getattr(self, table_name)\n        if table_obj is not None:\n            table_objects.append(table_obj)\n    return table_objects\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.initialize","title":"initialize","text":"<pre><code>initialize(\n    tables=None,\n    sample_size=None,\n    columns=None,\n    filters=None,\n)\n</code></pre> <p>Initialize specified tables with optional filtering and column selection.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>List[str]</code> <p>List of table names to load. Defaults to ['patient'].</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of rows to load for each table.</p> <code>None</code> <code>columns</code> <code>Dict[str, List[str]]</code> <p>Dictionary mapping table names to lists of columns to load.</p> <code>None</code> <code>filters</code> <code>Dict[str, Dict]</code> <p>Dictionary mapping table names to filter dictionaries.</p> <code>None</code> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def initialize(\n    self,\n    tables: Optional[List[str]] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[Dict[str, List[str]]] = None,\n    filters: Optional[Dict[str, Dict[str, Any]]] = None\n):\n    \"\"\"\n    Initialize specified tables with optional filtering and column selection.\n\n    Parameters:\n        tables (List[str], optional): List of table names to load. Defaults to ['patient'].\n        sample_size (int, optional): Number of rows to load for each table.\n        columns (Dict[str, List[str]], optional): Dictionary mapping table names to lists of columns to load.\n        filters (Dict[str, Dict], optional): Dictionary mapping table names to filter dictionaries.\n    \"\"\"\n    if tables is None:\n        tables = ['patient']\n\n    for table in tables:\n        # Get table-specific columns and filters if provided\n        table_columns = columns.get(table) if columns else None\n        table_filters = filters.get(table) if filters else None\n\n        try:\n            self.load_table(table, sample_size, table_columns, table_filters)\n        except ValueError as e:\n            print(f\"Warning: {e}\")\n\n    # Perform encounter stitching if enabled\n    if self.stitch_encounter:\n        self.run_stitch_encounters()\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.load_table","title":"load_table","text":"<pre><code>load_table(\n    table_name, sample_size=None, columns=None, filters=None\n)\n</code></pre> <p>Load table data and create table object.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to load</p> required <code>sample_size</code> <code>int</code> <p>Number of rows to load</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>Specific columns to load</p> <code>None</code> <code>filters</code> <code>Dict</code> <p>Filters to apply when loading</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded table object</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def load_table(\n    self,\n    table_name: str,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n) -&gt; Any:\n    \"\"\"\n    Load table data and create table object.\n\n    Parameters:\n        table_name (str): Name of the table to load\n        sample_size (int, optional): Number of rows to load\n        columns (List[str], optional): Specific columns to load\n        filters (Dict, optional): Filters to apply when loading\n\n    Returns:\n        The loaded table object\n    \"\"\"\n    if table_name not in TABLE_CLASSES:\n        raise ValueError(f\"Unknown table: {table_name}. Available tables: {list(TABLE_CLASSES.keys())}\")\n\n    table_class = TABLE_CLASSES[table_name]\n    table_object = table_class.from_file(\n        data_directory=self.data_directory,\n        filetype=self.filetype,\n        timezone=self.timezone,\n        output_directory=self.output_directory,\n        sample_size=sample_size,\n        columns=columns,\n        filters=filters\n    )\n    setattr(self, table_name, table_object)\n    return table_object\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.validate_all","title":"validate_all","text":"<pre><code>validate_all()\n</code></pre> <p>Run validation on all loaded tables.</p> <p>This method runs the validate() method on each loaded table and reports the results.</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def validate_all(self):\n    \"\"\"\n    Run validation on all loaded tables.\n\n    This method runs the validate() method on each loaded table\n    and reports the results.\n    \"\"\"\n    loaded_tables = self.get_loaded_tables()\n\n    if not loaded_tables:\n        print(\"No tables loaded to validate.\")\n        return\n\n    print(f\"Validating {len(loaded_tables)} table(s)...\")\n\n    for table_name in loaded_tables:\n        table_obj = getattr(self, table_name)\n        print(f\"\\nValidating {table_name}...\")\n        table_obj.validate()\n</code></pre>"},{"location":"api/tables/","title":"Table Classes","text":""},{"location":"api/tables/#patient","title":"Patient","text":""},{"location":"api/tables/#clifpy.tables.patient.Patient","title":"clifpy.tables.patient.Patient","text":"<pre><code>Patient(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient table wrapper inheriting from BaseTable.</p> <p>This class handles patient-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#adt-admission-discharge-transfer","title":"ADT (Admission, Discharge, Transfer)","text":""},{"location":"api/tables/#clifpy.tables.adt.Adt","title":"clifpy.tables.adt.Adt","text":"<pre><code>Adt(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>ADT (Admission/Discharge/Transfer) table wrapper inheriting from BaseTable.</p> <p>This class handles ADT-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the ADT table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the ADT table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: adt(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.filter_by_date_range","title":"filter_by_date_range","text":"<pre><code>filter_by_date_range(\n    start_date, end_date, date_column=\"in_dttm\"\n)\n</code></pre> <p>Return records within a specific date range for a given datetime column.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def filter_by_date_range(self, start_date: datetime, end_date: datetime, \n                       date_column: str = 'in_dttm') -&gt; pd.DataFrame:\n    \"\"\"Return records within a specific date range for a given datetime column.\"\"\"\n    if self.df is None or date_column not in self.df.columns:\n        return pd.DataFrame()\n\n    # Convert datetime column to datetime if it's not already\n    df_copy = self.df.copy()\n    df_copy[date_column] = pd.to_datetime(df_copy[date_column])\n\n    mask = (df_copy[date_column] &gt;= start_date) &amp; (df_copy[date_column] &lt;= end_date)\n    return df_copy[mask]\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.filter_by_hospitalization","title":"filter_by_hospitalization","text":"<pre><code>filter_by_hospitalization(hospitalization_id)\n</code></pre> <p>Return all ADT records for a specific hospitalization.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def filter_by_hospitalization(self, hospitalization_id: str) -&gt; pd.DataFrame:\n    \"\"\"Return all ADT records for a specific hospitalization.\"\"\"\n    if self.df is None:\n        return pd.DataFrame()\n\n    return self.df[self.df['hospitalization_id'] == hospitalization_id].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.filter_by_location_category","title":"filter_by_location_category","text":"<pre><code>filter_by_location_category(location_category)\n</code></pre> <p>Return all records for a specific location category (e.g., 'icu', 'ward').</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def filter_by_location_category(self, location_category: str) -&gt; pd.DataFrame:\n    \"\"\"Return all records for a specific location category (e.g., 'icu', 'ward').\"\"\"\n    if self.df is None or 'location_category' not in self.df.columns:\n        return pd.DataFrame()\n\n    return self.df[self.df['location_category'] == location_category].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.get_hospital_types","title":"get_hospital_types","text":"<pre><code>get_hospital_types()\n</code></pre> <p>Return unique hospital types in the dataset.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def get_hospital_types(self) -&gt; List[str]:\n    \"\"\"Return unique hospital types in the dataset.\"\"\"\n    if self.df is None or 'hospital_type' not in self.df.columns:\n        return []\n    return self.df['hospital_type'].dropna().unique().tolist()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.get_location_categories","title":"get_location_categories","text":"<pre><code>get_location_categories()\n</code></pre> <p>Return unique location categories in the dataset.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def get_location_categories(self) -&gt; List[str]:\n    \"\"\"Return unique location categories in the dataset.\"\"\"\n    if self.df is None or 'location_category' not in self.df.columns:\n        return []\n    return self.df['location_category'].dropna().unique().tolist()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.get_summary_stats","title":"get_summary_stats","text":"<pre><code>get_summary_stats()\n</code></pre> <p>Return summary statistics for the ADT data.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def get_summary_stats(self) -&gt; Dict:\n    \"\"\"Return summary statistics for the ADT data.\"\"\"\n    if self.df is None:\n        return {}\n\n    stats = {\n        'total_records': len(self.df),\n        'unique_hospitalizations': self.df['hospitalization_id'].nunique() if 'hospitalization_id' in self.df.columns else 0,\n        'unique_hospitals': self.df['hospital_id'].nunique() if 'hospital_id' in self.df.columns else 0,\n        'location_category_counts': self.df['location_category'].value_counts().to_dict() if 'location_category' in self.df.columns else {},\n        'hospital_type_counts': self.df['hospital_type'].value_counts().to_dict() if 'hospital_type' in self.df.columns else {},\n        'date_range': {\n            'earliest_in': self.df['in_dttm'].min() if 'in_dttm' in self.df.columns else None,\n            'latest_in': self.df['in_dttm'].max() if 'in_dttm' in self.df.columns else None,\n            'earliest_out': self.df['out_dttm'].min() if 'out_dttm' in self.df.columns else None,\n            'latest_out': self.df['out_dttm'].max() if 'out_dttm' in self.df.columns else None\n        }\n    }\n\n    return stats\n</code></pre>"},{"location":"api/tables/#hospitalization","title":"Hospitalization","text":""},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization","title":"clifpy.tables.hospitalization.Hospitalization","text":"<pre><code>Hospitalization(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Hospitalization table wrapper inheriting from BaseTable.</p> <p>This class handles hospitalization-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the hospitalization table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the hospitalization table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: hospitalization(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.calculate_length_of_stay","title":"calculate_length_of_stay","text":"<pre><code>calculate_length_of_stay()\n</code></pre> <p>Calculate length of stay for each hospitalization and return DataFrame with LOS column.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def calculate_length_of_stay(self) -&gt; pd.DataFrame:\n    \"\"\"Calculate length of stay for each hospitalization and return DataFrame with LOS column.\"\"\"\n    if self.df is None:\n        return pd.DataFrame()\n\n    required_cols = ['admission_dttm', 'discharge_dttm']\n    if not all(col in self.df.columns for col in required_cols):\n        print(f\"Missing required columns: {[col for col in required_cols if col not in self.df.columns]}\")\n        return pd.DataFrame()\n\n    df_copy = self.df.copy()\n    df_copy['admission_dttm'] = pd.to_datetime(df_copy['admission_dttm'])\n    df_copy['discharge_dttm'] = pd.to_datetime(df_copy['discharge_dttm'])\n\n    # Calculate LOS in days\n    df_copy['length_of_stay_days'] = (df_copy['discharge_dttm'] - df_copy['admission_dttm']).dt.total_seconds() / (24 * 3600)\n\n    return df_copy\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_mortality_rate","title":"get_mortality_rate","text":"<pre><code>get_mortality_rate()\n</code></pre> <p>Calculate in-hospital mortality rate.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_mortality_rate(self) -&gt; float:\n    \"\"\"Calculate in-hospital mortality rate.\"\"\"\n    if self.df is None or 'discharge_category' not in self.df.columns:\n        return 0.0\n\n    total_hospitalizations = len(self.df)\n    if total_hospitalizations == 0:\n        return 0.0\n\n    expired_count = len(self.df[self.df['discharge_category'] == 'Expired'])\n    return (expired_count / total_hospitalizations) * 100\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_patient_hospitalization_counts","title":"get_patient_hospitalization_counts","text":"<pre><code>get_patient_hospitalization_counts()\n</code></pre> <p>Return DataFrame with hospitalization counts per patient.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_patient_hospitalization_counts(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with hospitalization counts per patient.\"\"\"\n    if self.df is None or 'patient_id' not in self.df.columns:\n        return pd.DataFrame()\n\n    patient_counts = (self.df.groupby('patient_id')\n                     .agg({\n                         'hospitalization_id': 'count',\n                         'admission_dttm': ['min', 'max']\n                     })\n                     .reset_index())\n\n    # Flatten column names\n    patient_counts.columns = ['patient_id', 'hospitalization_count', 'first_admission', 'last_admission']\n\n    # Calculate span of care\n    patient_counts['first_admission'] = pd.to_datetime(patient_counts['first_admission'])\n    patient_counts['last_admission'] = pd.to_datetime(patient_counts['last_admission'])\n    patient_counts['care_span_days'] = (patient_counts['last_admission'] - patient_counts['first_admission']).dt.total_seconds() / (24 * 3600)\n\n    return patient_counts.sort_values('hospitalization_count', ascending=False)\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_summary_stats","title":"get_summary_stats","text":"<pre><code>get_summary_stats()\n</code></pre> <p>Return comprehensive summary statistics for hospitalization data.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_summary_stats(self) -&gt; Dict:\n    \"\"\"Return comprehensive summary statistics for hospitalization data.\"\"\"\n    if self.df is None:\n        return {}\n\n    stats = {\n        'total_hospitalizations': len(self.df),\n        'unique_patients': self.df['patient_id'].nunique() if 'patient_id' in self.df.columns else 0,\n        'discharge_category_counts': self.df['discharge_category'].value_counts().to_dict() if 'discharge_category' in self.df.columns else {},\n        'admission_type_counts': self.df['admission_type_category'].value_counts().to_dict() if 'admission_type_category' in self.df.columns else {},\n        'date_range': {\n            'earliest_admission': self.df['admission_dttm'].min() if 'admission_dttm' in self.df.columns else None,\n            'latest_admission': self.df['admission_dttm'].max() if 'admission_dttm' in self.df.columns else None,\n            'earliest_discharge': self.df['discharge_dttm'].min() if 'discharge_dttm' in self.df.columns else None,\n            'latest_discharge': self.df['discharge_dttm'].max() if 'discharge_dttm' in self.df.columns else None\n        }\n    }\n\n    # Age statistics\n    if 'age_at_admission' in self.df.columns:\n        age_data = self.df['age_at_admission'].dropna()\n        if not age_data.empty:\n            stats['age_stats'] = {\n                'mean': round(age_data.mean(), 1),\n                'median': age_data.median(),\n                'min': age_data.min(),\n                'max': age_data.max(),\n                'std': round(age_data.std(), 1)\n            }\n\n    # Length of stay statistics\n    if all(col in self.df.columns for col in ['admission_dttm', 'discharge_dttm']):\n        los_df = self.calculate_length_of_stay()\n        if 'length_of_stay_days' in los_df.columns:\n            los_data = los_df['length_of_stay_days'].dropna()\n            if not los_data.empty:\n                stats['length_of_stay_stats'] = {\n                    'mean_days': round(los_data.mean(), 1),\n                    'median_days': round(los_data.median(), 1),\n                    'min_days': round(los_data.min(), 1),\n                    'max_days': round(los_data.max(), 1),\n                    'std_days': round(los_data.std(), 1)\n                }\n\n    # Mortality rate\n    stats['mortality_rate_percent'] = round(self.get_mortality_rate(), 2)\n\n    return stats\n</code></pre>"},{"location":"api/tables/#labs","title":"Labs","text":""},{"location":"api/tables/#clifpy.tables.labs.Labs","title":"clifpy.tables.labs.Labs","text":"<pre><code>Labs(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Labs table wrapper inheriting from BaseTable.</p> <p>This class handles laboratory data and validations including reference unit validation while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the labs table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the labs table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: labs(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Initialize lab reference units\n    self._lab_reference_units = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load lab-specific schema data\n    self._load_labs_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.lab_reference_units","title":"lab_reference_units  <code>property</code>","text":"<pre><code>lab_reference_units\n</code></pre> <p>Get the lab reference units mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_category_stats","title":"get_lab_category_stats","text":"<pre><code>get_lab_category_stats()\n</code></pre> <p>Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_category_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.\"\"\"\n    if (\n        self.df is None\n        or 'lab_value_numeric' not in self.df.columns\n        or 'hospitalization_id' not in self.df.columns        # remove this line if hosp-id is optional\n    ):\n        return {\"status\": \"Missing columns\"}\n\n    stats = (\n        self.df\n        .groupby('lab_category')\n        .agg(\n            count=('lab_value_numeric', 'count'),\n            unique=('hospitalization_id', 'nunique'),\n            missing_pct=('lab_value_numeric', lambda x: 100 * x.isna().mean()),\n            mean=('lab_value_numeric', 'mean'),\n            std=('lab_value_numeric', 'std'),\n            min=('lab_value_numeric', 'min'),\n            q1=('lab_value_numeric', lambda x: x.quantile(0.25)),\n            median=('lab_value_numeric', 'median'),\n            q3=('lab_value_numeric', lambda x: x.quantile(0.75)),\n            max=('lab_value_numeric', 'max'),\n        )\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_specimen_stats","title":"get_lab_specimen_stats","text":"<pre><code>get_lab_specimen_stats()\n</code></pre> <p>Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_specimen_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.\"\"\"\n    if (\n        self.df is None\n        or 'lab_value_numeric' not in self.df.columns\n        or 'hospitalization_id' not in self.df.columns \n        or 'lab_speciment_category' not in self.df.columns       # remove this line if hosp-id is optional\n    ):\n        return {\"status\": \"Missing columns\"}\n\n    stats = (\n        self.df\n        .groupby('lab_specimen_category')\n        .agg(\n            count=('lab_value_numeric', 'count'),\n            unique=('hospitalization_id', 'nunique'),\n            missing_pct=('lab_value_numeric', lambda x: 100 * x.isna().mean()),\n            mean=('lab_value_numeric', 'mean'),\n            std=('lab_value_numeric', 'std'),\n            min=('lab_value_numeric', 'min'),\n            q1=('lab_value_numeric', lambda x: x.quantile(0.25)),\n            median=('lab_value_numeric', 'median'),\n            q3=('lab_value_numeric', lambda x: x.quantile(0.75)),\n            max=('lab_value_numeric', 'max'),\n        )\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#microbiology-culture","title":"Microbiology Culture","text":""},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture","title":"clifpy.tables.microbiology_culture.MicrobiologyCulture","text":"<pre><code>MicrobiologyCulture(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Microbiology Culture table wrapper inheriting from BaseTable.</p> <p>This class handles microbiology culture-specific data and validations including organism identification validation and culture method validation.</p> <p>Initialize the microbiology culture table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the microbiology culture table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Initialize time order validation errors list\n    self.time_order_validation_errors: List[dict] = []\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.cat_vs_name_map","title":"cat_vs_name_map  <code>staticmethod</code>","text":"<pre><code>cat_vs_name_map(\n    df,\n    category_col,\n    name_col,\n    *,\n    group_col=None,\n    dropna=True,\n    sort=\"freq_then_alpha\",\n    max_names_per_cat=None,\n    include_counts=False\n)\n</code></pre> <p>Build mappings from category\u2192names (2-level) or group\u2192category\u2192names (3-level).</p> <ul> <li>if group_col is None:         { category: [names...] }  or  { category: [{\"name\":..., \"n\":...}, ...] }</li> <li>if group_col is provided:         { group: { category: [names...] } }  or         { group: { category: [{\"name\":..., \"n\":...}, ...] } }</li> </ul> <p>Notes - Names are unique per (category[, group]) and sorted by:     freq desc, then alpha  (default), or alpha only if sort=\"alpha\" - Set include_counts=True to return [{\"name\":..., \"n\":...}] instead of plain strings. - Set max_names_per_cat to truncate long lists per category.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>@staticmethod\ndef cat_vs_name_map(\n    df: pd.DataFrame,\n    category_col: str,\n    name_col: str,\n    *,\n    group_col: Optional[str] = None,                 # \u2190 if provided, returns {group: {cat: [names...]}}\n    dropna: bool = True,\n    sort: Literal[\"freq_then_alpha\", \"alpha\"] = \"freq_then_alpha\",\n    max_names_per_cat: Optional[int] = None,\n    include_counts: bool = False,                    # if True \u2192 lists of {\"name\":..., \"n\":...}\n) -&gt; Union[Dict[str, List[str]], Dict[str, Dict[str, List[str]]],\n        Dict[str, Dict[str, List[Dict[str, int]]]], Dict[str, List[Dict[str, int]]]]:\n    \"\"\"\n    Build mappings from category\u2192names (2-level) or group\u2192category\u2192names (3-level).\n\n    Returns:\n    - if group_col is None:\n            { category: [names...] }  or  { category: [{\"name\":..., \"n\":...}, ...] }\n    - if group_col is provided:\n            { group: { category: [names...] } }  or\n            { group: { category: [{\"name\":..., \"n\":...}, ...] } }\n\n    Notes\n    - Names are unique per (category[, group]) and sorted by:\n        freq desc, then alpha  (default), or alpha only if sort=\"alpha\"\n    - Set include_counts=True to return [{\"name\":..., \"n\":...}] instead of plain strings.\n    - Set max_names_per_cat to truncate long lists per category.\n    \"\"\"\n    if df is None:\n        return {}\n\n    required = [category_col, name_col] + ([group_col] if group_col else [])\n    if any(col not in df.columns for col in required):\n        return {}\n\n    sub = df[required].copy()\n    if dropna:\n        sub = sub.dropna(subset=required)\n\n    # frequency at the most granular level available\n    group_by_cols = ([group_col] if group_col else []) + [category_col, name_col]\n    counts = (\n        sub.groupby(group_by_cols)\n        .size()\n        .reset_index(name=\"n\")\n    )\n\n    def _sort_block(block: pd.DataFrame) -&gt; pd.DataFrame:\n        if sort == \"alpha\":\n            return block.sort_values([name_col], ascending=[True], kind=\"mergesort\")\n        # default: freq desc then alpha\n        return block.sort_values([\"n\", name_col], ascending=[False, True], kind=\"mergesort\")\n\n    def _emit_names(block: pd.DataFrame):\n        if include_counts:\n            out = [{\"name\": str(r[name_col]), \"n\": int(r[\"n\"])} for _, r in block.iterrows()]\n        else:\n            out = block[name_col].astype(str).tolist()\n        if max_names_per_cat is not None:\n            out = out[:max_names_per_cat]\n        return out\n\n    if group_col:\n        # 3-level: group \u2192 category \u2192 [names or {\"name\",\"n\"}]\n        result: Dict[str, Dict[str, List[Union[str, Dict[str, int]]]]] = {}\n        for grp_val, grp_block in counts.groupby(group_col, sort=False):\n            cat_map: Dict[str, List[Union[str, Dict[str, int]]]] = {}\n            for cat_val, cat_block in grp_block.groupby(category_col, sort=False):\n                sorted_block = _sort_block(cat_block)\n                cat_map[str(cat_val)] = _emit_names(sorted_block)\n            result[str(grp_val)] = cat_map\n        return result\n    else:\n        # 2-level: category \u2192 [names or {\"name\",\"n\"}]\n        result2: Dict[str, List[Union[str, Dict[str, int]]]] = {}\n        for cat_val, cat_block in counts.groupby(category_col, sort=False):\n            sorted_block = _sort_block(cat_block)\n            result2[str(cat_val)] = _emit_names(sorted_block)\n        return result2\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Return <code>True</code> if the last validation finished without errors.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"Return ``True`` if the last validation finished without errors.\"\"\"\n    return not self.errors and not self.time_order_validation_errors\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.top_fluid_org_outliers","title":"top_fluid_org_outliers","text":"<pre><code>top_fluid_org_outliers(\n    level=\"organism_group\", min_count=0, top_k=10\n)\n</code></pre> <p>Identify top positive and negative outliers in fluid_category vs organism_group or organism_category.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>\"organism_group\" or \"organism_category\" (non-standard)</p> <code>'organism_group'</code> <code>min_count</code> <code>int</code> <p>Minimum observed count to consider</p> <code>0</code> <code>top_k</code> <code>int</code> <p>Number of top positive and negative outliers to return</p> <code>10</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dict with keys \"top_positive\" and \"top_negative\", each containing a DataFrame of outliers.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def top_fluid_org_outliers(\n    self,\n    level: Literal[\"organism_group\", \"organism_category\"] = \"organism_group\",\n    min_count: int = 0,\n    top_k: int = 10,\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Identify top positive and negative outliers in fluid_category vs organism_group or organism_category.\n\n    Parameters:\n        level (str): \"organism_group\" or \"organism_category\" (non-standard)\n        min_count (int): Minimum observed count to consider\n        top_k (int): Number of top positive and negative outliers to return\n\n    Returns:\n        Dict with keys \"top_positive\" and \"top_negative\", each containing a DataFrame of outliers.\n    \"\"\"\n    tbl = pd.crosstab(self.df[\"fluid_category\"], self.df[level])\n    if tbl.empty:\n        return {\"top_positive\": pd.DataFrame(), \"top_negative\": pd.DataFrame()}\n\n    total = tbl.values.sum()\n    exp = (tbl.sum(1).values.reshape(-1,1) @ tbl.sum(0).values.reshape(1,-1)) / total\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        z = (tbl.values - exp) / np.sqrt(exp)\n\n    long = pd.DataFrame({\n        \"fluid_category\": np.repeat(tbl.index.values, tbl.shape[1]),\n        level: np.tile(tbl.columns.values, tbl.shape[0]),\n        \"observed\": tbl.values.ravel().astype(float),\n        \"expected\": exp.ravel().astype(float),\n        \"std_resid\": z.ravel().astype(float),\n    }).dropna()\n\n    long = long[long[\"observed\"] &gt;= min_count]\n    top_pos = long.sort_values(\"std_resid\", ascending=False).head(top_k).reset_index(drop=True)\n    top_neg = long.sort_values(\"std_resid\", ascending=True).head(top_k).reset_index(drop=True)\n    return {\"top_positive\": top_pos, \"top_negative\": top_neg}\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.validate_timestamp_order","title":"validate_timestamp_order","text":"<pre><code>validate_timestamp_order()\n</code></pre> <p>Check that order_dttm \u2264 collect_dttm \u2264 result_dttm. - Resets self.time_order_validation_errors - Adds one entry per violated rule - Extends self.errors and logs: 'Found {len(self.time_order_validation_errors)} time order validation errors' Returns a dataframe of all violating rows (union of both rules) or None if OK.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def validate_timestamp_order(self):\n    \"\"\"\n    Check that order_dttm \u2264 collect_dttm \u2264 result_dttm.\n    - Resets self.time_order_validation_errors\n    - Adds one entry per violated rule\n    - Extends self.errors and logs: 'Found {len(self.time_order_validation_errors)} time order validation errors'\n    Returns a dataframe of all violating rows (union of both rules) or None if OK.\n    \"\"\"\n    # Reset time order validation bucket\n    self.time_order_validation_errors = []\n\n    df = self.df\n    key_cols = self.schema['composite_keys']\n\n    grace = pd.Timedelta(minutes=1)\n\n    # Flag if order is \u2265 1 minute after collect (allow small jitter where collect \u2265 order within 1 min)\n    m_order_ge_collect  = (df[\"order_dttm\"]   - df[\"collect_dttm\"]) &gt;= grace\n\n    # Flag if collect is \u2265 1 minute after result (allow small jitter where result \u2265 collect within 1 min)\n    m_collect_ge_result = (df[\"collect_dttm\"] - df[\"result_dttm\"]) &gt;= grace\n\n    n1 = int(m_order_ge_collect.sum())\n    n2 = int(m_collect_ge_result.sum())\n\n    if n1 &gt; 0:\n        self.time_order_validation_errors.append({\n            \"type\": \"time_order_validation\",\n            \"rule\": \"order_dttm &lt;= collect_dttm, grace 1 min\",\n            \"message\": f\"{n1} rows have order_dttm &gt; collect_dttm\",\n            \"rows\": n1,\n            \"table\": getattr(self, \"table_name\", \"unknown\"),\n        })\n    if n2 &gt; 0:\n        self.time_order_validation_errors.append({\n            \"type\": \"time_order_validation\",\n            \"rule\": \"collect_dttm &lt;= result_dttm, grace 1 min\",\n            \"message\": f\"{n2} rows have collect_dttm &gt; result_dttm\",\n            \"rows\": n2,\n            \"table\": getattr(self, \"table_name\", \"unknown\"),\n        })\n\n    # Add range validation errors to main errors list (exact logging style)\n    if self.time_order_validation_errors:\n        if hasattr(self, \"errors\"):\n            self.errors.extend(self.time_order_validation_errors)\n        self.logger.warning(f\"Found {len(self.time_order_validation_errors)} range validation errors\")\n\n    # Return violating rows (union), showing keys + timestamps\n    any_bad = m_order_ge_collect | m_collect_ge_result\n    if any_bad.any():\n        show_cols = [*key_cols, \"order_dttm\", \"collect_dttm\", \"result_dttm\"]\n        return df.loc[any_bad, [c for c in show_cols if c in df.columns]].copy()\n\n    # Nothing to report\n    self.logger.info(\"validate_timestamp_order: passed (no violations)\")\n    return None\n</code></pre>"},{"location":"api/tables/#vitals","title":"Vitals","text":""},{"location":"api/tables/#clifpy.tables.vitals.Vitals","title":"clifpy.tables.vitals.Vitals","text":"<pre><code>Vitals(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Vitals table wrapper inheriting from BaseTable.</p> <p>This class handles vitals-specific data and validations including range validation for vital signs.</p> <p>Initialize the vitals table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the vitals table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Initialize range validation errors list\n    self.range_validation_errors: List[dict] = []\n\n    # Load vital ranges and units from schema\n    self._vital_units = None\n    self._vital_ranges = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load vital-specific schema data\n    self._load_vitals_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.vital_ranges","title":"vital_ranges  <code>property</code>","text":"<pre><code>vital_ranges\n</code></pre> <p>Get the vital ranges from the schema.</p>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.vital_units","title":"vital_units  <code>property</code>","text":"<pre><code>vital_units\n</code></pre> <p>Get the vital units mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.filter_by_vital_category","title":"filter_by_vital_category","text":"<pre><code>filter_by_vital_category(vital_category)\n</code></pre> <p>Return all records for a specific vital category (e.g., 'heart_rate', 'temp_c').</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def filter_by_vital_category(self, vital_category: str) -&gt; pd.DataFrame:\n    \"\"\"Return all records for a specific vital category (e.g., 'heart_rate', 'temp_c').\"\"\"\n    if self.df is None or 'vital_category' not in self.df.columns:\n        return pd.DataFrame()\n\n    return self.df[self.df['vital_category'] == vital_category].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.get_vital_summary_stats","title":"get_vital_summary_stats","text":"<pre><code>get_vital_summary_stats()\n</code></pre> <p>Return summary statistics for each vital category.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def get_vital_summary_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each vital category.\"\"\"\n    if self.df is None or 'vital_value' not in self.df.columns:\n        return pd.DataFrame()\n\n    # Convert vital_value to numeric\n    df_copy = self.df.copy()\n    df_copy['vital_value'] = pd.to_numeric(df_copy['vital_value'], errors='coerce')\n\n    # Group by vital category and calculate stats\n    stats = df_copy.groupby('vital_category')['vital_value'].agg([\n        'count', 'mean', 'std', 'min', 'max',\n        ('q1', lambda x: x.quantile(0.25)),\n        ('median', lambda x: x.quantile(0.5)),\n        ('q3', lambda x: x.quantile(0.75))\n    ]).round(2)\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Return <code>True</code> if the last validation finished without errors.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"Return ``True`` if the last validation finished without errors.\"\"\"\n    return not self.errors and not self.range_validation_errors\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.validate_vital_ranges","title":"validate_vital_ranges","text":"<pre><code>validate_vital_ranges()\n</code></pre> <p>Validate vital values against expected ranges using grouped data for efficiency.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def validate_vital_ranges(self):\n    \"\"\"Validate vital values against expected ranges using grouped data for efficiency.\"\"\"\n    self.range_validation_errors = []\n\n    if self.df is None or not self._vital_ranges:\n        return\n\n    required_columns = ['vital_category', 'vital_value']\n    required_columns_for_df = ['vital_category', 'vital_value']\n    if not all(col in self.df.columns for col in required_columns_for_df):\n        self.range_validation_errors.append({\n            \"error_type\": \"missing_columns_for_range_validation\",\n            \"columns\": [col for col in required_columns_for_df if col not in self.df.columns],\n            \"message\": \"vital_category or vital_value column missing, cannot perform range validation.\"\n        })\n        return\n\n    # Work on a copy to safely convert vital_value to numeric for aggregation\n    df_for_stats = self.df[required_columns_for_df].copy()\n    df_for_stats['vital_value'] = pd.to_numeric(df_for_stats['vital_value'], errors='coerce')\n\n    # Filter out rows where vital_value could not be converted\n    df_for_stats.dropna(subset=['vital_value'], inplace=True)\n\n    if df_for_stats.empty:\n        # No numeric vital_value data to perform range validation on\n        return\n\n    vital_stats = (df_for_stats\n                   .groupby('vital_category')['vital_value']\n                   .agg(['min', 'max', 'mean', 'count'])\n                   .reset_index())\n\n    if vital_stats.empty:\n        return\n\n    # Check each vital category's ranges\n    for _, row in vital_stats.iterrows():\n        vital_category = row['vital_category']\n        min_val = row['min']\n        max_val = row['max']\n        count = row['count']\n        mean_val = row['mean']\n\n        # Check if vital category has defined ranges\n        if vital_category not in self._vital_ranges:\n            self.range_validation_errors.append({\n                'error_type': 'unknown_vital_category',\n                'vital_category': vital_category,\n                'affected_rows': count,\n                'observed_min': min_val,\n                'observed_max': max_val,\n                'message': f\"Unknown vital category '{vital_category}' found in data.\"\n            })\n            continue\n\n        expected_range = self._vital_ranges[vital_category]\n        expected_min = expected_range.get('min')\n        expected_max = expected_range.get('max')\n\n        # Check if any values are outside the expected range\n        if expected_min is not None and min_val &lt; expected_min:\n            self.range_validation_errors.append({\n                'error_type': 'below_range',\n                'vital_category': vital_category,\n                'observed_min': min_val,\n                'expected_min': expected_min,\n                'message': f\"Values below expected minimum for {vital_category}\"\n            })\n\n        if expected_max is not None and max_val &gt; expected_max:\n            self.range_validation_errors.append({\n                'error_type': 'above_range',\n                'vital_category': vital_category,\n                'observed_max': max_val,\n                'expected_max': expected_max,\n                'message': f\"Values above expected maximum for {vital_category}\"\n            })\n\n    # Add range validation errors to main errors list\n    if self.range_validation_errors:\n        self.errors.extend(self.range_validation_errors)\n        self.logger.warning(f\"Found {len(self.range_validation_errors)} range validation errors\")\n</code></pre>"},{"location":"api/tables/#respiratory-support","title":"Respiratory Support","text":""},{"location":"api/tables/#clifpy.tables.respiratory_support.RespiratorySupport","title":"clifpy.tables.respiratory_support.RespiratorySupport","text":"<pre><code>RespiratorySupport(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Respiratory support table wrapper inheriting from BaseTable.</p> <p>This class handles respiratory support data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the respiratory_support table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/respiratory_support.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the respiratory_support table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.respiratory_support.RespiratorySupport.waterfall","title":"waterfall","text":"<pre><code>waterfall(\n    *,\n    id_col=\"hospitalization_id\",\n    bfill=False,\n    verbose=True,\n    return_dataframe=False\n)\n</code></pre> <p>Clean + waterfall-fill the respiratory_support table.</p> <p>Parameters:</p> Name Type Description Default <code>id_col</code> <code>str</code> <p>Encounter-level identifier column (default: hospitalization_id)</p> <code>'hospitalization_id'</code> <code>bfill</code> <code>bool</code> <p>If True, numeric setters are back-filled after forward-fill</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print progress messages</p> <code>True</code> <code>return_dataframe</code> <code>bool</code> <p>If True, returns DataFrame instead of RespiratorySupport instance</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RespiratorySupport</code> <code>Union[RespiratorySupport, DataFrame]</code> <p>New instance with processed data (or DataFrame if return_dataframe=True)</p> Note <p>The waterfall function expects data in UTC timezone. If your data is in a different timezone, it will be converted to UTC for processing, then converted back to the original timezone on return. The original object is not modified.</p> Source code in <code>clifpy/tables/respiratory_support.py</code> <pre><code>    def waterfall(\n    self,\n    *,\n    id_col: str = \"hospitalization_id\",\n    bfill: bool = False,\n    verbose: bool = True,\n    return_dataframe: bool = False\n) -&gt; Union['RespiratorySupport', pd.DataFrame]:\n        \"\"\"\n        Clean + waterfall-fill the respiratory_support table.\n\n        Parameters:\n            id_col (str): Encounter-level identifier column (default: hospitalization_id)\n            bfill (bool): If True, numeric setters are back-filled after forward-fill\n            verbose (bool): Print progress messages\n            return_dataframe (bool): If True, returns DataFrame instead of RespiratorySupport instance\n\n        Returns:\n            RespiratorySupport: New instance with processed data (or DataFrame if return_dataframe=True)\n\n        Note:\n            The waterfall function expects data in UTC timezone. If your data is in a\n            different timezone, it will be converted to UTC for processing, then converted\n            back to the original timezone on return. The original object is not modified.\n        \"\"\"\n        if self.df is None or self.df.empty:\n            raise ValueError(\"No data available to process. Load data first.\")\n\n        # Work on a copy\n        df_copy = self.df.copy()\n\n        # --- Capture original tz (if any), convert to UTC for processing\n        original_tz = None\n        if 'recorded_dttm' in df_copy.columns:\n            if pd.api.types.is_datetime64tz_dtype(df_copy['recorded_dttm']):\n                original_tz = df_copy['recorded_dttm'].dt.tz\n                if verbose and str(original_tz) != 'UTC':\n                    print(f\"Converting timezone from {original_tz} to UTC for waterfall processing\")\n                # Convert to UTC (no-op if already UTC)\n                df_copy['recorded_dttm'] = df_copy['recorded_dttm'].dt.tz_convert('UTC')\n            else:\n                # tz-naive; leave as-is (function expects UTC semantics already)\n                original_tz = None\n\n        # --- Run the waterfall (expects UTC)\n        processed_df = process_resp_support_waterfall(\n            df_copy,\n            id_col=id_col,\n            bfill=bfill,\n            verbose=verbose\n        )\n\n        # --- Convert back to original tz if we had one\n        if original_tz is not None:\n            # Guard: ensure tz-aware before tz_convert\n            if pd.api.types.is_datetime64tz_dtype(processed_df['recorded_dttm']):\n                if verbose and str(original_tz) != 'UTC':\n                    print(f\"Converting timezone from UTC back to {original_tz} after processing\")\n                processed_df = processed_df.copy()\n                processed_df['recorded_dttm'] = processed_df['recorded_dttm'].dt.tz_convert(original_tz)\n            else:\n                # If something made it tz-naive, localize then convert (shouldn't happen, but safe)\n                processed_df = processed_df.copy()\n                processed_df['recorded_dttm'] = (\n                    processed_df['recorded_dttm']\n                    .dt.tz_localize('UTC')\n                    .dt.tz_convert(original_tz)\n                )\n\n        # Return DataFrame if requested\n        if return_dataframe:\n            return processed_df\n\n        # Otherwise, return a new wrapped instance (timezone metadata stays the same as the current object)\n        return RespiratorySupport(\n            data_directory=self.data_directory,\n            filetype=self.filetype,\n            timezone=self.timezone,  # this is your package-level field; we keep it unchanged\n            output_directory=self.output_directory,\n            data=processed_df\n        )\n</code></pre>"},{"location":"api/tables/#medication-administration-continuous","title":"Medication Administration (Continuous)","text":""},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous","title":"clifpy.tables.medication_admin_continuous.MedicationAdminContinuous","text":"<pre><code>MedicationAdminContinuous(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Medication administration continuous table wrapper inheriting from BaseTable.</p> <p>This class handles medication administration continuous data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the MedicationAdminContinuous table.</p> <p>This class handles continuous medication administration data, including validation, dose unit standardization, and unit conversion capabilities.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous--parameters","title":"Parameters","text":"<p>data_directory : str, optional     Path to the directory containing data files. If None and data is provided,     defaults to current directory. filetype : str, optional     Type of data file (csv, parquet, etc.). If None and data is provided,     defaults to 'parquet'. timezone : str, default=\"UTC\"     Timezone for datetime columns. Used for proper timestamp handling. output_directory : str, optional     Directory for saving output files and logs. If not specified, outputs     are saved to the current working directory. data : pd.DataFrame, optional     Pre-loaded DataFrame to use instead of loading from file. Supports     backward compatibility with direct DataFrame initialization.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous--notes","title":"Notes","text":"<p>The class supports two initialization patterns: 1. Loading from file: provide data_directory and filetype 2. Direct DataFrame: provide data parameter (legacy support)</p> <p>Upon initialization, the class loads medication schema data including category-to-group mappings from the YAML schema.</p> Source code in <code>clifpy/tables/medication_admin_continuous.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the MedicationAdminContinuous table.\n\n    This class handles continuous medication administration data, including validation,\n    dose unit standardization, and unit conversion capabilities.\n\n    Parameters\n    ----------\n    data_directory : str, optional\n        Path to the directory containing data files. If None and data is provided,\n        defaults to current directory.\n    filetype : str, optional\n        Type of data file (csv, parquet, etc.). If None and data is provided,\n        defaults to 'parquet'.\n    timezone : str, default=\"UTC\"\n        Timezone for datetime columns. Used for proper timestamp handling.\n    output_directory : str, optional\n        Directory for saving output files and logs. If not specified, outputs\n        are saved to the current working directory.\n    data : pd.DataFrame, optional\n        Pre-loaded DataFrame to use instead of loading from file. Supports\n        backward compatibility with direct DataFrame initialization.\n\n    Notes\n    -----\n    The class supports two initialization patterns:\n    1. Loading from file: provide data_directory and filetype\n    2. Direct DataFrame: provide data parameter (legacy support)\n\n    Upon initialization, the class loads medication schema data including\n    category-to-group mappings from the YAML schema.\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: medication_admin_continuous(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Load medication mappings\n    self._med_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load medication-specific schema data\n    self._load_medication_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping","title":"med_category_to_group_mapping  <code>property</code>","text":"<pre><code>med_category_to_group_mapping\n</code></pre> <p>Get the medication category to group mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping--returns","title":"Returns","text":"<p>Dict[str, str]     A dictionary mapping medication categories to their therapeutic groups.     Returns a copy to prevent external modification of the internal mapping.     Returns an empty dict if no mappings are loaded.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping--examples","title":"Examples","text":"<p>mac = MedicationAdminContinuous(data) mappings = mac.med_category_to_group_mapping mappings['Antibiotics'] 'Antimicrobials'</p>"},{"location":"api/tables/#patient-assessments","title":"Patient Assessments","text":""},{"location":"api/tables/#clifpy.tables.patient_assessments.PatientAssessments","title":"clifpy.tables.patient_assessments.PatientAssessments","text":"<pre><code>PatientAssessments(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient assessments table wrapper inheriting from BaseTable.</p> <p>This class handles patient assessment data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient_assessments table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient_assessments.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient_assessments table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: patient_assessments(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Initialize assessment mappings\n    self._assessment_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load assessment-specific schema data\n    self._load_assessment_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.patient_assessments.PatientAssessments.assessment_category_to_group_mapping","title":"assessment_category_to_group_mapping  <code>property</code>","text":"<pre><code>assessment_category_to_group_mapping\n</code></pre> <p>Get the assessment category to group mapping from the schema.</p>"},{"location":"api/tables/#position","title":"Position","text":""},{"location":"api/tables/#clifpy.tables.position.Position","title":"clifpy.tables.position.Position","text":"<pre><code>Position(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Position table wrapper inheriting from BaseTable.</p> <p>This class handles patient position data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the position table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/position.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the position table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: position(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.position.Position.get_position_category_stats","title":"get_position_category_stats","text":"<pre><code>get_position_category_stats()\n</code></pre> <p>Return summary statistics for each position category, including missingness and unique patient counts. Expects columns: 'position_category', 'position_name', and optionally 'hospitalization_id'.</p> Source code in <code>clifpy/tables/position.py</code> <pre><code>def get_position_category_stats(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Return summary statistics for each position category, including missingness and unique patient counts.\n    Expects columns: 'position_category', 'position_name', and optionally 'hospitalization_id'.\n    \"\"\"\n    if self.df is None or 'position_category' not in self.df.columns or 'hospitalization_id' not in self.df.columns:\n        return {\"status\": \"Missing columns\"}\n\n    agg_dict = {\n        'count': ('position_category', 'count'),\n        'unique': ('hospitalization_id', 'nunique'),\n    }\n\n    stats = (\n        self.df\n        .groupby('position_category')\n        .agg(**agg_dict)\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/utilities/","title":"Utilities API Reference","text":"<p>CLIFpy provides several utility modules to support data processing and analysis tasks.</p>"},{"location":"api/utilities/#med-unit-converter","title":"Med Unit Converter","text":"<p>The unit converter module provides comprehensive medication dose unit conversion functionality.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category","title":"clifpy.utils.unit_converter.convert_dose_units_by_med_category","text":"<pre><code>convert_dose_units_by_med_category(\n    med_df,\n    vitals_df=None,\n    preferred_units=None,\n    show_intermediate=False,\n    override=False,\n)\n</code></pre> <p>Convert medication dose units to user-defined preferred units for each med_category.</p> <p>This function performs a two-step conversion process:</p> <ol> <li>Standardizes all dose units to a base set of standard units (mcg/min, ml/min, u/min for rates)</li> <li>Converts from base units to medication-specific preferred units if provided</li> </ol> <p>The conversion maintains unit class consistency (rates stay rates, amounts stay amounts) and handles weight-based dosing appropriately using patient weights.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category--parameters","title":"Parameters","text":"<p>med_df : pd.DataFrame     Medication DataFrame with required columns:</p> <pre><code>- med_dose: Original dose values (numeric)\n- med_dose_unit: Original dose unit strings (e.g., 'MCG/KG/HR', 'mL/hr')\n- med_category: Medication category identifier (e.g., 'propofol', 'fentanyl')\n- weight_kg: Patient weight in kg (optional, will be extracted from vitals_df if missing)\n</code></pre> <p>vitals_df : pd.DataFrame, optional     Vitals DataFrame for extracting patient weights if not in med_df.     Required columns if weight_kg missing from med_df:</p> <pre><code>- hospitalization_id: Patient identifier\n- recorded_dttm: Timestamp of vital recording\n- vital_category: Must include 'weight_kg' values\n- vital_value: Weight values\n</code></pre> <p>preferred_units : dict, optional     Dictionary mapping medication categories to their preferred units.     Keys are medication category names, values are target unit strings.     Example: {'propofol': 'mcg/kg/min', 'fentanyl': 'mcg/hr', 'insulin': 'u/hr'}     If None, uses base units (mcg/min, ml/min, u/min) as defaults. show_intermediate : bool, default False     If False, excludes intermediate calculation columns (multipliers) from output.     If True, retains all columns including conversion multipliers for debugging. override : bool, default False     If True, prints warning messages for unacceptable preferred units but continues processing.     If False, raises ValueError when encountering unacceptable preferred units.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category--returns","title":"Returns","text":"<p>Tuple[pd.DataFrame, pd.DataFrame]     A tuple containing:</p> <pre><code>- [0] Converted medication DataFrame with additional columns:\n\n    * _clean_unit: Cleaned unit format\n    * _base_unit: Base unit after first conversion\n    * _base_dose: Dose value in base units\n    * _preferred_unit: Target unit for medication category\n    * med_dose_converted: Final dose value in preferred units\n    * med_dose_unit_converted: Final unit string after conversion\n    * _unit_class: Classification ('rate', 'amount', or 'unrecognized')\n    * _convert_status: Status message indicating success or reason for failure\n\n    If show_intermediate=True, also includes conversion multipliers.\n\n- [1] Summary counts DataFrame with conversion statistics grouped by medication category\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category--raises","title":"Raises","text":"<p>ValueError     If required columns (med_dose_unit, med_dose) are missing from med_df,     if standardization to base units fails, or if conversion to preferred units fails.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category--examples","title":"Examples","text":"<p>import pandas as pd med_df = pd.DataFrame({ ...     'med_category': ['propofol', 'fentanyl', 'insulin'], ...     'med_dose': [200, 2, 5], ...     'med_dose_unit': ['MCG/KG/MIN', 'mcg/kg/hr', 'units/hr'], ...     'weight_kg': [70, 80, 75] ... }) preferred = { ...     'propofol': 'mcg/kg/min', ...     'fentanyl': 'mcg/hr', ...     'insulin': 'u/hr' ... } result_df, counts_df = convert_dose_units_by_med_category(med_df, preferred_units=preferred)</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category--notes","title":"Notes","text":"<p>The function handles various unit formats including:</p> <ul> <li>Weight-based dosing: /kg, /lb (uses patient weight for conversion)</li> <li>Time conversions: /hr to /min</li> <li>Volume conversions: L to mL</li> <li>Mass conversions: mg, ng, g to mcg</li> <li>Unit conversions: milli-units (mu) to units (u)</li> </ul> <p>Unrecognized units are preserved but flagged in the _unit_class column.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category--todo","title":"Todo","text":"<p>Implement config file parsing for default preferred_units.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def convert_dose_units_by_med_category(\n    med_df: pd.DataFrame,\n    vitals_df: pd.DataFrame = None,\n    preferred_units: dict = None,\n    show_intermediate: bool = False,\n    override: bool = False\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Convert medication dose units to user-defined preferred units for each med_category.\n\n    This function performs a two-step conversion process:\n\n    1. Standardizes all dose units to a base set of standard units (mcg/min, ml/min, u/min for rates)\n    2. Converts from base units to medication-specific preferred units if provided\n\n    The conversion maintains unit class consistency (rates stay rates, amounts stay amounts)\n    and handles weight-based dosing appropriately using patient weights.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        Medication DataFrame with required columns:\n\n        - med_dose: Original dose values (numeric)\n        - med_dose_unit: Original dose unit strings (e.g., 'MCG/KG/HR', 'mL/hr')\n        - med_category: Medication category identifier (e.g., 'propofol', 'fentanyl')\n        - weight_kg: Patient weight in kg (optional, will be extracted from vitals_df if missing)\n    vitals_df : pd.DataFrame, optional\n        Vitals DataFrame for extracting patient weights if not in med_df.\n        Required columns if weight_kg missing from med_df:\n\n        - hospitalization_id: Patient identifier\n        - recorded_dttm: Timestamp of vital recording\n        - vital_category: Must include 'weight_kg' values\n        - vital_value: Weight values\n    preferred_units : dict, optional\n        Dictionary mapping medication categories to their preferred units.\n        Keys are medication category names, values are target unit strings.\n        Example: {'propofol': 'mcg/kg/min', 'fentanyl': 'mcg/hr', 'insulin': 'u/hr'}\n        If None, uses base units (mcg/min, ml/min, u/min) as defaults.\n    show_intermediate : bool, default False\n        If False, excludes intermediate calculation columns (multipliers) from output.\n        If True, retains all columns including conversion multipliers for debugging.\n    override : bool, default False\n        If True, prints warning messages for unacceptable preferred units but continues processing.\n        If False, raises ValueError when encountering unacceptable preferred units.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        A tuple containing:\n\n        - [0] Converted medication DataFrame with additional columns:\n\n            * _clean_unit: Cleaned unit format\n            * _base_unit: Base unit after first conversion\n            * _base_dose: Dose value in base units\n            * _preferred_unit: Target unit for medication category\n            * med_dose_converted: Final dose value in preferred units\n            * med_dose_unit_converted: Final unit string after conversion\n            * _unit_class: Classification ('rate', 'amount', or 'unrecognized')\n            * _convert_status: Status message indicating success or reason for failure\n\n            If show_intermediate=True, also includes conversion multipliers.\n\n        - [1] Summary counts DataFrame with conversion statistics grouped by medication category\n\n    Raises\n    ------\n    ValueError\n        If required columns (med_dose_unit, med_dose) are missing from med_df,\n        if standardization to base units fails, or if conversion to preferred units fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; med_df = pd.DataFrame({\n    ...     'med_category': ['propofol', 'fentanyl', 'insulin'],\n    ...     'med_dose': [200, 2, 5],\n    ...     'med_dose_unit': ['MCG/KG/MIN', 'mcg/kg/hr', 'units/hr'],\n    ...     'weight_kg': [70, 80, 75]\n    ... })\n    &gt;&gt;&gt; preferred = {\n    ...     'propofol': 'mcg/kg/min',\n    ...     'fentanyl': 'mcg/hr',\n    ...     'insulin': 'u/hr'\n    ... }\n    &gt;&gt;&gt; result_df, counts_df = convert_dose_units_by_med_category(med_df, preferred_units=preferred)\n\n    Notes\n    -----\n    The function handles various unit formats including:\n\n    - Weight-based dosing: /kg, /lb (uses patient weight for conversion)\n    - Time conversions: /hr to /min\n    - Volume conversions: L to mL\n    - Mass conversions: mg, ng, g to mcg\n    - Unit conversions: milli-units (mu) to units (u)\n\n    Unrecognized units are preserved but flagged in the _unit_class column.\n\n    Todo\n    ----\n    Implement config file parsing for default preferred_units.\n    \"\"\"\n    try:\n        med_df_base, _ = standardize_dose_to_base_units(med_df, vitals_df)\n    except ValueError as e:\n        raise ValueError(f\"Error standardizing dose units to base units: {e}\")\n\n    # check if the requested med_categories are in the input med_df\n    requested_med_categories = set(preferred_units.keys())\n    extra_med_categories = requested_med_categories - set(med_df_base['med_category'])\n    if extra_med_categories:\n        error_msg = f\"The following med_categories are given a preferred unit but not found in the input med_df: {extra_med_categories}\"\n        if override:\n            print(error_msg)\n        else:\n            raise ValueError(error_msg)\n\n    try:\n        # join the preferred units to the df\n        preferred_units_df = pd.DataFrame(preferred_units.items(), columns=['med_category', '_preferred_unit'])\n        q = \"\"\"\n        SELECT l.*\n            -- for unspecified preferred units, use the base units by default\n            , _preferred_unit: COALESCE(r._preferred_unit, l._base_unit)\n        FROM med_df_base l\n        LEFT JOIN preferred_units_df r USING (med_category)\n        \"\"\"\n        med_df_preferred = duckdb.sql(q).to_df()\n\n        med_df_converted = _convert_base_units_to_preferred_units(med_df_preferred, override=override)\n    except ValueError as e:\n        raise ValueError(f\"Error converting dose units to preferred units: {e}\")\n\n    try:\n        convert_counts_df = _create_unit_conversion_counts_table(\n            med_df_converted, \n            group_by=[\n                'med_category',\n                'med_dose_unit', '_clean_unit', '_base_unit', '_unit_class',\n                '_preferred_unit', 'med_dose_unit_converted', '_convert_status'\n                ]\n            )\n    except ValueError as e:\n        raise ValueError(f\"Error creating unit conversion counts table: {e}\")\n\n    if show_intermediate:\n        return med_df_converted, convert_counts_df\n    else:\n        # the default (detailed_output=False) is to drop multiplier columns which likely are not useful for the user\n        multiplier_cols = [col for col in med_df_converted.columns if 'multiplier' in col]\n        qa_cols = [\n            '_rn',\n            '_weight_recorded_dttm',\n            '_weighted', '_weighted_preferred',\n            '_base_dose', '_base_unit',\n            '_preferred_unit',\n            '_unit_class_preferred',\n            '_unit_subclass', '_unit_subclass_preferred'\n            ]\n\n        cols_to_drop = [c for c in multiplier_cols + qa_cols if c in med_df_converted.columns]\n\n        return med_df_converted.drop(columns=cols_to_drop), convert_counts_df\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units","title":"clifpy.utils.unit_converter.standardize_dose_to_base_units","text":"<pre><code>standardize_dose_to_base_units(med_df, vitals_df=None)\n</code></pre> <p>Standardize medication dose units to a base set of standard units.</p> <p>Main public API function that performs complete dose unit standardization pipeline: format cleaning, name cleaning, and unit conversion. Returns both base data and a summary table of conversions.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units--parameters","title":"Parameters","text":"<p>med_df : pd.DataFrame     Medication DataFrame with required columns:</p> <pre><code>- med_dose_unit: Original dose unit strings\n- med_dose: Dose values\n- weight_kg: Patient weights (optional, can be added from vitals_df)\n\nAdditional columns are preserved in output.\n</code></pre> <p>vitals_df : pd.DataFrame, optional     Vitals DataFrame for extracting patient weights if not in med_df.     Required columns if weight_kg missing from med_df:</p> <pre><code>- hospitalization_id: Patient identifier\n- recorded_dttm: Timestamp of vital recording\n- vital_category: Must include 'weight_kg' values\n- vital_value: Weight values\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units--returns","title":"Returns","text":"<p>Tuple[pd.DataFrame, pd.DataFrame]     A tuple containing:</p> <pre><code>- [0] base medication DataFrame with additional columns:\n\n    * _clean_unit: Cleaned unit string\n    * _unit_class: 'rate', 'amount', or 'unrecognized'\n    * _base_dose: base dose value\n    * _base_unit: base unit\n    * amount_multiplier, time_multiplier, weight_multiplier: Conversion factors\n\n- [1] Summary counts DataFrame showing conversion patterns and frequencies\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units--raises","title":"Raises","text":"<p>ValueError     If required columns are missing from med_df.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units--examples","title":"Examples","text":"<p>import pandas as pd med_df = pd.DataFrame({ ...     'med_dose': [6, 100, 500], ...     'med_dose_unit': ['MCG/KG/HR', 'mL / hr', 'mg'], ...     'weight_kg': [70, 80, 75] ... }) base_df, counts_df = standardize_dose_to_base_units(med_df) '_base_unit' in base_df.columns True 'count' in counts_df.columns True</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units--notes","title":"Notes","text":"<p>Standard units for conversion:</p> <ul> <li>Rate units: mcg/min, ml/min, u/min (all per minute)</li> <li>Amount units: mcg, ml, u (base units)</li> </ul> <p>The function automatically handles:</p> <ul> <li>Weight-based dosing (/kg, /lb) using patient weights</li> <li>Time conversions (per hour to per minute)</li> <li>Volume conversions (L to mL)</li> <li>Mass conversions (mg, ng, g to mcg)</li> <li>Unit conversions (milli-units to units)</li> </ul> <p>Unrecognized units are flagged but preserved in the output.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def standardize_dose_to_base_units(\n    med_df: pd.DataFrame,\n    vitals_df: pd.DataFrame = None\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Standardize medication dose units to a base set of standard units.\n\n    Main public API function that performs complete dose unit standardization\n    pipeline: format cleaning, name cleaning, and unit conversion.\n    Returns both base data and a summary table of conversions.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        Medication DataFrame with required columns:\n\n        - med_dose_unit: Original dose unit strings\n        - med_dose: Dose values\n        - weight_kg: Patient weights (optional, can be added from vitals_df)\n\n        Additional columns are preserved in output.\n    vitals_df : pd.DataFrame, optional\n        Vitals DataFrame for extracting patient weights if not in med_df.\n        Required columns if weight_kg missing from med_df:\n\n        - hospitalization_id: Patient identifier\n        - recorded_dttm: Timestamp of vital recording\n        - vital_category: Must include 'weight_kg' values\n        - vital_value: Weight values\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        A tuple containing:\n\n        - [0] base medication DataFrame with additional columns:\n\n            * _clean_unit: Cleaned unit string\n            * _unit_class: 'rate', 'amount', or 'unrecognized'\n            * _base_dose: base dose value\n            * _base_unit: base unit\n            * amount_multiplier, time_multiplier, weight_multiplier: Conversion factors\n\n        - [1] Summary counts DataFrame showing conversion patterns and frequencies\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from med_df.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; med_df = pd.DataFrame({\n    ...     'med_dose': [6, 100, 500],\n    ...     'med_dose_unit': ['MCG/KG/HR', 'mL / hr', 'mg'],\n    ...     'weight_kg': [70, 80, 75]\n    ... })\n    &gt;&gt;&gt; base_df, counts_df = standardize_dose_to_base_units(med_df)\n    &gt;&gt;&gt; '_base_unit' in base_df.columns\n    True\n    &gt;&gt;&gt; 'count' in counts_df.columns\n    True\n\n    Notes\n    -----\n    Standard units for conversion:\n\n    - Rate units: mcg/min, ml/min, u/min (all per minute)\n    - Amount units: mcg, ml, u (base units)\n\n    The function automatically handles:\n\n    - Weight-based dosing (/kg, /lb) using patient weights\n    - Time conversions (per hour to per minute)\n    - Volume conversions (L to mL)\n    - Mass conversions (mg, ng, g to mcg)\n    - Unit conversions (milli-units to units)\n\n    Unrecognized units are flagged but preserved in the output.\n    \"\"\"\n    if 'weight_kg' not in med_df.columns:\n        print(\"No weight_kg column found, adding the most recent from vitals\")\n        query = \"\"\"\n        SELECT m.*\n            , v.vital_value as weight_kg\n            , v.recorded_dttm as _weight_recorded_dttm\n            , ROW_NUMBER() OVER (\n                PARTITION BY m.hospitalization_id, m.admin_dttm, m.med_category\n                ORDER BY v.recorded_dttm DESC\n                ) as _rn\n        FROM med_df m\n        LEFT JOIN vitals_df v \n            ON m.hospitalization_id = v.hospitalization_id \n            AND v.vital_category = 'weight_kg' AND v.vital_value IS NOT NULL\n            AND v.recorded_dttm &lt;= m.admin_dttm  -- only past weights\n        -- rn = 1 for the weight w/ the latest recorded_dttm (and thus most recent)\n        QUALIFY (_rn = 1 OR _rn IS NULL) \n        ORDER BY m.hospitalization_id, m.admin_dttm, m.med_category, _rn\n        \"\"\"\n        med_df = duckdb.sql(query).to_df()\n\n    # check if the required columns are present\n    required_columns = {'med_dose_unit', 'med_dose', 'weight_kg'}\n    missing_columns = required_columns - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    med_df['_clean_unit'] = (\n        med_df['med_dose_unit'].pipe(_clean_dose_unit_formats)\n        .pipe(_clean_dose_unit_names)\n    )\n\n    med_df_base = _convert_clean_units_to_base_units(med_df)\n    convert_counts_df = _create_unit_conversion_counts_table(\n        med_df_base, \n        group_by=['med_dose_unit', '_clean_unit', '_base_unit', '_unit_class']\n        )\n\n    return med_df_base, convert_counts_df\n</code></pre>"},{"location":"api/utilities/#constants-and-data-structures","title":"Constants and Data Structures","text":""},{"location":"api/utilities/#acceptable-units","title":"Acceptable Units","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter.ACCEPTABLE_AMOUNT_UNITS","title":"clifpy.utils.unit_converter.ACCEPTABLE_AMOUNT_UNITS  <code>module-attribute</code>","text":"<pre><code>ACCEPTABLE_AMOUNT_UNITS = {\n    \"ml\",\n    \"l\",\n    \"mu\",\n    \"u\",\n    \"mcg\",\n    \"mg\",\n    \"ng\",\n    \"g\",\n}\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.ACCEPTABLE_RATE_UNITS","title":"clifpy.utils.unit_converter.ACCEPTABLE_RATE_UNITS  <code>module-attribute</code>","text":"<pre><code>ACCEPTABLE_RATE_UNITS = _acceptable_rate_units()\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.ALL_ACCEPTABLE_UNITS","title":"clifpy.utils.unit_converter.ALL_ACCEPTABLE_UNITS  <code>module-attribute</code>","text":"<pre><code>ALL_ACCEPTABLE_UNITS = (\n    ACCEPTABLE_RATE_UNITS | ACCEPTABLE_AMOUNT_UNITS\n)\n</code></pre>"},{"location":"api/utilities/#unit-patterns","title":"Unit Patterns","text":"<p>The following constants define regex patterns for unit classification:</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.MASS_REGEX","title":"clifpy.utils.unit_converter.MASS_REGEX  <code>module-attribute</code>","text":"<pre><code>MASS_REGEX = f'^(mcg|mg|ng|g){AMOUNT_ENDER}'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.VOLUME_REGEX","title":"clifpy.utils.unit_converter.VOLUME_REGEX  <code>module-attribute</code>","text":"<pre><code>VOLUME_REGEX = f'^(l|ml){AMOUNT_ENDER}'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.UNIT_REGEX","title":"clifpy.utils.unit_converter.UNIT_REGEX  <code>module-attribute</code>","text":"<pre><code>UNIT_REGEX = f'^(u|mu){AMOUNT_ENDER}'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.HR_REGEX","title":"clifpy.utils.unit_converter.HR_REGEX  <code>module-attribute</code>","text":"<pre><code>HR_REGEX = f'/hr$'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.WEIGHT_REGEX","title":"clifpy.utils.unit_converter.WEIGHT_REGEX  <code>module-attribute</code>","text":"<pre><code>WEIGHT_REGEX = f'/(lb|kg)/'\n</code></pre>"},{"location":"api/utilities/#conversion-mappings","title":"Conversion Mappings","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter.UNIT_NAMING_VARIANTS","title":"clifpy.utils.unit_converter.UNIT_NAMING_VARIANTS  <code>module-attribute</code>","text":"<pre><code>UNIT_NAMING_VARIANTS = {\n    \"/hr\": \"/h(r|our)?$\",\n    \"/min\": \"/m(in|inute)?$\",\n    \"u\": \"u(nits|nit)?\",\n    \"m\": \"milli-?\",\n    \"l\": \"l(iters|itres|itre|iter)?\",\n    \"mcg\": \"^(u|\u00b5|\u03bc)g\",\n    \"g\": \"^g(rams|ram)?\",\n}\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.REGEX_TO_FACTOR_MAPPER","title":"clifpy.utils.unit_converter.REGEX_TO_FACTOR_MAPPER  <code>module-attribute</code>","text":"<pre><code>REGEX_TO_FACTOR_MAPPER = {\n    HR_REGEX: \"1/60\",\n    L_REGEX: \"1000\",\n    MU_REGEX: \"1/1000\",\n    MG_REGEX: \"1000\",\n    NG_REGEX: \"1/1000\",\n    G_REGEX: \"1000000\",\n    KG_REGEX: \"weight_kg\",\n    LB_REGEX: \"weight_kg * 2.20462\",\n}\n</code></pre>"},{"location":"api/utilities/#internal-functions","title":"Internal Functions","text":"<p>The following functions are used internally by the main conversion functions. They are documented here for completeness and advanced usage.</p> <p>This section documents the utility functions available in CLIFpy for data processing, validation, and specialized operations.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_formats","title":"clifpy.utils.unit_converter._clean_dose_unit_formats","text":"<pre><code>_clean_dose_unit_formats(s)\n</code></pre> <p>Clean dose unit formatting by removing spaces and converting to lowercase.</p> <p>This is the first step in the cleaning pipeline. It standardizes the basic formatting of dose units before applying name cleaning.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_formats--parameters","title":"Parameters","text":"<p>s : pd.Series     Series containing dose unit strings to clean.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_formats--returns","title":"Returns","text":"<p>pd.Series     Series with cleaned formatting (no spaces, lowercase).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_formats--examples","title":"Examples","text":"<p>import pandas as pd s = pd.Series(['mL / hr', 'MCG/KG/MIN', ' Mg/Hr ']) result = _clean_dose_unit_formats(s) list(result) ['ml/hr', 'mcg/kg/min', 'mg/hr']</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_formats--notes","title":"Notes","text":"<p>This function is typically used as the first step in the cleaning pipeline, followed by _clean_dose_unit_names().</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _clean_dose_unit_formats(s: pd.Series) -&gt; pd.Series:\n    \"\"\"Clean dose unit formatting by removing spaces and converting to lowercase.\n\n    This is the first step in the cleaning pipeline. It standardizes\n    the basic formatting of dose units before applying name cleaning.\n\n    Parameters\n    ----------\n    s : pd.Series\n        Series containing dose unit strings to clean.\n\n    Returns\n    -------\n    pd.Series\n        Series with cleaned formatting (no spaces, lowercase).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; s = pd.Series(['mL / hr', 'MCG/KG/MIN', ' Mg/Hr '])\n    &gt;&gt;&gt; result = _clean_dose_unit_formats(s)\n    &gt;&gt;&gt; list(result)\n    ['ml/hr', 'mcg/kg/min', 'mg/hr']\n\n    Notes\n    -----\n    This function is typically used as the first step in the cleaning\n    pipeline, followed by _clean_dose_unit_names().\n    \"\"\"\n    return s.str.replace(r'\\s+', '', regex=True).str.lower().replace('', None, regex=False)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_names","title":"clifpy.utils.unit_converter._clean_dose_unit_names","text":"<pre><code>_clean_dose_unit_names(s)\n</code></pre> <p>Clean dose unit name variants to standard abbreviations.</p> <p>Applies regex patterns to convert various unit name variants to their standard abbreviated forms (e.g., 'milliliter' -&gt; 'ml', 'hour' -&gt; 'hr').</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_names--parameters","title":"Parameters","text":"<p>s : pd.Series     Series containing dose unit strings with name variants.     Should already be format-cleaned (lowercase, no spaces).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_names--returns","title":"Returns","text":"<p>pd.Series     Series with clean unit names.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_names--examples","title":"Examples","text":"<p>import pandas as pd s = pd.Series(['milliliter/hour', 'units/minute', '\u00b5g/kg/h']) result = _clean_dose_unit_names(s) list(result) ['ml/hr', 'u/min', 'mcg/kg/hr']</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_names--notes","title":"Notes","text":"<p>Handles conversions including:</p> <ul> <li>Time: hour/h -&gt; hr, minute/m -&gt; min</li> <li>Volume: liter/liters/litre/litres -&gt; l</li> <li>Units: units/unit -&gt; u, milli-units -&gt; mu</li> <li>Mass: \u00b5g/ug -&gt; mcg, gram -&gt; g</li> </ul> <p>This function should be applied after _clean_dose_unit_formats().</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _clean_dose_unit_names(s: pd.Series) -&gt; pd.Series:\n    \"\"\"Clean dose unit name variants to standard abbreviations.\n\n    Applies regex patterns to convert various unit name variants to their\n    standard abbreviated forms (e.g., 'milliliter' -&gt; 'ml', 'hour' -&gt; 'hr').\n\n    Parameters\n    ----------\n    s : pd.Series\n        Series containing dose unit strings with name variants.\n        Should already be format-cleaned (lowercase, no spaces).\n\n    Returns\n    -------\n    pd.Series\n        Series with clean unit names.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; s = pd.Series(['milliliter/hour', 'units/minute', '\u00b5g/kg/h'])\n    &gt;&gt;&gt; result = _clean_dose_unit_names(s)\n    &gt;&gt;&gt; list(result)\n    ['ml/hr', 'u/min', 'mcg/kg/hr']\n\n    Notes\n    -----\n    Handles conversions including:\n\n    - Time: hour/h -&gt; hr, minute/m -&gt; min\n    - Volume: liter/liters/litre/litres -&gt; l\n    - Units: units/unit -&gt; u, milli-units -&gt; mu\n    - Mass: \u00b5g/ug -&gt; mcg, gram -&gt; g\n\n    This function should be applied after _clean_dose_unit_formats().\n    \"\"\"\n    for repl, pattern in UNIT_NAMING_VARIANTS.items():\n        s = s.str.replace(pattern, repl, regex=True)\n    return s\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_clean_units_to_base_units","title":"clifpy.utils.unit_converter._convert_clean_units_to_base_units","text":"<pre><code>_convert_clean_units_to_base_units(med_df)\n</code></pre> <p>Convert clean dose units to base units.</p> <p>Core conversion function that transforms various dose units into a base set of standard units (mcg/min, ml/min, u/min for rates; mcg, ml, u for amounts). Uses DuckDB for efficient SQL-based transformations.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_clean_units_to_base_units--parameters","title":"Parameters","text":"<p>med_df : pd.DataFrame     DataFrame containing medication data with required columns:</p> <pre><code>- _clean_unit: Cleaned unit strings\n- med_dose: Original dose values\n- weight_kg: Patient weight (used for /kg and /lb conversions)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_clean_units_to_base_units--returns","title":"Returns","text":"<p>pd.DataFrame     Original DataFrame with additional columns:</p> <pre><code>- _unit_class: 'rate', 'amount', or 'unrecognized'\n- _amount_multiplier: Factor for amount conversion\n- _time_multiplier: Factor for time conversion (hr to min)\n- _weight_multiplier: Factor for weight-based conversion\n- _base_dose: base dose value\n- _base_unit: base unit string\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_clean_units_to_base_units--examples","title":"Examples","text":"<p>import pandas as pd df = pd.DataFrame({ ...     'med_dose': [6, 100], ...     '_clean_unit': ['mcg/kg/hr', 'ml/hr'], ...     'weight_kg': [70, 80] ... }) result = _convert_clean_dose_units_to_base_units(df) 'mcg/min' in result['_base_unit'].values True 'ml/min' in result['_base_unit'].values True</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_clean_units_to_base_units--notes","title":"Notes","text":"<p>Conversion targets:</p> <ul> <li>Rate units: mcg/min, ml/min, u/min</li> <li>Amount units: mcg, ml, u</li> <li>Unrecognized units: original dose and (cleaned) unit will be preserved</li> </ul> <p>Weight-based conversions use patient weight from weight_kg column. Time conversions: /hr -&gt; /min (divide by 60).</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _convert_clean_units_to_base_units(med_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert clean dose units to base units.\n\n    Core conversion function that transforms various dose units into a base\n    set of standard units (mcg/min, ml/min, u/min for rates; mcg, ml, u for amounts).\n    Uses DuckDB for efficient SQL-based transformations.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        DataFrame containing medication data with required columns:\n\n        - _clean_unit: Cleaned unit strings\n        - med_dose: Original dose values\n        - weight_kg: Patient weight (used for /kg and /lb conversions)\n\n    Returns\n    -------\n    pd.DataFrame\n        Original DataFrame with additional columns:\n\n        - _unit_class: 'rate', 'amount', or 'unrecognized'\n        - _amount_multiplier: Factor for amount conversion\n        - _time_multiplier: Factor for time conversion (hr to min)\n        - _weight_multiplier: Factor for weight-based conversion\n        - _base_dose: base dose value\n        - _base_unit: base unit string\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\n    ...     'med_dose': [6, 100],\n    ...     '_clean_unit': ['mcg/kg/hr', 'ml/hr'],\n    ...     'weight_kg': [70, 80]\n    ... })\n    &gt;&gt;&gt; result = _convert_clean_dose_units_to_base_units(df)\n    &gt;&gt;&gt; 'mcg/min' in result['_base_unit'].values\n    True\n    &gt;&gt;&gt; 'ml/min' in result['_base_unit'].values\n    True\n\n    Notes\n    -----\n    Conversion targets:\n\n    - Rate units: mcg/min, ml/min, u/min\n    - Amount units: mcg, ml, u\n    - Unrecognized units: original dose and (cleaned) unit will be preserved\n\n    Weight-based conversions use patient weight from weight_kg column.\n    Time conversions: /hr -&gt; /min (divide by 60).\n    \"\"\"\n\n    amount_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_base,\n        patterns=[L_REGEX, MU_REGEX, MG_REGEX, NG_REGEX, G_REGEX],\n        else_case='1'\n        )\n\n    time_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_base,\n        patterns=[HR_REGEX],\n        else_case='1'\n        )\n\n    weight_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_base,\n        patterns=[KG_REGEX, LB_REGEX],\n        else_case='1'\n        )\n\n    q = f\"\"\"\n    SELECT *\n        -- classify and check acceptability first\n        , _unit_class: CASE\n            WHEN _clean_unit IN ('{RATE_UNITS_STR}') THEN 'rate' \n            WHEN _clean_unit IN ('{AMOUNT_UNITS_STR}') THEN 'amount'\n            ELSE 'unrecognized' END\n        -- mark if the input unit is adjusted by weight (e.g. 'mcg/kg/hr')\n        , _weighted: CASE\n            WHEN regexp_matches(_clean_unit, '{WEIGHT_REGEX}') THEN 1 ELSE 0 END\n        -- parse and generate multipliers\n        , _amount_multiplier: CASE\n            WHEN _unit_class = 'unrecognized' THEN 1 ELSE ({amount_clause}) END \n        , _time_multiplier: CASE\n            WHEN _unit_class = 'unrecognized' THEN 1 ELSE ({time_clause}) END \n        , _weight_multiplier: CASE\n            WHEN _unit_class = 'unrecognized' THEN 1 ELSE ({weight_clause}) END\n        -- calculate the base dose\n        , _base_dose: CASE\n            -- when the input unit is weighted but weight_kg is missing, keep the original dose\n            WHEN _weighted = 1 AND weight_kg IS NULL THEN med_dose\n            ELSE med_dose * _amount_multiplier * _time_multiplier * _weight_multiplier \n            END\n        -- id the base unit\n        , _base_unit: CASE \n            -- when the input unit is weighted but weight_kg is missing, keep the original dose\n            WHEN _weighted = 1 AND weight_kg IS NULL THEN _clean_unit\n            WHEN _unit_class = 'unrecognized' THEN _clean_unit\n            WHEN _unit_class = 'rate' AND regexp_matches(_clean_unit, '{MASS_REGEX}') THEN 'mcg/min'\n            WHEN _unit_class = 'rate' AND regexp_matches(_clean_unit, '{VOLUME_REGEX}') THEN 'ml/min'\n            WHEN _unit_class = 'rate' AND regexp_matches(_clean_unit, '{UNIT_REGEX}') THEN 'u/min'\n            WHEN _unit_class = 'amount' AND regexp_matches(_clean_unit, '{MASS_REGEX}') THEN 'mcg'\n            WHEN _unit_class = 'amount' AND regexp_matches(_clean_unit, '{VOLUME_REGEX}') THEN 'ml'\n            WHEN _unit_class = 'amount' AND regexp_matches(_clean_unit, '{UNIT_REGEX}') THEN 'u'\n            END\n    FROM med_df \n    \"\"\"\n    return duckdb.sql(q).to_df()\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units","title":"clifpy.utils.unit_converter._convert_base_units_to_preferred_units","text":"<pre><code>_convert_base_units_to_preferred_units(\n    med_df, override=False\n)\n</code></pre> <p>Convert base standardized units to user-preferred units.</p> <p>Performs the second stage of unit conversion, transforming from standardized base units (mcg/min, ml/min, u/min) to medication-specific preferred units while maintaining unit class consistency.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units--parameters","title":"Parameters","text":"<p>med_df : pd.DataFrame     DataFrame with required columns from first-stage conversion:</p> <pre><code>- _base_dose: Dose values in standardized units\n- _base_unit: Standardized unit strings (may be NULL)\n- _preferred_unit: Target unit strings for each medication\n- weight_kg: Patient weights (optional, used for weight-based conversions)\n</code></pre> <p>override : bool, default False     If True, prints warnings but continues when encountering:</p> <pre><code>- Unacceptable preferred units not in ALL_ACCEPTABLE_UNITS\n- Cross-class conversions (e.g., rate to amount)\n- Cross-subclass conversions (e.g., mass to volume)\n\nIf False, raises ValueError for these conditions.\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units--returns","title":"Returns","text":"<p>pd.DataFrame     Original DataFrame with additional columns:</p> <pre><code>- _unit_class: Classification of base unit ('rate', 'amount', 'unrecognized')\n- _unit_subclass: Subclassification ('mass', 'volume', 'unit', 'unrecognized')\n- _unit_class_preferred: Classification of preferred unit\n- _unit_subclass_preferred: Subclassification of preferred unit\n- _convert_status: Success or failure reason message\n- _amount_multiplier_preferred: Conversion factor for amount units\n- _time_multiplier_preferred: Conversion factor for time units\n- _weight_multiplier_preferred: Conversion factor for weight-based units\n- med_dose_converted: Final converted dose value\n- med_dose_unit_converted: Final unit string after conversion\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units--raises","title":"Raises","text":"<p>ValueError     If required columns are missing from med_df or if preferred units are not     in ALL_ACCEPTABLE_UNITS (when override=False).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units--notes","title":"Notes","text":"<p>Conversion rules enforced:</p> <ul> <li>Conversions only allowed within same unit class (rate\u2192rate, amount\u2192amount)</li> <li>Cannot convert between incompatible subclasses (e.g., mass\u2192volume)</li> <li>When conversion fails, falls back to base units and dose values</li> <li>Missing units (NULL) are handled with 'original unit is missing' status</li> </ul> <p>The function uses DuckDB SQL for efficient processing and applies regex pattern matching to classify units and calculate conversion factors.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units--see-also","title":"See Also","text":"<p>_convert_clean_dose_units_to_base_units : First-stage conversion convert_dose_units_by_med_category : Public API for complete conversion pipeline</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _convert_base_units_to_preferred_units(\n    med_df: pd.DataFrame,\n    override: bool = False\n    ) -&gt; pd.DataFrame:\n    \"\"\"Convert base standardized units to user-preferred units.\n\n    Performs the second stage of unit conversion, transforming from standardized\n    base units (mcg/min, ml/min, u/min) to medication-specific preferred units\n    while maintaining unit class consistency.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        DataFrame with required columns from first-stage conversion:\n\n        - _base_dose: Dose values in standardized units\n        - _base_unit: Standardized unit strings (may be NULL)\n        - _preferred_unit: Target unit strings for each medication\n        - weight_kg: Patient weights (optional, used for weight-based conversions)\n    override : bool, default False\n        If True, prints warnings but continues when encountering:\n\n        - Unacceptable preferred units not in ALL_ACCEPTABLE_UNITS\n        - Cross-class conversions (e.g., rate to amount)\n        - Cross-subclass conversions (e.g., mass to volume)\n\n        If False, raises ValueError for these conditions.\n\n    Returns\n    -------\n    pd.DataFrame\n        Original DataFrame with additional columns:\n\n        - _unit_class: Classification of base unit ('rate', 'amount', 'unrecognized')\n        - _unit_subclass: Subclassification ('mass', 'volume', 'unit', 'unrecognized')\n        - _unit_class_preferred: Classification of preferred unit\n        - _unit_subclass_preferred: Subclassification of preferred unit\n        - _convert_status: Success or failure reason message\n        - _amount_multiplier_preferred: Conversion factor for amount units\n        - _time_multiplier_preferred: Conversion factor for time units\n        - _weight_multiplier_preferred: Conversion factor for weight-based units\n        - med_dose_converted: Final converted dose value\n        - med_dose_unit_converted: Final unit string after conversion\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from med_df or if preferred units are not\n        in ALL_ACCEPTABLE_UNITS (when override=False).\n\n    Notes\n    -----\n    Conversion rules enforced:\n\n    - Conversions only allowed within same unit class (rate\u2192rate, amount\u2192amount)\n    - Cannot convert between incompatible subclasses (e.g., mass\u2192volume)\n    - When conversion fails, falls back to base units and dose values\n    - Missing units (NULL) are handled with 'original unit is missing' status\n\n    The function uses DuckDB SQL for efficient processing and applies regex\n    pattern matching to classify units and calculate conversion factors.\n\n    See Also\n    --------\n    _convert_clean_dose_units_to_base_units : First-stage conversion\n    convert_dose_units_by_med_category : Public API for complete conversion pipeline\n    \"\"\"\n    # check presense of all required columns\n    required_columns = {'_base_dose', '_preferred_unit'}\n    missing_columns = required_columns - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    # check user-defined _preferred_unit are in the set of acceptable units\n    unacceptable_preferred_units = set(med_df['_preferred_unit']) - ALL_ACCEPTABLE_UNITS - {None}\n    if unacceptable_preferred_units:\n        error_msg = f\"Cannot accommodate the conversion to the following preferred units: {unacceptable_preferred_units}. Consult the function documentation for a list of acceptable units.\"\n        if override:\n            print(error_msg)\n        else:\n            raise ValueError(error_msg)\n\n    amount_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_preferred,\n        patterns=[L_REGEX, MU_REGEX, MG_REGEX, NG_REGEX, G_REGEX],\n        else_case='1'\n        )\n\n    time_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_preferred,\n        patterns=[HR_REGEX],\n        else_case='1'\n        )\n\n    weight_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_preferred,\n        patterns=[KG_REGEX, LB_REGEX],\n        else_case='1'\n        )\n\n    unit_class_clause = f\"\"\"\n    , _unit_class: CASE\n        WHEN _base_unit IN ('{RATE_UNITS_STR}') THEN 'rate' \n        WHEN _base_unit IN ('{AMOUNT_UNITS_STR}') THEN 'amount'\n        ELSE 'unrecognized' END\n    \"\"\" if '_unit_class' not in med_df.columns else ''\n\n    weighted_clause = f\"\"\"\n    , _weighted: CASE\n        WHEN regexp_matches(_clean_unit, '{WEIGHT_REGEX}') THEN 1 ELSE 0 END\n    \"\"\" if '_weighted' not in med_df.columns else ''\n\n    dose_converted_name = \"med_dose\" if \"med_dose\" in med_df.columns else \"_base_dose\"\n    unit_converted_name = \"_clean_unit\" if \"_clean_unit\" in med_df.columns else \"_base_unit\"\n\n    q = f\"\"\"\n    SELECT l.*\n        {unit_class_clause}\n        , _unit_subclass: CASE \n            WHEN regexp_matches(_base_unit, '{MASS_REGEX}') THEN 'mass'\n            WHEN regexp_matches(_base_unit, '{VOLUME_REGEX}') THEN 'volume'\n            WHEN regexp_matches(_base_unit, '{UNIT_REGEX}') THEN 'unit'\n            ELSE 'unrecognized' END\n        , _unit_class_preferred: CASE \n            WHEN _preferred_unit IN ('{RATE_UNITS_STR}') THEN 'rate' \n            WHEN _preferred_unit IN ('{AMOUNT_UNITS_STR}') THEN 'amount'\n            ELSE 'unrecognized' END\n        , _unit_subclass_preferred: CASE \n            WHEN regexp_matches(_preferred_unit, '{MASS_REGEX}') THEN 'mass'\n            WHEN regexp_matches(_preferred_unit, '{VOLUME_REGEX}') THEN 'volume'\n            WHEN regexp_matches(_preferred_unit, '{UNIT_REGEX}') THEN 'unit'\n            ELSE 'unrecognized' END\n        , _weighted_preferred: CASE\n            WHEN regexp_matches(_preferred_unit, '{WEIGHT_REGEX}') THEN 1 ELSE 0 END\n        , _convert_status: CASE \n            WHEN _weighted_preferred = 1 AND weight_kg IS NULL \n                THEN 'cannot convert to a weighted unit if weight_kg is missing'\n            WHEN _base_unit IS NULL THEN 'original unit is missing'\n            WHEN _unit_class == 'unrecognized' OR _unit_subclass == 'unrecognized'\n                THEN 'original unit ' || _base_unit || ' is not recognized'\n            WHEN _unit_class_preferred == 'unrecognized' OR _unit_subclass_preferred == 'unrecognized'\n                THEN 'user-preferred unit ' || _preferred_unit || ' is not recognized'\n            WHEN _unit_class != _unit_class_preferred \n                THEN 'cannot convert ' || _unit_class || ' to ' || _unit_class_preferred\n            WHEN _unit_subclass != _unit_subclass_preferred\n                THEN 'cannot convert ' || _unit_subclass || ' to ' || _unit_subclass_preferred\n            WHEN _unit_class == _unit_class_preferred AND _unit_subclass == _unit_subclass_preferred\n                -- AND _unit_class != 'unrecognized' AND _unit_subclass != 'unrecognized'\n                THEN 'success'\n            ELSE 'other error - please report'\n            END\n        , _amount_multiplier_preferred: {amount_clause}\n        , _time_multiplier_preferred: {time_clause}\n        , _weight_multiplier_preferred: {weight_clause}\n        -- fall back to the base units and dose (i.e. the input) if conversion cannot be accommondated\n        , med_dose_converted: CASE\n            WHEN _convert_status == 'success' THEN _base_dose * _amount_multiplier_preferred * _time_multiplier_preferred * _weight_multiplier_preferred\n            ELSE {dose_converted_name}\n            END\n        , med_dose_unit_converted: CASE\n            WHEN _convert_status == 'success' THEN _preferred_unit\n            ELSE {unit_converted_name}\n            END\n    FROM med_df l\n    \"\"\"\n    return duckdb.sql(q).to_df()\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table","title":"clifpy.utils.unit_converter._create_unit_conversion_counts_table","text":"<pre><code>_create_unit_conversion_counts_table(med_df, group_by)\n</code></pre> <p>Create summary table of unit conversion counts.</p> <p>Generates a grouped summary showing the frequency of each unit conversion pattern, useful for data quality assessment and identifying common or problematic unit patterns.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--parameters","title":"Parameters","text":"<p>med_df : pd.DataFrame     DataFrame with required columns from conversion process:</p> <pre><code>- med_dose_unit: Original unit string\n- _clean_unit: Cleaned unit string\n- _base_unit: base standard unit\n- _unit_class: Classification (rate/amount/unrecognized)\n</code></pre> <p>group_by : List[str]     List of columns to group by.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--returns","title":"Returns","text":"<p>pd.DataFrame     Summary DataFrame with columns:</p> <pre><code>- med_dose_unit: Original unit\n- _clean_unit: After cleaning\n- _base_unit: After conversion\n- _unit_class: Classification\n- count: Number of occurrences\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--raises","title":"Raises","text":"<p>ValueError     If required columns are missing from input DataFrame.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--examples","title":"Examples","text":"<p>import pandas as pd</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--df_base-standardize_dose_to_base_unitsmed_df0","title":"df_base = standardize_dose_to_base_units(med_df)[0]","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--counts-_create_unit_conversion_counts_tabledf_base-med_dose_unit","title":"counts = _create_unit_conversion_counts_table(df_base, ['med_dose_unit'])","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--count-in-countscolumns","title":"'count' in counts.columns","text":"<p>True</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table--notes","title":"Notes","text":"<p>This table is particularly useful for:</p> <ul> <li>Identifying unrecognized units that need handling</li> <li>Understanding the distribution of unit types in your data</li> <li>Quality control and validation of conversions</li> </ul> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _create_unit_conversion_counts_table(\n    med_df: pd.DataFrame,\n    group_by: List[str]\n    ) -&gt; pd.DataFrame:\n    \"\"\"Create summary table of unit conversion counts.\n\n    Generates a grouped summary showing the frequency of each unit conversion\n    pattern, useful for data quality assessment and identifying common or\n    problematic unit patterns.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        DataFrame with required columns from conversion process:\n\n        - med_dose_unit: Original unit string\n        - _clean_unit: Cleaned unit string\n        - _base_unit: base standard unit\n        - _unit_class: Classification (rate/amount/unrecognized)\n    group_by : List[str]\n        List of columns to group by.\n\n    Returns\n    -------\n    pd.DataFrame\n        Summary DataFrame with columns:\n\n        - med_dose_unit: Original unit\n        - _clean_unit: After cleaning\n        - _base_unit: After conversion\n        - _unit_class: Classification\n        - count: Number of occurrences\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from input DataFrame.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; # df_base = standardize_dose_to_base_units(med_df)[0]\n    &gt;&gt;&gt; # counts = _create_unit_conversion_counts_table(df_base, ['med_dose_unit'])\n    &gt;&gt;&gt; # 'count' in counts.columns\n    True\n\n    Notes\n    -----\n    This table is particularly useful for:\n\n    - Identifying unrecognized units that need handling\n    - Understanding the distribution of unit types in your data\n    - Quality control and validation of conversions\n    \"\"\"\n    # check presense of all the group by columns\n    # required_columns = {'med_dose_unit', 'med_dose_unit_normalized', 'med_dose_unit_limited', 'unit_class'}\n    missing_columns = set(group_by) - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    # build the string that enumerates the group by columns \n    # e.g. 'med_dose_unit, med_dose_unit_normalized, unit_class'\n    cols_enum_str = f\"{', '.join(group_by)}\"\n    order_by_clause = f\"med_category, count DESC\" if 'med_category' in group_by else \"count DESC\"\n\n    q = f\"\"\"\n    SELECT {cols_enum_str}   \n        , COUNT(*) as count\n    FROM med_df\n    GROUP BY {cols_enum_str}\n    ORDER BY {order_by_clause}\n    \"\"\"\n    return duckdb.sql(q).to_df()\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql","title":"clifpy.utils.unit_converter._convert_set_to_str_for_sql","text":"<pre><code>_convert_set_to_str_for_sql(s)\n</code></pre> <p>Convert a set of strings to SQL IN clause format.</p> <p>Transforms a Python set into a comma-separated string suitable for use in SQL IN clauses within DuckDB queries.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql--parameters","title":"Parameters","text":"<p>s : Set[str]     Set of strings to be formatted for SQL.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql--returns","title":"Returns","text":"<p>str     Comma-separated string with items separated by \"','\".     Does not include outer quotes - those are added in SQL query.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql--examples","title":"Examples","text":"<p>units = {'ml/hr', 'mcg/min', 'u/hr'} _convert_set_to_str_for_sql(units) \"ml/hr','mcg/min','u/hr\"</p> <p>Usage in SQL queries:</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql--fwhere-unit-in-_convert_set_to_str_for_sqlunits","title":"f\"WHERE unit IN ('{_convert_set_to_str_for_sql(units)}')\"","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql--notes","title":"Notes","text":"<p>This is a helper function for building DuckDB SQL queries that need to check if values are in a set of acceptable units.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _convert_set_to_str_for_sql(s: Set[str]) -&gt; str:\n    \"\"\"Convert a set of strings to SQL IN clause format.\n\n    Transforms a Python set into a comma-separated string suitable for use\n    in SQL IN clauses within DuckDB queries.\n\n    Parameters\n    ----------\n    s : Set[str]\n        Set of strings to be formatted for SQL.\n\n    Returns\n    -------\n    str\n        Comma-separated string with items separated by \"','\".\n        Does not include outer quotes - those are added in SQL query.\n\n    Examples\n    --------\n    &gt;&gt;&gt; units = {'ml/hr', 'mcg/min', 'u/hr'}\n    &gt;&gt;&gt; _convert_set_to_str_for_sql(units)\n    \"ml/hr','mcg/min','u/hr\"\n\n    Usage in SQL queries:\n\n    &gt;&gt;&gt; # f\"WHERE unit IN ('{_convert_set_to_str_for_sql(units)}')\"\n\n    Notes\n    -----\n    This is a helper function for building DuckDB SQL queries that need to check\n    if values are in a set of acceptable units.\n    \"\"\"\n    return \"','\".join(s)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._concat_builders_by_patterns","title":"clifpy.utils.unit_converter._concat_builders_by_patterns","text":"<pre><code>_concat_builders_by_patterns(\n    builder, patterns, else_case=\"1\"\n)\n</code></pre> <p>Concatenate multiple SQL CASE WHEN statements from patterns.</p> <p>Helper function that combines multiple regex pattern builders into a single SQL CASE statement for DuckDB queries. Used internally to build conversion factor calculations for different unit components (amount, time, weight).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._concat_builders_by_patterns--parameters","title":"Parameters","text":"<p>builder : callable     Function that generates CASE WHEN clauses from regex patterns.     Should accept a pattern string and return a WHEN...THEN clause. patterns : list     List of regex patterns to process with the builder function. else_case : str, default '1'     Value to use in the ELSE clause when no patterns match.     Default is '1' (no conversion factor).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._concat_builders_by_patterns--returns","title":"Returns","text":"<p>str     Complete SQL CASE statement with all pattern conditions.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._concat_builders_by_patterns--examples","title":"Examples","text":"<p>patterns = ['/hr$', '/min$'] builder = lambda p: f\"WHEN regexp_matches(col, '{p}') THEN factor\" result = _concat_builders_by_patterns(builder, patterns) 'CASE WHEN' in result and 'ELSE 1 END' in result True</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._concat_builders_by_patterns--notes","title":"Notes","text":"<p>This function is used internally by conversion functions to build SQL queries that apply different conversion factors based on unit patterns.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _concat_builders_by_patterns(builder: callable, patterns: list, else_case: str = '1') -&gt; str:\n    \"\"\"Concatenate multiple SQL CASE WHEN statements from patterns.\n\n    Helper function that combines multiple regex pattern builders into a single\n    SQL CASE statement for DuckDB queries. Used internally to build conversion\n    factor calculations for different unit components (amount, time, weight).\n\n    Parameters\n    ----------\n    builder : callable\n        Function that generates CASE WHEN clauses from regex patterns.\n        Should accept a pattern string and return a WHEN...THEN clause.\n    patterns : list\n        List of regex patterns to process with the builder function.\n    else_case : str, default '1'\n        Value to use in the ELSE clause when no patterns match.\n        Default is '1' (no conversion factor).\n\n    Returns\n    -------\n    str\n        Complete SQL CASE statement with all pattern conditions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; patterns = ['/hr$', '/min$']\n    &gt;&gt;&gt; builder = lambda p: f\"WHEN regexp_matches(col, '{p}') THEN factor\"\n    &gt;&gt;&gt; result = _concat_builders_by_patterns(builder, patterns)\n    &gt;&gt;&gt; 'CASE WHEN' in result and 'ELSE 1 END' in result\n    True\n\n    Notes\n    -----\n    This function is used internally by conversion functions to build\n    SQL queries that apply different conversion factors based on unit patterns.\n    \"\"\"\n    return \"CASE \" + \" \".join([builder(pattern) for pattern in patterns]) + f\" ELSE {else_case} END\"\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base","title":"clifpy.utils.unit_converter._pattern_to_factor_builder_for_base","text":"<pre><code>_pattern_to_factor_builder_for_base(pattern)\n</code></pre> <p>Build SQL CASE WHEN statement for regex pattern matching.</p> <p>Helper function that generates SQL CASE WHEN clauses for DuckDB queries based on regex patterns and their corresponding conversion factors.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base--parameters","title":"Parameters","text":"<p>pattern : str     Regex pattern to match (must exist in REGEX_TO_FACTOR_MAPPER).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base--returns","title":"Returns","text":"<p>str     SQL CASE WHEN clause string.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base--raises","title":"Raises","text":"<p>ValueError     If the pattern is not found in REGEX_TO_FACTOR_MAPPER.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base--examples","title":"Examples","text":"<p>clause = _pattern_to_factor_builder_for_base(HR_REGEX) 'WHEN regexp_matches' in clause and 'THEN' in clause True</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base--notes","title":"Notes","text":"<p>This function is used internally by _convert_clean_dose_units_to_base_units to build the SQL query for unit conversion.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _pattern_to_factor_builder_for_base(pattern: str) -&gt; str:\n    \"\"\"Build SQL CASE WHEN statement for regex pattern matching.\n\n    Helper function that generates SQL CASE WHEN clauses for DuckDB queries\n    based on regex patterns and their corresponding conversion factors.\n\n    Parameters\n    ----------\n    pattern : str\n        Regex pattern to match (must exist in REGEX_TO_FACTOR_MAPPER).\n\n    Returns\n    -------\n    str\n        SQL CASE WHEN clause string.\n\n    Raises\n    ------\n    ValueError\n        If the pattern is not found in REGEX_TO_FACTOR_MAPPER.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clause = _pattern_to_factor_builder_for_base(HR_REGEX)\n    &gt;&gt;&gt; 'WHEN regexp_matches' in clause and 'THEN' in clause\n    True\n\n    Notes\n    -----\n    This function is used internally by _convert_clean_dose_units_to_base_units\n    to build the SQL query for unit conversion.\n    \"\"\"\n    if pattern in REGEX_TO_FACTOR_MAPPER:\n        return f\"WHEN regexp_matches(_clean_unit, '{pattern}') THEN {REGEX_TO_FACTOR_MAPPER.get(pattern)}\"\n    raise ValueError(f\"regex pattern {pattern} not found in REGEX_TO_FACTOR_MAPPER dict\")\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred","title":"clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred","text":"<pre><code>_pattern_to_factor_builder_for_preferred(pattern)\n</code></pre> <p>Build SQL CASE WHEN statement for preferred unit conversion.</p> <p>Generates SQL clauses for converting from base units back to preferred units by applying the inverse of the original conversion factor. Used when converting from standardized base units to medication-specific preferred units.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred--parameters","title":"Parameters","text":"<p>pattern : str     Regex pattern to match in _preferred_unit column.     Must exist in REGEX_TO_FACTOR_MAPPER dictionary.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred--returns","title":"Returns","text":"<p>str     SQL CASE WHEN clause with inverse conversion factor.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred--raises","title":"Raises","text":"<p>ValueError     If the pattern is not found in REGEX_TO_FACTOR_MAPPER.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred--examples","title":"Examples","text":"<p>clause = _pattern_to_factor_builder_for_preferred('/hr$') 'WHEN regexp_matches(_preferred_unit' in clause and 'THEN 1/' in clause True</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred--notes","title":"Notes","text":"<p>This function applies the inverse of the factor used in _pattern_to_factor_builder_for_base, allowing bidirectional conversion between unit systems. The inverse is calculated as 1/(original_factor).</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred--see-also","title":"See Also","text":"<p>_pattern_to_factor_builder_for_base : Builds patterns for base unit conversion</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _pattern_to_factor_builder_for_preferred(pattern: str) -&gt; str:\n    \"\"\"Build SQL CASE WHEN statement for preferred unit conversion.\n\n    Generates SQL clauses for converting from base units back to preferred units\n    by applying the inverse of the original conversion factor. Used when converting\n    from standardized base units to medication-specific preferred units.\n\n    Parameters\n    ----------\n    pattern : str\n        Regex pattern to match in _preferred_unit column.\n        Must exist in REGEX_TO_FACTOR_MAPPER dictionary.\n\n    Returns\n    -------\n    str\n        SQL CASE WHEN clause with inverse conversion factor.\n\n    Raises\n    ------\n    ValueError\n        If the pattern is not found in REGEX_TO_FACTOR_MAPPER.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clause = _pattern_to_factor_builder_for_preferred('/hr$')\n    &gt;&gt;&gt; 'WHEN regexp_matches(_preferred_unit' in clause and 'THEN 1/' in clause\n    True\n\n    Notes\n    -----\n    This function applies the inverse of the factor used in\n    _pattern_to_factor_builder_for_base, allowing bidirectional conversion\n    between unit systems. The inverse is calculated as 1/(original_factor).\n\n    See Also\n    --------\n    _pattern_to_factor_builder_for_base : Builds patterns for base unit conversion\n    \"\"\"\n    if pattern in REGEX_TO_FACTOR_MAPPER:\n        return f\"WHEN regexp_matches(_preferred_unit, '{pattern}') THEN 1/({REGEX_TO_FACTOR_MAPPER.get(pattern)})\"\n    raise ValueError(f\"regex pattern {pattern} not found in REGEX_TO_FACTOR_MAPPER dict\")\n</code></pre>"},{"location":"api/utilities/#encounter-stitching","title":"Encounter Stitching","text":""},{"location":"api/utilities/#clifpy.utils.stitching_encounters.stitch_encounters","title":"clifpy.utils.stitching_encounters.stitch_encounters","text":"<pre><code>stitch_encounters(hospitalization, adt, time_interval=6)\n</code></pre> <p>Stitches together related hospital encounters that occur within a specified time interval.</p> <p>This function identifies and groups hospitalizations that occur within a specified time window of each other (default 6 hours), treating them as a single continuous encounter. This is useful for handling cases where patients are discharged and readmitted quickly (e.g., ED to inpatient transfers).</p>"},{"location":"api/utilities/#clifpy.utils.stitching_encounters.stitch_encounters--parameters","title":"Parameters","text":"<p>hospitalization : pd.DataFrame     Hospitalization table with required columns:     - patient_id     - hospitalization_id     - admission_dttm     - discharge_dttm     - age_at_admission     - admission_type_category     - discharge_category</p> pd.DataFrame <p>ADT (Admission/Discharge/Transfer) table with required columns: - hospitalization_id - in_dttm - out_dttm - location_category - hospital_id</p> int, default=6 <p>Number of hours between discharge and next admission to consider encounters linked. If a patient is readmitted within this window, the encounters are stitched together.</p>"},{"location":"api/utilities/#clifpy.utils.stitching_encounters.stitch_encounters--returns","title":"Returns","text":"<p>Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]     hospitalization_stitched : pd.DataFrame         Enhanced hospitalization data with encounter_block column     adt_stitched : pd.DataFrame         Enhanced ADT data with encounter_block column     encounter_mapping : pd.DataFrame         Mapping of hospitalization_id to encounter_block</p>"},{"location":"api/utilities/#clifpy.utils.stitching_encounters.stitch_encounters--raises","title":"Raises","text":"<p>ValueError     If required columns are missing from input DataFrames</p>"},{"location":"api/utilities/#clifpy.utils.stitching_encounters.stitch_encounters--examples","title":"Examples","text":"<p>hosp_stitched, adt_stitched, mapping = stitch_encounters( ...     hospitalization_df,  ...     adt_df,  ...     time_interval=12  # 12-hour window ... )</p> Source code in <code>clifpy/utils/stitching_encounters.py</code> <pre><code>def stitch_encounters(\n    hospitalization: pd.DataFrame, \n    adt: pd.DataFrame, \n    time_interval: int = 6\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Stitches together related hospital encounters that occur within a specified time interval.\n\n    This function identifies and groups hospitalizations that occur within a specified time window\n    of each other (default 6 hours), treating them as a single continuous encounter. This is useful\n    for handling cases where patients are discharged and readmitted quickly (e.g., ED to inpatient\n    transfers).\n\n    Parameters\n    ----------\n    hospitalization : pd.DataFrame\n        Hospitalization table with required columns:\n        - patient_id\n        - hospitalization_id\n        - admission_dttm\n        - discharge_dttm\n        - age_at_admission\n        - admission_type_category\n        - discharge_category\n\n    adt : pd.DataFrame\n        ADT (Admission/Discharge/Transfer) table with required columns:\n        - hospitalization_id\n        - in_dttm\n        - out_dttm\n        - location_category\n        - hospital_id\n\n    time_interval : int, default=6\n        Number of hours between discharge and next admission to consider encounters linked.\n        If a patient is readmitted within this window, the encounters are stitched together.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n        hospitalization_stitched : pd.DataFrame\n            Enhanced hospitalization data with encounter_block column\n        adt_stitched : pd.DataFrame\n            Enhanced ADT data with encounter_block column\n        encounter_mapping : pd.DataFrame\n            Mapping of hospitalization_id to encounter_block\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from input DataFrames\n\n    Examples\n    --------\n    &gt;&gt;&gt; hosp_stitched, adt_stitched, mapping = stitch_encounters(\n    ...     hospitalization_df, \n    ...     adt_df, \n    ...     time_interval=12  # 12-hour window\n    ... )\n    \"\"\"\n    # Validate input DataFrames\n    hosp_required_cols = [\n        \"patient_id\", \"hospitalization_id\", \"admission_dttm\", \n        \"discharge_dttm\", \"age_at_admission\", \"admission_type_category\", \n        \"discharge_category\"\n    ]\n    adt_required_cols = [\n        \"hospitalization_id\", \"in_dttm\", \"out_dttm\", \n        \"location_category\", \"hospital_id\"\n    ]\n\n    missing_hosp_cols = [col for col in hosp_required_cols if col not in hospitalization.columns]\n    if missing_hosp_cols:\n        raise ValueError(f\"Missing required columns in hospitalization DataFrame: {missing_hosp_cols}\")\n\n    missing_adt_cols = [col for col in adt_required_cols if col not in adt.columns]\n    if missing_adt_cols:\n        raise ValueError(f\"Missing required columns in ADT DataFrame: {missing_adt_cols}\")\n    hospitalization_filtered = hospitalization[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\n                                                \"discharge_dttm\",\"age_at_admission\", \"admission_type_category\", \"discharge_category\"]].copy()\n    hospitalization_filtered['admission_dttm'] = pd.to_datetime(hospitalization_filtered['admission_dttm'])\n    hospitalization_filtered['discharge_dttm'] = pd.to_datetime(hospitalization_filtered['discharge_dttm'])\n\n    hosp_adt_join = pd.merge(hospitalization_filtered[[\"patient_id\",\"hospitalization_id\",\"age_at_admission\",\"admission_type_category\",\n                                                       \"admission_dttm\",\"discharge_dttm\",\n                                                        \"discharge_category\"]], \n                      adt[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"location_category\",\"hospital_id\"]],\n                 on=\"hospitalization_id\",how=\"left\")\n\n    hospital_cat = hosp_adt_join[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"hospital_id\"]]\n\n    # Step 1: Sort by patient_id and admission_dttm\n    hospital_block = hosp_adt_join[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\", \"age_at_admission\",  \"discharge_category\", \"admission_type_category\"]]\n    hospital_block = hospital_block.drop_duplicates()\n    hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\"]).reset_index(drop=True)\n    hospital_block = hospital_block[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\", \"age_at_admission\",  \"discharge_category\", \"admission_type_category\"]]\n\n    # Step 2: Calculate time between discharge and next admission\n    hospital_block[\"next_admission_dttm\"] = hospital_block.groupby(\"patient_id\")[\"admission_dttm\"].shift(-1)\n    hospital_block[\"discharge_to_next_admission_hrs\"] = (\n        (hospital_block[\"next_admission_dttm\"] - hospital_block[\"discharge_dttm\"]).dt.total_seconds() / 3600\n    )\n\n    # Step 3: Create linked column based on time_interval\n    eps = 1e-6  # tiny tolerance for float rounding\n    hospital_block[\"linked_hrs\"] = (\n        hospital_block[\"discharge_to_next_admission_hrs\"].le(time_interval + eps).fillna(False)\n    )\n\n    # Sort values to ensure correct order\n    hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\"]).reset_index(drop=True)\n\n    # Initialize encounter_block with row indices + 1\n    hospital_block['encounter_block'] = hospital_block.index + 1\n\n    # Iteratively propagate the encounter_block values\n    while True:\n      shifted = hospital_block['encounter_block'].shift(-1)\n      mask = hospital_block['linked_hrs'] &amp; (hospital_block['patient_id'] == hospital_block['patient_id'].shift(-1))\n      old_values = hospital_block['encounter_block'].copy()\n      hospital_block.loc[mask, 'encounter_block'] = shifted[mask]\n      if hospital_block['encounter_block'].equals(old_values):\n          break\n\n    hospital_block['encounter_block'] = hospital_block['encounter_block'].bfill().astype('int32')\n    hospital_block = pd.merge(hospital_block,hospital_cat,how=\"left\",on=\"hospitalization_id\")\n    hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\",\"in_dttm\",\"out_dttm\"]).reset_index(drop=True)\n    hospital_block = hospital_block.drop_duplicates()\n\n    hospital_block2 = hospital_block.groupby(['patient_id','encounter_block']).agg(\n        admission_dttm=pd.NamedAgg(column='admission_dttm', aggfunc='min'),\n        discharge_dttm=pd.NamedAgg(column='discharge_dttm', aggfunc='max'),\n        admission_type_category=pd.NamedAgg(column='admission_type_category', aggfunc='first'),\n        discharge_category=pd.NamedAgg(column='discharge_category', aggfunc='last'),\n        hospital_id = pd.NamedAgg(column='hospital_id', aggfunc='last'),\n        age_at_admission=pd.NamedAgg(column='age_at_admission', aggfunc='last'),\n        list_hospitalization_id=pd.NamedAgg(column='hospitalization_id', aggfunc=lambda x: sorted(x.unique()))\n    ).reset_index()\n\n    df = pd.merge(hospital_block[[\"patient_id\",\n                                  \"hospitalization_id\",\n                                  \"encounter_block\"]].drop_duplicates(),\n             hosp_adt_join[[\"hospitalization_id\",\"location_category\",\"in_dttm\",\"out_dttm\"]], on=\"hospitalization_id\",how=\"left\")\n\n    df = pd.merge(df,hospital_block2[[\"encounter_block\",\n                                      \"admission_dttm\",\n                                      \"discharge_dttm\",\n                                      \"discharge_category\",\n                                      \"admission_type_category\",\n                                      \"age_at_admission\",\n                                      \"hospital_id\",\n                                     \"list_hospitalization_id\"]],on=\"encounter_block\",how=\"left\")\n    df = df.drop_duplicates(subset=[\"patient_id\",\"encounter_block\",\"in_dttm\",\"out_dttm\",\"location_category\"])\n\n    # Create the mapping DataFrame\n    encounter_mapping = hospital_block[[\"hospitalization_id\", \"encounter_block\"]].drop_duplicates()\n\n    # Create hospitalization_stitched DataFrame\n    hospitalization_stitched = hospitalization.merge(\n        encounter_mapping, \n        on=\"hospitalization_id\", \n        how=\"left\"\n    )\n\n    # Create adt_stitched DataFrame  \n    adt_stitched = adt.merge(\n        encounter_mapping,\n        on=\"hospitalization_id\",\n        how=\"left\"\n    )\n\n    return hospitalization_stitched, adt_stitched, encounter_mapping\n</code></pre>"},{"location":"api/utilities/#respiratory-support-waterfall","title":"Respiratory Support Waterfall","text":""},{"location":"api/utilities/#clifpy.utils.waterfall.process_resp_support_waterfall","title":"clifpy.utils.waterfall.process_resp_support_waterfall","text":"<pre><code>process_resp_support_waterfall(\n    resp_support,\n    *,\n    id_col=\"hospitalization_id\",\n    bfill=False,\n    verbose=True\n)\n</code></pre> <p>Clean + waterfall-fill the CLIF <code>resp_support</code> table (Python port of Nick's reference R pipeline).</p>"},{"location":"api/utilities/#clifpy.utils.waterfall.process_resp_support_waterfall--parameters","title":"Parameters","text":"<p>resp_support : pd.DataFrame     Raw CLIF respiratory-support table already in UTC. id_col : str, default <code>\"hospitalization_id\"</code>     Encounter-level identifier column. bfill : bool, default <code>False</code>     If True, numeric setters are back-filled after forward-fill.     If False (default) only forward-fill is used. verbose : bool, default <code>True</code>     Prints progress banners when True.</p>"},{"location":"api/utilities/#clifpy.utils.waterfall.process_resp_support_waterfall--returns","title":"Returns","text":"<p>pd.DataFrame     Fully processed table with</p> <pre><code>* hourly scaffold rows (``HH:59:59``) inserted,\n* device / mode heuristics applied,\n* hierarchical episode IDs (``device_cat_id \u2192 \u2026``),\n* numeric waterfall fill inside each ``mode_name_id`` block\n  (forward-only or bi-directional per *bfill*),\n* tracheostomy flag forward-filled,\n* one unique row per ``(id_col, recorded_dttm)`` in\n  chronological order.\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.waterfall.process_resp_support_waterfall--notes","title":"Notes","text":"<p>The function does not change time-zones; convert before calling if needed.</p> Source code in <code>clifpy/utils/waterfall.py</code> <pre><code>def process_resp_support_waterfall(\n    resp_support: pd.DataFrame,\n    *,\n    id_col: str = \"hospitalization_id\",\n    bfill: bool = False,                \n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Clean + waterfall-fill the CLIF **`resp_support`** table\n    (Python port of Nick's reference R pipeline).\n\n    Parameters\n    ----------\n    resp_support : pd.DataFrame\n        Raw CLIF respiratory-support table **already in UTC**.\n    id_col : str, default ``\"hospitalization_id\"``\n        Encounter-level identifier column.\n    bfill : bool, default ``False``\n        If *True*, numeric setters are back-filled after forward-fill.\n        If *False* (default) only forward-fill is used.\n    verbose : bool, default ``True``\n        Prints progress banners when *True*.\n\n    Returns\n    -------\n    pd.DataFrame\n        Fully processed table with\n\n        * hourly scaffold rows (``HH:59:59``) inserted,\n        * device / mode heuristics applied,\n        * hierarchical episode IDs (``device_cat_id \u2192 \u2026``),\n        * numeric waterfall fill inside each ``mode_name_id`` block\n          (forward-only or bi-directional per *bfill*),\n        * tracheostomy flag forward-filled,\n        * one unique row per ``(id_col, recorded_dttm)`` in\n          chronological order.\n\n    Notes\n    -----\n    The function **does not** change time-zones; convert before\n    calling if needed.\n    \"\"\"\n\n    p = print if verbose else (lambda *_, **__: None)\n\n    # ------------------------------------------------------------------ #\n    # Helper: forward-fill only or forward + back depending on flag      #\n    # ------------------------------------------------------------------ #\n    def fb(obj):\n        if isinstance(obj, (pd.DataFrame, pd.Series)):\n            return obj.ffill().bfill() if bfill else obj.ffill()\n        raise TypeError(\"obj must be a pandas DataFrame or Series\")\n\n    # ------------------------------------------------------------------ #\n    # Small helper to build the hourly scaffold                          #\n    #   - tries DuckDB (fast), falls back to pandas                      #\n    # ------------------------------------------------------------------ #\n    def _build_hourly_scaffold(rs: pd.DataFrame) -&gt; pd.DataFrame:\n        # Try DuckDB first\n        try:\n            # local import so package doesn't hard-depend on it\n            if verbose:\n                p(\"  \u2022 Building hourly scaffold via DuckDB\")\n\n            con = duckdb.connect()\n            # Only need id + timestamps for bounds\n            con.register(\"rs\", rs[[id_col, \"recorded_dttm\"]].dropna(subset=[\"recorded_dttm\"]))\n\n            # Generate hourly series from floor(min) to floor(max), then add :59:59\n            sql = f\"\"\"\n            WITH bounds AS (\n              SELECT\n                {id_col} AS id,\n                date_trunc('hour', MIN(recorded_dttm)) AS tmin_h,\n                date_trunc('hour', MAX(recorded_dttm)) AS tmax_h\n              FROM rs\n              GROUP BY 1\n            ),\n            hour_sequence AS (\n              SELECT\n                b.id AS {id_col},\n                gs.ts + INTERVAL '59 minutes 59 seconds' AS recorded_dttm\n              FROM bounds b,\n                   LATERAL generate_series(b.tmin_h, b.tmax_h, INTERVAL 1 HOUR) AS gs(ts)\n            )\n            SELECT {id_col}, recorded_dttm\n            FROM hour_sequence\n            ORDER BY {id_col}, recorded_dttm\n            \"\"\"\n            scaffold = con.execute(sql).df()\n            con.close()\n\n            # Ensure pandas datetime with UTC if input was tz-aware\n            # (function contract says already UTC; this keeps dtype consistent)\n            scaffold[\"recorded_dttm\"] = pd.to_datetime(scaffold[\"recorded_dttm\"], utc=True, errors=\"coerce\")\n            scaffold[\"recorded_date\"] = scaffold[\"recorded_dttm\"].dt.date\n            scaffold[\"recorded_hour\"] = scaffold[\"recorded_dttm\"].dt.hour\n            scaffold[\"is_scaffold\"]   = True\n            return scaffold\n\n        except Exception as e:\n            if verbose:\n                p(f\"  \u2022 DuckDB scaffold unavailable ({type(e).__name__}: {e}). Falling back to pandas...\")\n            # ---- Original pandas scaffold (ground truth) ----\n            rs_copy = rs.copy()\n            rs_copy[\"recorded_date\"] = rs_copy[\"recorded_dttm\"].dt.date\n            rs_copy[\"recorded_hour\"] = rs_copy[\"recorded_dttm\"].dt.hour\n\n            min_max = rs_copy.groupby(id_col)[\"recorded_dttm\"].agg([\"min\", \"max\"]).reset_index()\n            tqdm.pandas(disable=not verbose, desc=\"Creating hourly scaffolds\")\n            scaffold = (\n                min_max.progress_apply(\n                    lambda r: pd.date_range(\n                        r[\"min\"].floor(\"h\"),\n                        r[\"max\"].floor(\"h\"),\n                        freq=\"1h\", tz=\"UTC\"\n                    ),\n                    axis=1,\n                )\n                .explode()\n                .rename(\"recorded_dttm\")\n            )\n            scaffold = (\n                min_max[[id_col]].join(scaffold)\n                .assign(recorded_dttm=lambda d: d[\"recorded_dttm\"].dt.floor(\"h\")\n                                               + pd.Timedelta(minutes=59, seconds=59))\n            )\n            scaffold[\"recorded_date\"] = scaffold[\"recorded_dttm\"].dt.date\n            scaffold[\"recorded_hour\"] = scaffold[\"recorded_dttm\"].dt.hour\n            scaffold[\"is_scaffold\"]   = True\n            return scaffold\n\n    # ------------------------------------------------------------------ #\n    # Phase 0 \u2013 set-up &amp; hourly scaffold                                 #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 0: initialise &amp; create hourly scaffold\")\n    rs = resp_support.copy()\n\n    # Lower-case categorical strings\n    for c in [\"device_category\", \"device_name\", \"mode_category\", \"mode_name\"]:\n        if c in rs.columns:\n            rs[c] = rs[c].str.lower()\n\n    # Numeric coercion\n    num_cols = [\n        \"tracheostomy\", \"fio2_set\", \"lpm_set\", \"peep_set\",\n        \"tidal_volume_set\", \"resp_rate_set\", \"resp_rate_obs\",\n        \"pressure_support_set\", \"peak_inspiratory_pressure_set\",\n    ]\n    num_cols = [c for c in num_cols if c in rs.columns]\n    if num_cols:\n        rs[num_cols] = rs[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n\n    # FiO\u2082 scaling if documented 40 \u2192 0.40\n    if \"fio2_set\" in rs.columns:\n        fio2_mean = rs[\"fio2_set\"].mean(skipna=True)\n        if pd.notna(fio2_mean) and fio2_mean &gt; 1.0:\n            rs.loc[rs[\"fio2_set\"] &gt; 1, \"fio2_set\"] /= 100\n            p(\"  \u2022 Scaled FiO\u2082 values &gt; 1 down by /100\")\n\n    # Build hourly scaffold (DuckDB if available, else pandas)\n    scaffold = _build_hourly_scaffold(rs)\n    if verbose:\n        p(f\"  \u2022 Scaffold rows created: {len(scaffold):,}\")\n\n    # We keep recorded_date/hour on rs only for temporary ops below\n    rs[\"recorded_date\"] = rs[\"recorded_dttm\"].dt.date\n    rs[\"recorded_hour\"] = rs[\"recorded_dttm\"].dt.hour\n\n    # ------------------------------------------------------------------ #\n    # Phase 1 \u2013 heuristic device / mode inference                        #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 1: heuristic inference of device &amp; mode\")\n\n    # Most-frequent fall-back labels\n    device_counts = rs[[\"device_name\", \"device_category\"]].value_counts().reset_index()\n\n    imv_devices = device_counts.loc[device_counts[\"device_category\"] == \"imv\", \"device_name\"]\n    most_common_imv_name = imv_devices.iloc[0] if len(imv_devices) &gt; 0 else \"ventilator\"\n\n    nippv_devices = device_counts.loc[device_counts[\"device_category\"] == \"nippv\", \"device_name\"]\n    most_common_nippv_name = nippv_devices.iloc[0] if len(nippv_devices) &gt; 0 else \"bipap\"\n\n    mode_counts = rs[[\"mode_name\", \"mode_category\"]].value_counts().reset_index()\n    cmv_modes = mode_counts.loc[\n        mode_counts[\"mode_category\"] == \"assist control-volume control\", \"mode_name\"\n    ]\n    most_common_cmv_name = cmv_modes.iloc[0] if len(cmv_modes) &gt; 0 else \"AC/VC\"\n\n    # --- 1-a IMV from mode_category\n    mask = (\n        rs[\"device_category\"].isna() &amp; rs[\"device_name\"].isna()\n        &amp; rs[\"mode_category\"].str.contains(\n            r\"(?:assist control-volume control|simv|pressure control)\", na=False, regex=True\n            )\n    )\n    rs.loc[mask, [\"device_category\", \"device_name\"]] = [\"imv\", most_common_imv_name]\n\n    # --- 1-b IMV look-behind/ahead\n    rs = rs.sort_values([id_col, \"recorded_dttm\"])\n    prev_cat = rs.groupby(id_col)[\"device_category\"].shift()\n    next_cat = rs.groupby(id_col)[\"device_category\"].shift(-1)\n    imv_like = (\n        rs[\"device_category\"].isna()\n        &amp; ((prev_cat == \"imv\") | (next_cat == \"imv\"))\n        &amp; rs[\"peep_set\"].gt(1) &amp; rs[\"resp_rate_set\"].gt(1) &amp; rs[\"tidal_volume_set\"].gt(1)\n    )\n    rs.loc[imv_like, [\"device_category\", \"device_name\"]] = [\"imv\", most_common_imv_name]\n\n    # --- 1-c NIPPV heuristics\n    prev_cat = rs.groupby(id_col)[\"device_category\"].shift()\n    next_cat = rs.groupby(id_col)[\"device_category\"].shift(-1)\n    nippv_like = (\n        rs[\"device_category\"].isna()\n        &amp; ((prev_cat == \"nippv\") | (next_cat == \"nippv\"))\n        &amp; rs[\"peak_inspiratory_pressure_set\"].gt(1)\n        &amp; rs[\"pressure_support_set\"].gt(1)\n    )\n    rs.loc[nippv_like, \"device_category\"] = \"nippv\"\n    rs.loc[nippv_like &amp; rs[\"device_name\"].isna(), \"device_name\"] = most_common_nippv_name\n\n    # --- 1-d Clean duplicates &amp; empty rows\n    rs = rs.sort_values([id_col, \"recorded_dttm\"])\n    rs[\"dup_count\"] = rs.groupby([id_col, \"recorded_dttm\"])[\"recorded_dttm\"].transform(\"size\")\n    rs = rs[~((rs[\"dup_count\"] &gt; 1) &amp; (rs[\"device_category\"] == \"nippv\"))]\n    rs[\"dup_count\"] = rs.groupby([id_col, \"recorded_dttm\"])[\"recorded_dttm\"].transform(\"size\")\n    rs = rs[~((rs[\"dup_count\"] &gt; 1) &amp; rs[\"device_category\"].isna())].drop(columns=\"dup_count\")\n\n    # --- 1-e Guard: nasal-cannula rows must never carry PEEP\n    if \"peep_set\" in rs.columns:\n        mask_bad_nc = (rs[\"device_category\"] == \"nasal cannula\") &amp; rs[\"peep_set\"].gt(0)\n        if mask_bad_nc.any():\n            rs.loc[mask_bad_nc, \"device_category\"] = np.nan\n            p(f\"{mask_bad_nc.sum():,} rows had PEEP&gt;0 on nasal cannula device_category reset\")\n\n    # Drop rows with nothing useful\n    all_na_cols = [\n        \"device_category\", \"device_name\", \"mode_category\", \"mode_name\",\n        \"tracheostomy\", \"fio2_set\", \"lpm_set\", \"peep_set\", \"tidal_volume_set\",\n        \"resp_rate_set\", \"resp_rate_obs\", \"pressure_support_set\",\n        \"peak_inspiratory_pressure_set\",\n    ]\n    rs = rs.dropna(subset=[c for c in all_na_cols if c in rs.columns], how=\"all\")\n\n    # Unique per timestamp\n    rs = rs.drop_duplicates(subset=[id_col, \"recorded_dttm\"], keep=\"first\")\n\n    # Merge scaffold (exactly like original)\n    rs[\"is_scaffold\"] = False\n    rs = pd.concat([rs, scaffold], ignore_index=True).sort_values(\n        [id_col, \"recorded_dttm\", \"recorded_date\", \"recorded_hour\"]\n    )\n\n    # ------------------------------------------------------------------ #\n    # Phase 2 \u2013 hierarchical IDs                                         #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 2: build hierarchical IDs\")\n\n    def change_id(col: pd.Series, by: pd.Series) -&gt; pd.Series:\n        return (\n            col.fillna(\"missing\")\n            .groupby(by)\n            .transform(lambda s: s.ne(s.shift()).cumsum())\n            .astype(\"int32\")\n        )\n\n    rs[\"device_category\"] = rs.groupby(id_col)[\"device_category\"].ffill()\n    rs[\"device_cat_id\"]   = change_id(rs[\"device_category\"], rs[id_col])\n\n    rs[\"device_name\"] = (\n        rs.sort_values(\"recorded_dttm\")\n          .groupby([id_col, \"device_cat_id\"])[\"device_name\"]\n          .transform(fb).infer_objects(copy=False)\n    )\n    rs[\"device_id\"] = change_id(rs[\"device_name\"], rs[id_col])\n\n    rs = rs.sort_values([id_col, \"recorded_dttm\"])\n    rs[\"mode_category\"] = (\n        rs.groupby([id_col, \"device_id\"])[\"mode_category\"]\n          .transform(fb).infer_objects(copy=False)\n    )\n    rs[\"mode_cat_id\"] = change_id(\n        rs[\"mode_category\"].fillna(\"missing\"), rs[id_col]\n    )\n\n    rs[\"mode_name\"] = (\n        rs.groupby([id_col, \"mode_cat_id\"])[\"mode_name\"]\n          .transform(fb).infer_objects(copy=False)\n    )\n    rs[\"mode_name_id\"] = change_id(\n        rs[\"mode_name\"].fillna(\"missing\"), rs[id_col]\n    )\n\n    # ------------------------------------------------------------------ #\n    # Phase 3 \u2013 numeric waterfall                                        #\n    # ------------------------------------------------------------------ #\n    fill_type = \"bi-directional\" if bfill else \"forward-only\"\n    p(f\"\u2726 Phase 3: {fill_type} numeric fill inside mode_name_id blocks\")\n\n    # FiO\u2082 default for room-air\n    if \"fio2_set\" in rs.columns:\n        rs.loc[(rs[\"device_category\"] == \"room air\") &amp; rs[\"fio2_set\"].isna(), \"fio2_set\"] = 0.21\n\n    # Tidal-volume clean-up\n    if \"tidal_volume_set\" in rs.columns:\n        bad_tv = (\n            ((rs[\"mode_category\"] == \"pressure support/cpap\") &amp; rs.get(\"pressure_support_set\").notna())\n            | (rs[\"mode_category\"].isna() &amp; rs.get(\"device_name\").str.contains(\"trach\", na=False))\n            | ((rs[\"mode_category\"] == \"pressure support/cpap\") &amp; rs.get(\"device_name\").str.contains(\"trach\", na=False))\n        )\n        rs.loc[bad_tv, \"tidal_volume_set\"] = np.nan\n\n    num_cols_fill = [\n        c for c in [\n            \"fio2_set\", \"lpm_set\", \"peep_set\", \"tidal_volume_set\",\n            \"pressure_support_set\", \"resp_rate_set\", \"resp_rate_obs\",\n            \"peak_inspiratory_pressure_set\",\n        ] if c in rs.columns\n    ]\n\n    def fill_block(g: pd.DataFrame) -&gt; pd.DataFrame:\n        if (g[\"device_category\"] == \"trach collar\").any():\n            breaker = (g[\"device_category\"] == \"trach collar\").cumsum()\n            return g.groupby(breaker)[num_cols_fill].apply(fb)\n        return fb(g[num_cols_fill])\n\n    p(f\"  \u2022 applying waterfall fill to {rs[id_col].nunique():,} encounters\")\n    tqdm.pandas(disable=not verbose, desc=\"Waterfall fill by mode_name_id\")\n    rs[num_cols_fill] = (\n        rs.groupby([id_col, \"mode_name_id\"], group_keys=False, sort=False)\n          .progress_apply(fill_block)\n    )\n\n    # \u201cT-piece\u201d \u2192 classify as blow-by\n    tpiece = rs[\"mode_category\"].isna() &amp; rs.get(\"device_name\").str.contains(\"t-piece\", na=False)\n    rs.loc[tpiece, \"mode_category\"] = \"blow by\"\n\n    # Tracheostomy flag forward-fill per encounter\n    if \"tracheostomy\" in rs.columns:\n        rs[\"tracheostomy\"] = rs.groupby(id_col)[\"tracheostomy\"].ffill()\n\n    # ------------------------------------------------------------------ #\n    # Phase 4 \u2013 final tidy-up                                            #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 4: final dedup &amp; ordering\")\n    rs = (\n        rs.drop_duplicates()\n          .sort_values([id_col, \"recorded_dttm\"])\n          .reset_index(drop=True)\n    )\n\n    # Drop helper cols\n    rs = rs.drop(columns=[c for c in [\"recorded_date\", \"recorded_hour\"] if c in rs.columns])\n\n    p(\"[OK] Respiratory-support waterfall complete.\")\n    return rs\n</code></pre>"},{"location":"api/utilities/#data-io-utilities","title":"Data I/O Utilities","text":""},{"location":"api/utilities/#clifpy.utils.io.load_data","title":"clifpy.utils.io.load_data","text":"<pre><code>load_data(\n    table_name,\n    table_path,\n    table_format_type,\n    sample_size=None,\n    columns=None,\n    filters=None,\n    site_tz=None,\n)\n</code></pre> <p>Load data from a file in the specified directory with the option to select specific columns and apply filters.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to load.</p> required <code>sample_size</code> <code>int</code> <p>Number of rows to load.</p> <code>None</code> <code>columns</code> <code>list of str</code> <p>List of column names to load.</p> <code>None</code> <code>filters</code> <code>dict</code> <p>Dictionary of filters to apply.</p> <code>None</code> <code>site_tz</code> <code>str</code> <p>Timezone string for datetime conversion, e.g., \"America/New_York\".</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the requested data.</p> <p>Usage:</p> Source code in <code>clifpy/utils/io.py</code> <pre><code>def load_data(table_name, table_path, table_format_type, sample_size=None, columns=None, filters=None, site_tz=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load data from a file in the specified directory with the option to select specific columns and apply filters.\n\n    Parameters:\n        table_name (str): The name of the table to load.\n        sample_size (int, optional): Number of rows to load.\n        columns (list of str, optional): List of column names to load.\n        filters (dict, optional): Dictionary of filters to apply.\n        site_tz (str, optional): Timezone string for datetime conversion, e.g., \"America/New_York\".\n\n    Returns:\n        pd.DataFrame: DataFrame containing the requested data.\n\n    Usage:\n\n    \"\"\"\n    # Determine the file path based on the directory and filetype\n\n    file_path = os.path.join(table_path, 'clif_'+ table_name + '.' + table_format_type)\n\n    # Load the data based on filetype\n    if os.path.exists(file_path):\n        if  table_format_type == 'csv':\n            print('Loading CSV file')\n            # For CSV, we can use DuckDB to read specific columns and apply filters efficiently\n            con = duckdb.connect()\n            # Build the SELECT clause\n            select_clause = \"*\" if not columns else \", \".join(columns)\n            # Start building the query\n            query = f\"SELECT {select_clause} FROM read_csv_auto('{file_path}')\"\n            # Apply filters\n            if filters:\n                filter_clauses = []\n                for column, values in filters.items():\n                    if isinstance(values, list):\n                        # Escape single quotes and wrap values in quotes\n                        values_list = ', '.join([\"'\" + str(value).replace(\"'\", \"''\") + \"'\" for value in values])\n                        filter_clauses.append(f\"{column} IN ({values_list})\")\n                    else:\n                        value = str(values).replace(\"'\", \"''\")\n                        filter_clauses.append(f\"{column} = '{value}'\")\n                if filter_clauses:\n                    query += \" WHERE \" + \" AND \".join(filter_clauses)\n            # Apply sample size limit\n            if sample_size:\n                query += f\" LIMIT {sample_size}\"\n            # Execute the query and fetch the data\n            df = con.execute(query).fetchdf()\n            con.close()\n        elif table_format_type == 'parquet':\n            df = load_parquet_with_tz(file_path, columns, filters, sample_size)\n        else:\n            raise ValueError(\"Unsupported filetype. Only 'csv' and 'parquet' are supported.\")\n        # Extract just the filename for cleaner output\n        filename = os.path.basename(file_path)\n        print(f\"Data loaded successfully from {filename}\")\n        df = _cast_id_cols_to_string(df) # Cast id columns to string\n\n        # Convert datetime columns to site timezone if specified\n        if site_tz:\n            df = convert_datetime_columns_to_site_tz(df, site_tz)\n\n        return df\n    else:\n        raise FileNotFoundError(f\"The file {file_path} does not exist in the specified directory.\")\n</code></pre>"},{"location":"api/utilities/#configuration-management","title":"Configuration Management","text":""},{"location":"api/utilities/#clifpy.utils.config.load_clif_config","title":"clifpy.utils.config.load_clif_config","text":"<pre><code>load_clif_config(config_path=None)\n</code></pre> <p>Load CLIF configuration from JSON or YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file. If None, looks for 'clif_config.json' or 'clif_config.yaml' in current directory.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Configuration dictionary with required fields validated</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If config file doesn't exist</p> <code>ValueError</code> <p>If required fields are missing or invalid</p> <code>JSONDecodeError</code> <p>If JSON config file is not valid</p> <code>YAMLError</code> <p>If YAML config file is not valid</p> Source code in <code>clifpy/utils/config.py</code> <pre><code>def load_clif_config(config_path: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load CLIF configuration from JSON or YAML file.\n\n    Parameters:\n        config_path (str, optional): Path to the configuration file.\n            If None, looks for 'clif_config.json' or 'clif_config.yaml' in current directory.\n\n    Returns:\n        dict: Configuration dictionary with required fields validated\n\n    Raises:\n        FileNotFoundError: If config file doesn't exist\n        ValueError: If required fields are missing or invalid\n        json.JSONDecodeError: If JSON config file is not valid\n        yaml.YAMLError: If YAML config file is not valid\n    \"\"\"\n    # Determine config file path\n    if config_path is None:\n        # Look for config files in order of preference: JSON, YAML, YML\n        cwd = os.getcwd()\n        for filename in ['clif_config.json', 'clif_config.yaml', 'clif_config.yml']:\n            potential_path = os.path.join(cwd, filename)\n            if os.path.exists(potential_path):\n                config_path = potential_path\n                break\n\n        if config_path is None:\n            raise FileNotFoundError(\n                f\"Configuration file not found in {cwd}\\n\"\n                \"Please either:\\n\"\n                \"  1. Create a clif_config.json or clif_config.yaml file in the current directory\\n\"\n                \"  2. Provide config_path parameter pointing to your config file\\n\"\n                \"  3. Provide data_directory, filetype, and timezone parameters directly\"\n            )\n\n    # Check if config file exists\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(\n            f\"Configuration file not found: {config_path}\\n\"\n            \"Please either:\\n\"\n            \"  1. Create a clif_config.json or clif_config.yaml file in the current directory\\n\"\n            \"  2. Provide config_path parameter pointing to your config file\\n\"\n            \"  3. Provide data_directory, filetype, and timezone parameters directly\"\n        )\n\n    # Load configuration using helper function\n    config = _load_config_file(config_path)\n\n    # Validate required fields\n    required_fields = ['data_directory', 'filetype', 'timezone']\n    missing_fields = [field for field in required_fields if field not in config]\n\n    if missing_fields:\n        raise ValueError(\n            f\"Missing required fields in configuration file {config_path}: {missing_fields}\\n\"\n            f\"Required fields are: {required_fields}\"\n        )\n\n    # Validate data_directory exists\n    data_dir = config['data_directory']\n    if not os.path.exists(data_dir):\n        raise ValueError(\n            f\"Data directory specified in config does not exist: {data_dir}\\n\"\n            f\"Please check the 'data_directory' path in {config_path}\"\n        )\n\n    # Validate filetype\n    supported_filetypes = ['csv', 'parquet']\n    if config['filetype'] not in supported_filetypes:\n        raise ValueError(\n            f\"Unsupported filetype '{config['filetype']}' in {config_path}\\n\"\n            f\"Supported filetypes are: {supported_filetypes}\"\n        )\n\n    print(f\"Configuration loaded successfully from {config_path}\")\n    return config\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.config.get_config_or_params","title":"clifpy.utils.config.get_config_or_params","text":"<pre><code>get_config_or_params(\n    config_path=None,\n    data_directory=None,\n    filetype=None,\n    timezone=None,\n    output_directory=None,\n)\n</code></pre> <p>Get configuration from either config file or direct parameters.</p> <p>Loading priority: 1. If all required params provided directly \u2192 use them 2. If config_path provided \u2192 load from that path, allow param overrides 3. If no params and no config_path \u2192 auto-detect clif_config.json/yaml/yml 4. Parameters override config file values when both are provided</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to configuration file</p> <code>None</code> <code>data_directory</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Direct parameter  </p> <code>None</code> <code>timezone</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Final configuration dictionary</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither config nor required params are provided</p> Source code in <code>clifpy/utils/config.py</code> <pre><code>def get_config_or_params(\n    config_path: Optional[str] = None,\n    data_directory: Optional[str] = None,\n    filetype: Optional[str] = None,\n    timezone: Optional[str] = None,\n    output_directory: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get configuration from either config file or direct parameters.\n\n    Loading priority:\n    1. If all required params provided directly \u2192 use them\n    2. If config_path provided \u2192 load from that path, allow param overrides\n    3. If no params and no config_path \u2192 auto-detect clif_config.json/yaml/yml\n    4. Parameters override config file values when both are provided\n\n    Parameters:\n        config_path (str, optional): Path to configuration file\n        data_directory (str, optional): Direct parameter\n        filetype (str, optional): Direct parameter  \n        timezone (str, optional): Direct parameter\n        output_directory (str, optional): Direct parameter\n\n    Returns:\n        dict: Final configuration dictionary\n\n    Raises:\n        ValueError: If neither config nor required params are provided\n    \"\"\"\n    # Check if all required params are provided directly\n    required_params = [data_directory, filetype, timezone]\n    if all(param is not None for param in required_params):\n        # All required params provided - use them directly\n        config = {\n            'data_directory': data_directory,\n            'filetype': filetype,\n            'timezone': timezone\n        }\n        if output_directory is not None:\n            config['output_directory'] = output_directory\n        print(\"Using directly provided parameters\")\n        return config\n\n    # Try to load from config file\n    try:\n        config = load_clif_config(config_path)\n    except FileNotFoundError:\n        # If no config file and incomplete params, raise helpful error\n        if any(param is not None for param in required_params):\n            # Some params provided but not all\n            missing = []\n            if data_directory is None:\n                missing.append('data_directory')\n            if filetype is None:\n                missing.append('filetype') \n            if timezone is None:\n                missing.append('timezone')\n            raise ValueError(\n                f\"Incomplete parameters provided. Missing: {missing}\\n\"\n                \"Please either:\\n\"\n                \"  1. Provide all required parameters (data_directory, filetype, timezone)\\n\"\n                \"  2. Create a clif_config.json or clif_config.yaml file\\n\"\n                \"  3. Provide a config_path parameter\"\n            )\n        else:\n            # No params and no config file - re-raise the original error\n            raise\n\n    # Override config values with any provided parameters\n    if data_directory is not None:\n        config['data_directory'] = data_directory\n        print(f\"Overriding data_directory from config with: {data_directory}\")\n\n    if filetype is not None:\n        config['filetype'] = filetype\n        print(f\"Overriding filetype from config with: {filetype}\")\n\n    if timezone is not None:\n        config['timezone'] = timezone\n        print(f\"Overriding timezone from config with: {timezone}\")\n\n    if output_directory is not None:\n        config['output_directory'] = output_directory\n        print(f\"Overriding output_directory from config with: {output_directory}\")\n\n    return config\n</code></pre>"},{"location":"api/utilities/#data-validation","title":"Data Validation","text":""},{"location":"api/utilities/#clifpy.utils.validator.check_required_columns","title":"clifpy.utils.validator.check_required_columns","text":"<pre><code>check_required_columns(df, column_names, table_name)\n</code></pre> <p>Validate that required columns are present in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>column_names</code> <code>List[str]</code> <p>List of required column names</p> required <code>table_name</code> <code>str</code> <p>Name of the table being validated</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary with validation results including missing columns</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def check_required_columns(\n    df: pd.DataFrame, \n    column_names: List[str], \n    table_name: str\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate that required columns are present in the dataframe.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe to validate\n        column_names (List[str]): List of required column names\n        table_name (str): Name of the table being validated\n\n    Returns:\n        dict: Dictionary with validation results including missing columns\n    \"\"\"\n    try:\n        missing_columns = [col for col in column_names if col not in df.columns]\n\n        if missing_columns:\n            return {\n                \"type\": \"missing_required_columns\",\n                \"table\": table_name,\n                \"missing_columns\": missing_columns,\n                \"status\": \"error\"\n            }\n\n        return {\n            \"type\": \"missing_required_columns\",\n            \"table\": table_name,\n            \"status\": \"success\"\n        }\n\n    except Exception as e:\n        return {\n            \"type\": \"missing_required_columns\",\n            \"table\": table_name,\n            \"status\": \"error\",\n            \"error_message\": str(e)\n        }\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.verify_column_dtypes","title":"clifpy.utils.validator.verify_column_dtypes","text":"<pre><code>verify_column_dtypes(df, schema)\n</code></pre> <p>Ensure columns have correct data types per schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>schema</code> <code>dict</code> <p>Schema containing column definitions</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[dict]: List of datatype mismatch errors</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def verify_column_dtypes(df: pd.DataFrame, schema: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Ensure columns have correct data types per schema.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe to validate\n        schema (dict): Schema containing column definitions\n\n    Returns:\n        List[dict]: List of datatype mismatch errors\n    \"\"\"\n    errors = []\n\n    try:\n        for col_spec in schema.get(\"columns\", []):\n            name = col_spec[\"name\"]\n            if name not in df.columns:\n                continue\n\n            expected_type = col_spec.get(\"data_type\")\n            if not expected_type:\n                continue\n\n            series = df[name]\n            checker = _DATATYPE_CHECKERS.get(expected_type)\n            cast_checker = _DATATYPE_CAST_CHECKERS.get(expected_type)\n\n            if checker and not checker(series):\n                # Check if data can be cast to the correct type\n                if cast_checker and cast_checker(series):\n                    # Data can be cast - this is a warning, not an error\n                    errors.append({\n                        \"type\": \"datatype_verification_castable\",\n                        \"column\": name,\n                        \"expected\": expected_type,\n                        \"actual\": str(series.dtype),\n                        \"status\": \"warning\",\n                        \"message\": f\"Column '{name}' has type {series.dtype} but can be cast to {expected_type}\"\n                    })\n                else:\n                    # Data cannot be cast - this is an error\n                    errors.append({\n                        \"type\": \"datatype_verification\",\n                        \"column\": name,\n                        \"expected\": expected_type,\n                        \"actual\": str(series.dtype),\n                        \"status\": \"error\",\n                        \"message\": f\"Column '{name}' has type {series.dtype} and cannot be cast to {expected_type}\"\n                    })\n\n    except Exception as e:\n        errors.append({\n            \"type\": \"datatype_verification\",\n            \"status\": \"error\",\n            \"error_message\": str(e)\n        })\n\n    return errors\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.validate_datetime_timezone","title":"clifpy.utils.validator.validate_datetime_timezone","text":"<pre><code>validate_datetime_timezone(df, datetime_columns)\n</code></pre> <p>Validate that all datetime columns are in UTC format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>datetime_columns</code> <code>List[str]</code> <p>List of datetime column names</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[dict]: List of timezone validation results</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def validate_datetime_timezone(\n    df: pd.DataFrame, \n    datetime_columns: List[str]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Validate that all datetime columns are in UTC format.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe to validate\n        datetime_columns (List[str]): List of datetime column names\n\n    Returns:\n        List[dict]: List of timezone validation results\n    \"\"\"\n    results = []\n\n    try:\n        for col in datetime_columns:\n            if col not in df.columns:\n                continue\n\n            if pd.api.types.is_datetime64_any_dtype(df[col]):\n                # Check if timezone-aware\n                if df[col].dt.tz is not None:\n                    # Check if UTC\n                    if str(df[col].dt.tz) != 'UTC':\n                        results.append({\n                            \"type\": \"datetime_timezone\",\n                            \"column\": col,\n                            \"timezone\": str(df[col].dt.tz),\n                            \"expected\": \"UTC\",\n                            \"status\": \"warning\"\n                        })\n                else:\n                    # Timezone-naive datetime\n                    results.append({\n                        \"type\": \"datetime_timezone\",\n                        \"column\": col,\n                        \"timezone\": \"naive\",\n                        \"expected\": \"UTC\",\n                        \"status\": \"info\"\n                    })\n\n    except Exception as e:\n        results.append({\n            \"type\": \"datetime_timezone\",\n            \"status\": \"error\",\n            \"error_message\": str(e)\n        })\n\n    return results\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.calculate_missing_stats","title":"clifpy.utils.validator.calculate_missing_stats","text":"<pre><code>calculate_missing_stats(df, format='long')\n</code></pre> <p>Report count and percentage of missing values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to analyze</p> required <code>format</code> <code>str</code> <p>Output format ('long' or 'wide')</p> <code>'long'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Missing data statistics</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def calculate_missing_stats(\n    df: pd.DataFrame, \n    format: str = 'long'\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Report count and percentage of missing values.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe to analyze\n        format (str): Output format ('long' or 'wide')\n\n    Returns:\n        pd.DataFrame: Missing data statistics\n    \"\"\"\n    try:\n        missing_count = df.isnull().sum()\n        missing_percent = (missing_count / len(df)) * 100\n\n        if format == 'long':\n            stats_df = pd.DataFrame({\n                'column': missing_count.index,\n                'missing_count': missing_count.values,\n                'missing_percent': missing_percent.values,\n                'total_rows': len(df)\n            })\n            # Sort by missing percentage descending\n            stats_df = stats_df.sort_values('missing_percent', ascending=False)\n\n        else:  # wide format\n            stats_df = pd.DataFrame({\n                'missing_count': missing_count,\n                'missing_percent': missing_percent\n            }).T\n\n        return stats_df\n\n    except Exception as e:\n        return pd.DataFrame({'error': [str(e)]})\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.validate_categorical_values","title":"clifpy.utils.validator.validate_categorical_values","text":"<pre><code>validate_categorical_values(df, schema)\n</code></pre> <p>Check values against permitted categories.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>schema</code> <code>dict</code> <p>Schema containing category definitions</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[dict]: List of invalid category value errors</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def validate_categorical_values(\n    df: pd.DataFrame, \n    schema: Dict[str, Any]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Check values against permitted categories.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe to validate\n        schema (dict): Schema containing category definitions\n\n    Returns:\n        List[dict]: List of invalid category value errors\n    \"\"\"\n    errors = []\n\n    try:\n        category_columns = schema.get(\"category_columns\", [])\n\n        for col_spec in schema.get(\"columns\", []):\n            name = col_spec[\"name\"]\n\n            if name not in df.columns or name not in category_columns:\n                continue\n\n            if col_spec.get(\"permissible_values\"):\n                allowed = set(col_spec[\"permissible_values\"])\n\n                # Get unique values in the column (excluding NaN)\n                unique_values = set(df[name].dropna().unique())\n                # Check for missing expected values (permissible values not present in data)\n                missing_values = [v for v in allowed if v not in unique_values]\n                if missing_values:\n                    errors.append({\n                                \"type\": \"missing_categorical_values\",\n                                \"column\": name,\n                                \"missing_values\": missing_values,  \n                                \"total_missing\": len(missing_values),\n                                \"message\": f\"Column '{name}' is missing {len(missing_values)} expected category values\"\n                            })\n\n    except Exception as e:\n        errors.append({\n            \"type\": \"categorical_validation\",\n            \"status\": \"error\",\n            \"error_message\": str(e)\n        })\n\n    return errors\n</code></pre>"},{"location":"api/utilities/#outlier-handling","title":"Outlier Handling","text":""},{"location":"api/utilities/#clifpy.utils.outlier_handler.apply_outlier_handling","title":"clifpy.utils.outlier_handler.apply_outlier_handling","text":"<pre><code>apply_outlier_handling(table_obj, outlier_config_path=None)\n</code></pre> <p>Apply outlier handling to a table object's dataframe.</p> <p>This function identifies numeric values that fall outside acceptable ranges and converts them to NaN. For category-dependent columns (vitals, labs, medications, assessments), ranges are applied based on the category value.</p> <p>Uses ultra-fast Polars implementation with progress tracking.</p> <p>Parameters:</p> Name Type Description Default <code>table_obj</code> <p>A pyCLIF table object with .df (DataFrame) and .table_name attributes</p> required <code>outlier_config_path</code> <code>str</code> <p>Path to custom outlier configuration YAML.                                If None, uses internal CLIF standard config.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None (modifies table_obj.df in-place)</p> Source code in <code>clifpy/utils/outlier_handler.py</code> <pre><code>def apply_outlier_handling(table_obj, outlier_config_path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Apply outlier handling to a table object's dataframe.\n\n    This function identifies numeric values that fall outside acceptable ranges\n    and converts them to NaN. For category-dependent columns (vitals, labs,\n    medications, assessments), ranges are applied based on the category value.\n\n    Uses ultra-fast Polars implementation with progress tracking.\n\n    Parameters:\n        table_obj: A pyCLIF table object with .df (DataFrame) and .table_name attributes\n        outlier_config_path (str, optional): Path to custom outlier configuration YAML.\n                                           If None, uses internal CLIF standard config.\n\n    Returns:\n        None (modifies table_obj.df in-place)\n    \"\"\"\n    if table_obj.df is None or table_obj.df.empty:\n        print(\"No data to process for outlier handling.\")\n        return\n\n    # Load outlier configuration\n    config = _load_outlier_config(outlier_config_path)\n    if not config:\n        print(\"Failed to load outlier configuration.\")\n        return\n\n    # Print which configuration is being used\n    if outlier_config_path is None:\n        print(\"Using CLIF standard outlier ranges\\n\")\n    else:\n        print(f\"Using custom outlier ranges from: {outlier_config_path}\\n\")\n\n    # Get table-specific configuration\n    table_config = config.get('tables', {}).get(table_obj.table_name, {})\n    if not table_config:\n        print(f\"No outlier configuration found for table: {table_obj.table_name}\")\n        return\n\n    # Filter columns that exist in the dataframe\n    existing_columns = {col: conf for col, conf in table_config.items() if col in table_obj.df.columns}\n\n    if not existing_columns:\n        print(\"No configured columns found in dataframe.\")\n        return\n\n    # Ultra-fast processing with single conversion\n    _process_all_columns_ultra_fast(table_obj, existing_columns)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.outlier_handler.get_outlier_summary","title":"clifpy.utils.outlier_handler.get_outlier_summary","text":"<pre><code>get_outlier_summary(table_obj, outlier_config_path=None)\n</code></pre> <p>Get a summary of potential outliers without modifying the data.</p> <p>Parameters:</p> Name Type Description Default <code>table_obj</code> <p>A pyCLIF table object with .df and .table_name attributes</p> required <code>outlier_config_path</code> <code>str</code> <p>Path to custom outlier configuration</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Summary of outliers by column and category</p> Source code in <code>clifpy/utils/outlier_handler.py</code> <pre><code>def get_outlier_summary(table_obj, outlier_config_path: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get a summary of potential outliers without modifying the data.\n\n    Parameters:\n        table_obj: A pyCLIF table object with .df and .table_name attributes\n        outlier_config_path (str, optional): Path to custom outlier configuration\n\n    Returns:\n        dict: Summary of outliers by column and category\n    \"\"\"\n    if table_obj.df is None or table_obj.df.empty:\n        return {\"status\": \"No data to analyze\"}\n\n    config = _load_outlier_config(outlier_config_path)\n    if not config:\n        return {\"status\": \"Failed to load configuration\"}\n\n    table_config = config.get('tables', {}).get(table_obj.table_name, {})\n    if not table_config:\n        return {\"status\": f\"No configuration for table: {table_obj.table_name}\"}\n\n    summary = {\n        \"table_name\": table_obj.table_name,\n        \"total_rows\": len(table_obj.df),\n        \"columns_analyzed\": {},\n        \"config_source\": \"CLIF standard\" if outlier_config_path is None else \"Custom\"\n    }\n\n    # Analyze each column without modifying data\n    for column_name, column_config in table_config.items():\n        if column_name not in table_obj.df.columns:\n            continue\n\n        column_summary = _analyze_column_outliers_pandas(table_obj, column_name, column_config)\n        if column_summary:\n            summary[\"columns_analyzed\"][column_name] = column_summary\n\n    return summary\n</code></pre>"},{"location":"api/utilities/#wide-dataset-creation","title":"Wide Dataset Creation","text":""},{"location":"api/utilities/#clifpy.utils.wide_dataset.create_wide_dataset","title":"clifpy.utils.wide_dataset.create_wide_dataset","text":"<pre><code>create_wide_dataset(\n    clif_instance,\n    optional_tables=None,\n    category_filters=None,\n    sample=False,\n    hospitalization_ids=None,\n    cohort_df=None,\n    output_format=\"dataframe\",\n    save_to_data_location=False,\n    output_filename=None,\n    return_dataframe=True,\n    base_table_columns=None,\n    batch_size=1000,\n    memory_limit=None,\n    threads=None,\n    show_progress=True,\n)\n</code></pre> <p>Create a wide dataset by joining multiple CLIF tables with pivoting support.</p> <p>Parameters:</p> Name Type Description Default <code>clif_instance</code> <p>CLIF object with loaded data</p> required <code>optional_tables</code> <code>Optional[List[str]]</code> <p>DEPRECATED - use category_filters to specify tables</p> <code>None</code> <code>category_filters</code> <code>Optional[Dict[str, List[str]]]</code> <p>Dict specifying which categories to include for each table              Keys are table names, values are lists of categories to filter              Table presence in this dict determines if it will be loaded</p> <code>None</code> <code>sample</code> <code>bool</code> <p>Boolean - if True, randomly select 20 hospitalizations</p> <code>False</code> <code>hospitalization_ids</code> <code>Optional[List[str]]</code> <p>List of specific hospitalization IDs to filter</p> <code>None</code> <code>cohort_df</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame with columns ['hospitalization_id', 'start_time', 'end_time']        If provided, data will be filtered to only include events within the specified        time windows for each hospitalization</p> <code>None</code> <code>output_format</code> <code>str</code> <p>'dataframe', 'csv', or 'parquet'</p> <code>'dataframe'</code> <code>save_to_data_location</code> <code>bool</code> <p>Boolean - save output to data directory</p> <code>False</code> <code>output_filename</code> <code>Optional[str]</code> <p>Custom filename (default: 'wide_dataset_YYYYMMDD_HHMMSS')</p> <code>None</code> <code>return_dataframe</code> <code>bool</code> <p>Boolean - return DataFrame even when saving to file (default=True)</p> <code>True</code> <code>base_table_columns</code> <code>Optional[Dict[str, List[str]]]</code> <p>DEPRECATED - columns are selected automatically</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of hospitalizations to process in each batch (default=1000)</p> <code>1000</code> <code>memory_limit</code> <code>Optional[str]</code> <p>DuckDB memory limit (e.g., '8GB')</p> <code>None</code> <code>threads</code> <code>Optional[int]</code> <p>Number of threads for DuckDB to use</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bars for long operations</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>pd.DataFrame or None (if return_dataframe=False)</p> Source code in <code>clifpy/utils/wide_dataset.py</code> <pre><code>def create_wide_dataset(\n    clif_instance,\n    optional_tables: Optional[List[str]] = None,\n    category_filters: Optional[Dict[str, List[str]]] = None,\n    sample: bool = False,\n    hospitalization_ids: Optional[List[str]] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    output_format: str = 'dataframe',\n    save_to_data_location: bool = False,\n    output_filename: Optional[str] = None,\n    return_dataframe: bool = True,\n    base_table_columns: Optional[Dict[str, List[str]]] = None,\n    batch_size: int = 1000,\n    memory_limit: Optional[str] = None,\n    threads: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Create a wide dataset by joining multiple CLIF tables with pivoting support.\n\n    Parameters:\n        clif_instance: CLIF object with loaded data\n        optional_tables: DEPRECATED - use category_filters to specify tables\n        category_filters: Dict specifying which categories to include for each table\n                         Keys are table names, values are lists of categories to filter\n                         Table presence in this dict determines if it will be loaded\n        sample: Boolean - if True, randomly select 20 hospitalizations\n        hospitalization_ids: List of specific hospitalization IDs to filter\n        cohort_df: Optional DataFrame with columns ['hospitalization_id', 'start_time', 'end_time']\n                   If provided, data will be filtered to only include events within the specified\n                   time windows for each hospitalization\n        output_format: 'dataframe', 'csv', or 'parquet'\n        save_to_data_location: Boolean - save output to data directory\n        output_filename: Custom filename (default: 'wide_dataset_YYYYMMDD_HHMMSS')\n        return_dataframe: Boolean - return DataFrame even when saving to file (default=True)\n        base_table_columns: DEPRECATED - columns are selected automatically\n        batch_size: Number of hospitalizations to process in each batch (default=1000)\n        memory_limit: DuckDB memory limit (e.g., '8GB')\n        threads: Number of threads for DuckDB to use\n        show_progress: Show progress bars for long operations\n\n    Returns:\n        pd.DataFrame or None (if return_dataframe=False)\n    \"\"\"\n\n    print(\"\\nPhase 4: Wide Dataset Processing (utility function)\")\n    print(\"  4.1: Starting wide dataset creation...\")\n\n    # Validate cohort_df if provided\n    if cohort_df is not None:\n        required_cols = ['hospitalization_id', 'start_time', 'end_time']\n        missing_cols = [col for col in required_cols if col not in cohort_df.columns]\n        if missing_cols:\n            raise ValueError(f\"cohort_df must contain columns: {required_cols}. Missing: {missing_cols}\")\n\n        # Ensure hospitalization_id is string type to match with other tables\n        cohort_df['hospitalization_id'] = cohort_df['hospitalization_id'].astype(str)\n\n        # Ensure time columns are datetime\n        for time_col in ['start_time', 'end_time']:\n            if not pd.api.types.is_datetime64_any_dtype(cohort_df[time_col]):\n                cohort_df[time_col] = pd.to_datetime(cohort_df[time_col])\n\n        print(\"  === SPECIAL: COHORT TIME WINDOW FILTERING ===\")\n        print(f\"       - Processing {len(cohort_df)} hospitalizations with time windows\")\n        print(f\"       - Ensuring datetime types for start_time, end_time\")\n        print(\"\")\n\n    # Define tables that need pivoting vs those already wide\n    PIVOT_TABLES = ['vitals', 'labs', 'medication_admin_continuous', 'medication_admin_intermittent', 'patient_assessments']\n    WIDE_TABLES = ['respiratory_support']\n\n    # Determine which tables to load from category_filters\n    if category_filters is None:\n        category_filters = {}\n\n    # For backward compatibility with optional_tables\n    if optional_tables and not category_filters:\n        print(\"Warning: optional_tables parameter is deprecated. Converting to category_filters format.\")\n        category_filters = {table: [] for table in optional_tables}\n\n    tables_to_load = list(category_filters.keys())\n\n    # Create DuckDB connection with optimized settings\n    conn_config = {\n        'preserve_insertion_order': 'false'\n    }\n\n    if memory_limit:\n        conn_config['memory_limit'] = memory_limit\n    if threads:\n        conn_config['threads'] = str(threads)\n\n    # Use context manager for connection\n    with duckdb.connect(':memory:', config=conn_config) as conn:\n        # Set additional optimization settings\n        conn.execute(\"SET preserve_insertion_order = false\")\n\n        # Get hospitalization IDs to process\n        hospitalization_df = clif_instance.hospitalization.df.copy()\n\n        if hospitalization_ids is not None:\n            print(f\"Filtering to specific hospitalization IDs: {len(hospitalization_ids)} encounters\")\n            required_ids = hospitalization_ids\n        elif cohort_df is not None:\n            # Use hospitalization IDs from cohort_df\n            required_ids = cohort_df['hospitalization_id'].unique().tolist()\n            print(f\"Using {len(required_ids)} hospitalization IDs from cohort_df\")\n        elif sample:\n            print(\"Sampling 20 random hospitalizations...\")\n            all_ids = hospitalization_df['hospitalization_id'].unique()\n            required_ids = np.random.choice(all_ids, size=min(20, len(all_ids)), replace=False).tolist()\n            print(f\"Selected {len(required_ids)} hospitalizations for sampling\")\n        else:\n            required_ids = hospitalization_df['hospitalization_id'].unique().tolist()\n            print(f\"Processing all {len(required_ids)} hospitalizations\")\n\n        # Filter all base tables by required IDs immediately\n        print(\"\\nLoading and filtering base tables...\")\n        # Only keep required columns from hospitalization table\n        hosp_required_cols = ['hospitalization_id', 'patient_id', 'age_at_admission']\n        hosp_available_cols = [col for col in hosp_required_cols if col in hospitalization_df.columns]\n        hospitalization_df = hospitalization_df[hosp_available_cols]\n        hospitalization_df = hospitalization_df[hospitalization_df['hospitalization_id'].isin(required_ids)]\n        patient_df = clif_instance.patient.df[['patient_id']].copy()\n\n        # Get ADT with selected columns\n        adt_df = clif_instance.adt.df.copy()\n        adt_df = adt_df[adt_df['hospitalization_id'].isin(required_ids)]\n\n        # Apply time filtering to ADT if cohort_df is provided\n        if cohort_df is not None and 'in_dttm' in adt_df.columns:\n            pre_filter_count = len(adt_df)\n            # Merge with cohort_df to get time windows\n            adt_df = pd.merge(\n                adt_df,\n                cohort_df[['hospitalization_id', 'start_time', 'end_time']],\n                on='hospitalization_id',\n                how='inner'\n            )\n\n            # Ensure in_dttm column is datetime\n            if not pd.api.types.is_datetime64_any_dtype(adt_df['in_dttm']):\n                adt_df['in_dttm'] = pd.to_datetime(adt_df['in_dttm'])\n\n            # Filter to time window\n            adt_df = adt_df[\n                (adt_df['in_dttm'] &gt;= adt_df['start_time']) &amp;\n                (adt_df['in_dttm'] &lt;= adt_df['end_time'])\n            ].copy()\n\n            # Drop the time window columns\n            adt_df = adt_df.drop(columns=['start_time', 'end_time'])\n\n            print(f\"  ADT time filtering: {pre_filter_count} \u2192 {len(adt_df)} records\")\n\n        # Remove duplicate columns and _name columns\n        adt_cols = [col for col in adt_df.columns if not col.endswith('_name') and col != 'patient_id']\n        adt_df = adt_df[adt_cols]\n\n        print(f\"       - Base tables filtered - Hospitalization: {len(hospitalization_df)}, Patient: {len(patient_df)}, ADT: {len(adt_df)}\")\n\n        print(\"\\n  4.2: Determining processing mode\")\n        # Process in batches to avoid memory issues\n        if batch_size &gt; 0 and len(required_ids) &gt; batch_size:\n            print(f\"       - Batch mode: {len(required_ids)} hospitalizations in {len(required_ids)//batch_size + 1} batches of {batch_size}\")\n            print(\"  4.B: === BATCH PROCESSING MODE ===\")\n            return _process_in_batches(\n                conn, clif_instance, required_ids, patient_df, hospitalization_df, adt_df,\n                tables_to_load, category_filters, PIVOT_TABLES, WIDE_TABLES,\n                batch_size, show_progress, save_to_data_location, output_filename,\n                output_format, return_dataframe, cohort_df\n            )\n        else:\n            print(f\"       - Single mode: Processing all {len(required_ids)} hospitalizations at once\")\n            print(\"  4.S: === SINGLE PROCESSING MODE ===\")\n            # Process all at once for small datasets\n            return _process_hospitalizations(\n                conn, clif_instance, required_ids, patient_df, hospitalization_df, adt_df,\n                tables_to_load, category_filters, PIVOT_TABLES, WIDE_TABLES,\n                show_progress, cohort_df\n            )\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.wide_dataset.convert_wide_to_hourly","title":"clifpy.utils.wide_dataset.convert_wide_to_hourly","text":"<pre><code>convert_wide_to_hourly(\n    wide_df,\n    aggregation_config,\n    memory_limit=\"4GB\",\n    temp_directory=None,\n    batch_size=None,\n)\n</code></pre> <p>Convert a wide dataset to hourly aggregation with user-defined aggregation methods.</p> <p>This function uses DuckDB for high-performance aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>wide_df</code> <code>DataFrame</code> <p>Wide dataset DataFrame from create_wide_dataset()</p> required <code>aggregation_config</code> <code>Dict[str, List[str]]</code> <p>Dict mapping aggregation methods to list of columns Example: {     'max': ['map', 'temp_c', 'sbp'],     'mean': ['heart_rate', 'respiratory_rate'],     'min': ['spo2'],     'median': ['glucose'],     'first': ['gcs_total', 'rass'],     'last': ['assessment_value'],     'boolean': ['norepinephrine', 'propofol'],     'one_hot_encode': ['medication_name', 'assessment_category'] }</p> required <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '4GB', '8GB')</p> <code>'4GB'</code> <code>temp_directory</code> <code>Optional[str]</code> <p>Directory for temporary files (default: system temp)</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Process in batches if dataset is large (auto-determined if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Hourly aggregated wide dataset with nth_hour column</p> Source code in <code>clifpy/utils/wide_dataset.py</code> <pre><code>def convert_wide_to_hourly(\n    wide_df: pd.DataFrame, \n    aggregation_config: Dict[str, List[str]],\n    memory_limit: str = '4GB',\n    temp_directory: Optional[str] = None,\n    batch_size: Optional[int] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a wide dataset to hourly aggregation with user-defined aggregation methods.\n\n    This function uses DuckDB for high-performance aggregation.\n\n    Parameters:\n        wide_df: Wide dataset DataFrame from create_wide_dataset()\n        aggregation_config: Dict mapping aggregation methods to list of columns\n            Example: {\n                'max': ['map', 'temp_c', 'sbp'],\n                'mean': ['heart_rate', 'respiratory_rate'],\n                'min': ['spo2'],\n                'median': ['glucose'],\n                'first': ['gcs_total', 'rass'],\n                'last': ['assessment_value'],\n                'boolean': ['norepinephrine', 'propofol'],\n                'one_hot_encode': ['medication_name', 'assessment_category']\n            }\n        memory_limit: DuckDB memory limit (e.g., '4GB', '8GB')\n        temp_directory: Directory for temporary files (default: system temp)\n        batch_size: Process in batches if dataset is large (auto-determined if None)\n\n    Returns:\n        pd.DataFrame: Hourly aggregated wide dataset with nth_hour column\n    \"\"\"\n\n    print(\"Starting optimized hourly aggregation using DuckDB...\")\n    print(f\"Input dataset shape: {wide_df.shape}\")\n    print(f\"Memory limit: {memory_limit}\")\n\n    # Validate input\n    required_columns = ['event_time', 'hospitalization_id', 'day_number']\n    for col in required_columns:\n        if col not in wide_df.columns:\n            raise ValueError(f\"wide_df must contain '{col}' column\")\n\n    # Auto-determine batch size for very large datasets\n    if batch_size is None:\n        n_rows = len(wide_df)\n        n_hospitalizations = wide_df['hospitalization_id'].nunique()\n\n        # Use batching if dataset is very large\n        if n_rows &gt; 1_000_000 or n_hospitalizations &gt; 10_000:\n            batch_size = min(5000, n_hospitalizations // 4)\n            print(f\"Large dataset detected ({n_rows:,} rows, {n_hospitalizations:,} hospitalizations)\")\n            print(f\"Will process in batches of {batch_size} hospitalizations\")\n        else:\n            batch_size = 0  # Process all at once\n\n    # Configure DuckDB connection\n    config = {\n        'memory_limit': memory_limit,\n        'temp_directory': temp_directory or '/tmp/duckdb_temp',\n        'preserve_insertion_order': 'false',\n        'threads': '4'\n    }\n\n    # Remove None values from config\n    config = {k: v for k, v in config.items() if v is not None}\n\n    try:\n        # Create DuckDB connection with error handling\n        with duckdb.connect(':memory:', config=config) as conn:\n            # Set additional optimization settings\n            conn.execute(\"SET preserve_insertion_order = false\")\n\n            if batch_size &gt; 0:\n                return _process_hourly_in_batches(conn, wide_df, aggregation_config, batch_size)\n            else:\n                return _process_hourly_single_batch(conn, wide_df, aggregation_config)\n\n    except Exception as e:\n        print(f\"DuckDB processing failed: {str(e)}\")\n        raise\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples of using CLIFpy for common ICU data analysis tasks.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":""},{"location":"examples/#loading-data","title":"Loading Data","text":"<p>Learn different ways to load CLIF data, including: - Loading from CSV and Parquet files - Using filters and column selection - Working with sample data - Handling large datasets efficiently</p>"},{"location":"examples/#analyzing-icu-stays","title":"Analyzing ICU Stays","text":"<p>Common ICU analysis patterns: - Identifying ICU admissions - Calculating length of stay - Tracking patient movement - Analyzing severity of illness</p>"},{"location":"examples/#clinical-calculations","title":"Clinical Calculations","text":"<p>Implement clinical calculations and scores: - Calculating SOFA scores - Tracking vasopressor requirements - Monitoring ventilation parameters - Assessing prone positioning compliance</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#basic-data-loading","title":"Basic Data Loading","text":"<pre><code>from clifpy.tables import Patient, Labs, Vitals\nfrom clifpy.clif_orchestrator import ClifOrchestrator\n\n# Load individual tables\npatient = Patient.from_file('/data', 'parquet', timezone='US/Central')\nlabs = Labs.from_file('/data', 'parquet', timezone='US/Central')\n\n# Or use orchestrator for multiple tables\norchestrator = ClifOrchestrator('/data', 'parquet', 'US/Central')\norchestrator.initialize(tables=['patient', 'labs', 'vitals', 'adt'])\n</code></pre>"},{"location":"examples/#finding-icu-patients","title":"Finding ICU Patients","text":"<pre><code># Get ICU admissions\nicu_stays = orchestrator.adt.filter_by_location_category('icu')\nicu_patients = icu_stays['patient_id'].unique()\n\n# Get their demographics\nicu_demographics = orchestrator.patient.df[\n    orchestrator.patient.df['patient_id'].isin(icu_patients)\n]\n</code></pre>"},{"location":"examples/#analyzing-lab-trends","title":"Analyzing Lab Trends","text":"<pre><code># Get recent abnormal labs\nrecent_labs = orchestrator.labs.get_recent(hours=24)\nabnormal = recent_labs[\n    (recent_labs['lab_name'] == 'creatinine') &amp; \n    (recent_labs['lab_value'] &gt; 2.0)\n]\n\n# Track patient's lab trend\npatient_labs = orchestrator.labs.df[\n    orchestrator.labs.df['patient_id'] == 'P12345'\n].sort_values('lab_datetime')\n</code></pre>"},{"location":"examples/#medication-analysis","title":"Medication Analysis","text":"<pre><code># Find patients on multiple vasopressors\nvasopressors = orchestrator.medication_admin_continuous.filter_by_med_group('vasopressor')\nconcurrent = orchestrator.medication_admin_continuous.get_concurrent_medications('P12345')\nmulti_pressor = concurrent[concurrent['medication_group'] == 'vasopressor']\n</code></pre>"},{"location":"examples/#example-notebooks","title":"Example Notebooks","text":"<p>The repository includes Jupyter notebooks demonstrating: - <code>labs_demo.ipynb</code> - Laboratory data analysis - <code>respiratory_support_demo.ipynb</code> - Ventilation analysis - <code>position_demo.ipynb</code> - Prone positioning analysis</p>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Explore specific examples in detail</li> <li>Review the API documentation</li> <li>See the User Guide for comprehensive coverage</li> </ul>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the fundamental patterns for working with CLIFpy.</p>"},{"location":"getting-started/basic-usage/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/basic-usage/#table-classes","title":"Table Classes","text":"<p>Each CLIF table is represented by a Python class that inherits from <code>BaseTable</code>:</p> <ul> <li><code>Patient</code> - Demographics and patient identification</li> <li><code>Adt</code> - Admission, discharge, and transfer events</li> <li><code>Hospitalization</code> - Hospital stay information</li> <li><code>Labs</code> - Laboratory test results</li> <li><code>Vitals</code> - Vital signs measurements</li> <li><code>RespiratorySupport</code> - Ventilation and oxygen therapy</li> <li><code>MedicationAdminContinuous</code> - Continuous infusions</li> <li><code>MicrobiologyCulture</code> - Microbiology culture results</li> <li><code>PatientAssessments</code> - Clinical assessment scores</li> <li><code>Position</code> - Patient positioning</li> </ul>"},{"location":"getting-started/basic-usage/#data-loading","title":"Data Loading","text":"<p>All tables support two loading methods:</p> <pre><code># Method 1: From files\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',  # or 'csv'\n    timezone='US/Central'\n)\n\n# Method 2: From existing DataFrame\ntable = TableClass(\n    data=existing_dataframe,\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#validation","title":"Validation","text":"<p>Every table includes built-in validation:</p> <pre><code># Run validation\ntable.validate()\n\n# Check if valid\nif table.isvalid():\n    print(\"Validation passed!\")\nelse:\n    # Review errors\n    for error in table.errors[:5]:\n        print(f\"{error['type']}: {error['message']}\")\n</code></pre>"},{"location":"getting-started/basic-usage/#working-with-dataframes","title":"Working with DataFrames","text":"<p>All table data is accessible via the <code>df</code> attribute:</p> <pre><code># Access the underlying DataFrame\ndf = table.df\n\n# Use standard pandas operations\nprint(df.shape)\nprint(df.columns.tolist())\nprint(df.dtypes)\n\n# Filter data\nfiltered = df[df['some_column'] &gt; threshold]\n</code></pre>"},{"location":"getting-started/basic-usage/#common-operations","title":"Common Operations","text":""},{"location":"getting-started/basic-usage/#date-range-filtering","title":"Date Range Filtering","text":"<p>Most tables with datetime columns support date range filtering:</p> <pre><code>from datetime import datetime\n\n# Filter by date range\nstart = datetime(2023, 1, 1)\nend = datetime(2023, 12, 31)\n\n# For tables with custom methods\nfiltered = table.filter_by_date_range(start, end)\n\n# Or using pandas\nmask = (df['datetime_column'] &gt;= start) &amp; (df['datetime_column'] &lt;= end)\nfiltered = df[mask]\n</code></pre>"},{"location":"getting-started/basic-usage/#category-filtering","title":"Category Filtering","text":"<p>Tables with standardized categories provide filtering methods:</p> <pre><code># Labs by category\nchemistry = labs.filter_by_category('chemistry')\nhematology = labs.filter_by_category('hematology')\n\n# ADT by location\nicu_stays = adt.filter_by_location_category('icu')\ned_visits = adt.filter_by_location_category('ed')\n\n# Medications by group\nvasopressors = meds.filter_by_med_group('vasopressor')\nsedatives = meds.filter_by_med_group('sedative')\n</code></pre>"},{"location":"getting-started/basic-usage/#patient-specific-data","title":"Patient-specific Data","text":"<pre><code># Single patient\npatient_id = 'P12345'\npatient_labs = labs.df[labs.df['patient_id'] == patient_id]\n\n# Multiple patients\npatient_ids = ['P001', 'P002', 'P003']\ncohort_data = vitals.df[vitals.df['patient_id'].isin(patient_ids)]\n</code></pre>"},{"location":"getting-started/basic-usage/#output-and-reporting","title":"Output and Reporting","text":""},{"location":"getting-started/basic-usage/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Get table summary\nsummary = table.get_summary()\nprint(f\"Rows: {summary['num_rows']}\")\nprint(f\"Columns: {summary['num_columns']}\")\nprint(f\"Memory usage: {summary['memory_usage_mb']:.2f} MB\")\n\n# Save summary to file\ntable.save_summary()\n</code></pre>"},{"location":"getting-started/basic-usage/#validation-reports","title":"Validation Reports","text":"<p>Validation results are automatically saved to the output directory:</p> <pre><code># Set custom output directory\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    output_directory='/path/to/reports'\n)\n\n# After validation, check output files:\n# - validation_log_[table_name].log\n# - validation_errors_[table_name].csv\n# - missing_data_stats_[table_name].csv\n</code></pre>"},{"location":"getting-started/basic-usage/#timezone-handling","title":"Timezone Handling","text":"<p>CLIFpy ensures consistent timezone handling:</p> <pre><code># Specify timezone when loading\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'  # All datetime columns converted to this timezone\n)\n\n# Datetime columns are timezone-aware\nprint(table.df['datetime_column'].dt.tz)\n</code></pre>"},{"location":"getting-started/basic-usage/#memory-management","title":"Memory Management","text":"<p>For large datasets:</p> <pre><code># Load only specific columns\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    columns=['patient_id', 'datetime', 'value']\n)\n\n# Load a sample\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    sample_size=10000\n)\n\n# Apply filters during loading\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    filters={'patient_id': patient_list}\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    table = TableClass.from_file('/path/to/data', 'parquet')\n    table.validate()\n\n    if not table.isvalid():\n        # Handle validation errors\n        error_df = pd.DataFrame(table.errors)\n        error_df.to_csv('validation_errors.csv', index=False)\n\nexcept FileNotFoundError:\n    print(\"Data files not found\")\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the full User Guide</li> <li>Learn about the Orchestrator</li> <li>See table-specific guides</li> <li>View practical examples</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install CLIFpy and its dependencies.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install clifpy\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Clone the repository and install in development mode:</p> <pre><code># Clone the repository\ngit clone https://github.com/Common-Longitudinal-ICU-data-Format/CLIFpy.git\ncd CLIFpy\n\n# Create a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<p>To build the documentation locally:</p> <pre><code>pip install clifpy[docs]\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that CLIFpy is properly installed:</p> <pre><code>import clifpy\nprint(clifpy.__version__)\n</code></pre> <p>You should see the version number (e.g., <code>0.0.1</code>).</p>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>CLIFpy automatically installs the following dependencies:</p> <ul> <li>pandas: Data manipulation and analysis</li> <li>duckdb: SQL analytics engine</li> <li>pyarrow: Parquet file support</li> <li>pytz: Timezone handling</li> <li>matplotlib &amp; seaborn: Visualization (for demos)</li> <li>pytest: Testing framework</li> <li>tqdm: Progress bars</li> <li>marimo: Interactive notebooks</li> </ul>"},{"location":"getting-started/installation/#platform-support","title":"Platform Support","text":"<p>CLIFpy is tested on:</p> <ul> <li>Linux (Ubuntu 20.04+)</li> <li>macOS (10.15+)</li> <li>Windows (10+)</li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using the correct Python environment:</p> <pre><code>which python\npython --version\n</code></pre>"},{"location":"getting-started/installation/#permission-errors","title":"Permission Errors","text":"<p>On some systems, you may need to use <code>pip install --user</code>:</p> <pre><code>pip install --user clifpy\n</code></pre>"},{"location":"getting-started/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency conflicts, consider using a virtual environment:</p> <pre><code>python -m venv clifpy-env\nsource clifpy-env/bin/activate  # On Windows: clifpy-env\\Scripts\\activate\npip install clifpy\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quick Start guide</li> <li>Learn about basic usage</li> <li>Explore the User Guide</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will get you up and running with CLIFpy in just a few minutes.</p>"},{"location":"getting-started/quickstart/#loading-demo-data","title":"Loading Demo Data","text":"<p>CLIFpy includes demo data to help you get started:</p> <pre><code>from clifpy.data import load_dataset\n\n# Load all demo tables\ntables = load_dataset()\n\n# Access individual tables\npatient_df = tables['patient']\nlabs_df = tables['labs']\nvitals_df = tables['vitals']\n</code></pre>"},{"location":"getting-started/quickstart/#using-individual-tables","title":"Using Individual Tables","text":""},{"location":"getting-started/quickstart/#loading-a-single-table","title":"Loading a Single Table","text":"<pre><code>from clifpy.tables import Patient\n\n# Load patient data from files\npatient = Patient.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Validate the data\npatient.validate()\n\n# Check if data is valid\nif patient.isvalid():\n    print(\"Data validation passed!\")\nelse:\n    print(f\"Found {len(patient.errors)} validation errors\")\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-lab-data","title":"Working with Lab Data","text":"<pre><code>from clifpy.tables import Labs\n\n# Load lab data\nlabs = Labs.from_file('/path/to/data', 'parquet')\n\n# Get recent lab results\nrecent_labs = labs.get_recent(hours=24)\n\n# Filter by lab category\nchemistry_labs = labs.filter_by_category('chemistry')\n\n# Get common lab panels\ncbc = labs.get_common_labs('cbc')\nbmp = labs.get_common_labs('bmp')\n</code></pre>"},{"location":"getting-started/quickstart/#analyzing-vital-signs","title":"Analyzing Vital Signs","text":"<pre><code>from clifpy.tables import Vitals\n\n# Load vitals data\nvitals = Vitals.from_file('/path/to/data', 'parquet')\n\n# Get specific vital types\nheart_rates = vitals.filter_by_vital_type('heart_rate')\nblood_pressures = vitals.filter_by_vital_type('sbp')\n\n# Calculate summary statistics\nhr_stats = vitals.get_summary_by_vital_type()\nprint(hr_stats)\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-orchestrator","title":"Using the Orchestrator","text":"<p>For working with multiple tables at once:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize orchestrator\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Load multiple tables\norchestrator.initialize(\n    tables=['patient', 'labs', 'vitals', 'adt'],\n    sample_size=1000  # Optional: load sample for testing\n)\n\n# Validate all tables\norchestrator.validate_all()\n\n# Get summary of loaded tables\nloaded = orchestrator.get_loaded_tables()\nprint(f\"Loaded tables: {loaded}\")\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#filtering-by-patient","title":"Filtering by Patient","text":"<pre><code># Get data for specific patients\npatient_ids = ['P001', 'P002', 'P003']\n\n# Filter labs\npatient_labs = labs.df[labs.df['patient_id'].isin(patient_ids)]\n\n# Filter vitals\npatient_vitals = vitals.df[vitals.df['patient_id'].isin(patient_ids)]\n</code></pre>"},{"location":"getting-started/quickstart/#time-based-analysis","title":"Time-based Analysis","text":"<pre><code>from datetime import datetime, timedelta\n\n# Get data from the last 7 days\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=7)\n\n# Filter ADT movements\nrecent_movements = adt.filter_by_date_range(start_date, end_date)\n\n# Get ICU admissions\nicu_admissions = adt.filter_by_location_category('icu')\n</code></pre>"},{"location":"getting-started/quickstart/#clinical-calculations","title":"Clinical Calculations","text":"<pre><code># Calculate SOFA scores\nfrom clifpy.tables import PatientAssessments\n\nassessments = PatientAssessments.from_file('/path/to/data', 'parquet')\n\n# Get assessment trends\ngcs_trend = assessments.get_assessment_trend(\n    patient_id='P001',\n    assessment_category='neurological',\n    hours=48\n)\n\n# Check compliance\ncompliance = assessments.get_assessment_compliance(\n    assessment_category='pain',\n    expected_frequency_hours=4\n)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about basic usage patterns</li> <li>Explore the full User Guide</li> <li>See more examples</li> <li>Read the API documentation</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the CLIFpy User Guide. This guide provides comprehensive documentation for working with CLIF data using CLIFpy.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>CLIFpy is designed to make working with CLIF (Common Longitudinal ICU data Format) data straightforward and efficient. Whether you're a researcher analyzing ICU outcomes, a data scientist building predictive models, or a clinician exploring patient data, this guide will help you make the most of CLIFpy.</p>"},{"location":"user-guide/#guide-organization","title":"Guide Organization","text":""},{"location":"user-guide/#clif-orchestrator","title":"CLIF Orchestrator","text":"<p>Learn how to manage multiple CLIF tables simultaneously with consistent configuration and validation.</p>"},{"location":"user-guide/#wide-dataset-creation","title":"Wide Dataset Creation","text":"<p>Create comprehensive time-series datasets by joining multiple CLIF tables with automatic pivoting and high-performance processing.</p>"},{"location":"user-guide/#outlier-handling","title":"Outlier Handling","text":"<p>Detect and remove physiologically implausible values using configurable ranges and category-specific validation.</p>"},{"location":"user-guide/#comorbidity-index-computation","title":"Comorbidity Index Computation","text":"<p>Calculate Charlson and Elixhauser comorbidity indices from hospital diagnosis data for risk stratification and outcomes research.</p>"},{"location":"user-guide/#tables","title":"Tables","text":"<p>Detailed guides for each CLIF table type: </p> <ul> <li>Patient demographics</li> <li>ADT (Admission, Discharge, Transfer) events</li> <li>Hospitalization information</li> <li>Laboratory results</li> <li>Vital signs</li> <li>Respiratory support</li> <li>Medication administration</li> <li>Clinical assessments</li> <li>Patient positioning</li> </ul>"},{"location":"user-guide/#data-validation","title":"Data Validation","text":"<p>Understand how CLIFpy validates your data against CLIF schemas and how to interpret validation results.</p>"},{"location":"user-guide/#working-with-timezones","title":"Working with Timezones","text":"<p>Learn best practices for handling timezone-aware datetime data across different hospital systems.</p>"},{"location":"user-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"user-guide/#table-based-architecture","title":"Table-Based Architecture","text":"<p>CLIFpy organizes ICU data into standardized tables, each representing a specific aspect of patient care:</p> <pre><code>from clifpy.tables import Patient, Labs, Vitals\n\n# Method 1: Direct parameters (traditional)\npatient = Patient.from_file('/data', 'parquet', timezone='US/Eastern')\nlabs = Labs.from_file('/data', 'parquet', timezone='US/Eastern')\n\n# Method 2: Using configuration file (recommended)\npatient = Patient.from_file(config_path='./clif_config.json')\nlabs = Labs.from_file(config_path='./clif_config.json')\n</code></pre>"},{"location":"user-guide/#consistent-interface","title":"Consistent Interface","text":"<p>All tables share common methods inherited from <code>BaseTable</code>: </p> <ul> <li><code>from_file()</code> - Load data from files</li> <li><code>validate()</code> - Run comprehensive validation</li> <li><code>isvalid()</code> - Check validation status</li> <li><code>get_summary()</code> - Get table statistics</li> </ul>"},{"location":"user-guide/#standardized-categories","title":"Standardized Categories","text":"<p>CLIF defines standardized categories for consistent data representation: - Lab categories: chemistry, hematology, coagulation, etc. - Location categories: icu, ward, ed, etc. - Medication groups: vasopressor, sedative, antibiotic, etc.</p>"},{"location":"user-guide/#timezone-awareness","title":"Timezone Awareness","text":"<p>All datetime columns are timezone-aware to handle data from different time zones correctly:</p> <pre><code># Specify timezone when loading\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"user-guide/#configuration-files","title":"Configuration Files","text":"<p>CLIFpy supports configuration files for easier data loading and consistent settings across projects. You can use a <code>clif_config.json</code> file to centralize your configuration:</p>"},{"location":"user-guide/#configuration-structure","title":"Configuration Structure","text":"<p>Create a <code>clif_config.json</code> file with the following structure:</p> <pre><code>{\n  \"data_directory\": \"/path/to/data\",\n  \"filetype\": \"parquet\",\n  \"timezone\": \"US/Eastern\", \n  \"output_directory\": \"/path/to/output\"  // optional\n}\n</code></pre>"},{"location":"user-guide/#using-configuration-files","title":"Using Configuration Files","text":"<p>Load tables using the config file:</p> <pre><code>from clifpy.tables import Patient, Labs, Vitals\n\n# Using configuration file\npatient = Patient.from_file(config_path='./clif_config.json')\nlabs = Labs.from_file(config_path='./clif_config.json')\n\n# Or with the orchestrator\nfrom clifpy.clif_orchestrator import ClifOrchestrator\norchestrator = ClifOrchestrator(config_path='./clif_config.json')\n</code></pre> <p>You can still override specific parameters when needed:</p> <pre><code># Use config but override timezone and add sampling\nvitals = Vitals.from_file(\n    config_path='./clif_config.json',\n    timezone='UTC',\n    sample_size=1000\n)\n</code></pre>"},{"location":"user-guide/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/#loading-and-validating-data","title":"Loading and Validating Data","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Method 1: Direct parameters\norchestrator = ClifOrchestrator('/data', 'parquet', 'US/Central')\n\n# Method 2: Using configuration file (recommended)\norchestrator = ClifOrchestrator(config_path='./clif_config.json')\n\n# Both methods work the same way after initialization\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Validate all tables\norchestrator.validate_all()\n\n# Check validation status\nfor table_name in orchestrator.get_loaded_tables():\n    table = getattr(orchestrator, table_name)\n    print(f\"{table_name}: {'Valid' if table.isvalid() else 'Invalid'}\")\n</code></pre>"},{"location":"user-guide/#filtering-and-analysis","title":"Filtering and Analysis","text":"<pre><code># Category-based filtering\nicu_stays = adt.filter_by_location_category('icu')\n\n# Patient cohort analysis \ncohort_ids = ['P001', 'P002', 'P003']\ncohort_vitals = vitals.df[vitals.df['hospitalization_id'].isin(cohort_ids)]\n</code></pre>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate data after loading to ensure compliance with CLIF standards</li> <li>Use configuration files for consistent settings across your project (create <code>clif_config.json</code>)</li> <li>Use appropriate timezones for your data source</li> <li>Filter early to reduce memory usage with large datasets</li> <li>Review validation errors to understand data quality issues</li> <li>Use the orchestrator when working with multiple related tables</li> </ol>"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Explore specific table guides</li> <li>Calculate comorbidity indices for risk stratification</li> <li>Learn about data validation</li> <li>See practical examples</li> <li>Review the API reference</li> </ul>"},{"location":"user-guide/comorbidity-index/","title":"Comorbidity Index Computation","text":"<p>CLIFpy provides comprehensive functionality for calculating comorbidity indices from hospital diagnosis data. These indices are essential tools in clinical research for quantifying patient complexity and adjusting for disease burden in outcomes studies.</p>"},{"location":"user-guide/comorbidity-index/#overview","title":"Overview","text":"<p>Comorbidity indices are standardized scoring systems that summarize the burden of concurrent diseases in hospitalized patients. CLIFpy implements two widely-used indices:</p> <ul> <li>Charlson Comorbidity Index (CCI) - 17 conditions with differential weighting</li> <li>Elixhauser Comorbidity Index - 31 conditions with van Walraven weights</li> </ul> <p>Both indices use ICD-10-CM diagnosis codes and implement hierarchy logic to prevent double-counting of related conditions.</p> <p>Important Usage Note</p> <p>Hospital diagnosis codes are finalized billing diagnosis codes for reimbursement. They are appropriate for calculating comorbidity scores but should not be used as input features for prediction models of inpatient events.</p>"},{"location":"user-guide/comorbidity-index/#charlson-comorbidity-index-cci","title":"Charlson Comorbidity Index (CCI)","text":"<p>The Charlson Comorbidity Index predicts 10-year mortality risk based on 17 comorbid conditions. CLIFpy implements the Quan et al. (2011) adaptation for ICD-10-CM codes.</p>"},{"location":"user-guide/comorbidity-index/#cci-conditions-and-weights","title":"CCI Conditions and Weights","text":"Condition Weight Example ICD-10-CM Codes Myocardial Infarction 1 I21, I22, I252 Congestive Heart Failure 1 I50, I099, I110 Peripheral Vascular Disease 1 I70, I71, I731 Cerebrovascular Disease 1 I60-I69, G45, G46 Dementia 1 F00-F03, G30, G311 Chronic Pulmonary Disease 1 J40-J47, J60-J67 Connective Tissue Disease 1 M05, M06, M32-M34 Peptic Ulcer Disease 1 K25-K28 Mild Liver Disease 1 K70-K77 Diabetes (uncomplicated) 1 E10-E14 Hemiplegia 2 G81, G82 Renal Disease 2 N18-N19, N052-N057 Diabetes with Complications 2 E10-E14 with complications Cancer 2 C00-C26, C30-C34, C37-C41 Moderate/Severe Liver Disease 3 I85, I864, I982, K704 Metastatic Solid Tumor 6 C77-C80 AIDS 6 B20-B22, B24"},{"location":"user-guide/comorbidity-index/#basic-cci-usage","title":"Basic CCI Usage","text":"<pre><code>from clifpy.tables.hospital_diagnosis import HospitalDiagnosis\nfrom clifpy.utils.comorbidity import calculate_cci\n\n# Load hospital diagnosis data\nhosp_dx = HospitalDiagnosis(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Eastern'\n)\n\n# Calculate CCI scores\ncci_results = calculate_cci(hosp_dx, hierarchy=True)\n\n# View results\nprint(cci_results.head())\nprint(f\"CCI score range: {cci_results['cci_score'].min()} - {cci_results['cci_score'].max()}\")\n</code></pre>"},{"location":"user-guide/comorbidity-index/#cci-output-format","title":"CCI Output Format","text":"<p>The function returns a pandas DataFrame with:</p> <ul> <li><code>hospitalization_id</code> (index) - Unique hospitalization identifier</li> <li>17 binary condition columns (0/1) - One for each CCI condition</li> <li><code>cci_score</code> - Weighted sum of present conditions</li> </ul> <pre><code># Example output structure\nhospitalization_id  myocardial_infarction  congestive_heart_failure  ...  cci_score\nHOSP_001           1                      0                         ...  3\nHOSP_002           0                      1                         ...  1\nHOSP_003           0                      0                         ...  0\n</code></pre>"},{"location":"user-guide/comorbidity-index/#elixhauser-comorbidity-index","title":"Elixhauser Comorbidity Index","text":"<p>The Elixhauser Index captures a broader range of comorbidities (31 conditions) and uses van Walraven weights for mortality prediction. It often provides better discrimination than CCI for in-hospital outcomes.</p>"},{"location":"user-guide/comorbidity-index/#elixhauser-conditions-and-van-walraven-weights","title":"Elixhauser Conditions and van Walraven Weights","text":"Condition Weight Description Congestive Heart Failure 7 Heart failure, cardiomyopathy Cardiac Arrhythmias 5 Atrial fibrillation, other arrhythmias Valvular Disease -1 Heart valve disorders Pulmonary Circulation 4 Pulmonary embolism, pulmonary hypertension Peripheral Vascular 2 Peripheral artery disease Hypertension (uncomplicated) 0 Essential hypertension Hypertension (complicated) 0 Hypertensive complications Paralysis 7 Paraplegia, hemiplegia Other Neurological 6 Parkinson's, epilepsy, other Chronic Pulmonary 3 COPD, asthma Diabetes (uncomplicated) 0 Diabetes without complications Diabetes (complicated) 0 Diabetes with complications Hypothyroidism 0 Thyroid disorders Renal Failure 5 Chronic kidney disease Liver Disease 11 Chronic liver disease Peptic Ulcer Disease 0 Peptic ulcers AIDS/HIV 0 HIV/AIDS Lymphoma 9 Lymphomas Metastatic Cancer 12 Metastatic solid tumors Solid Tumor 4 Non-metastatic cancer ... ... ... (31 total conditions)"},{"location":"user-guide/comorbidity-index/#basic-elixhauser-usage","title":"Basic Elixhauser Usage","text":"<pre><code>from clifpy.utils.comorbidity import calculate_elix\n\n# Calculate Elixhauser scores\nelix_results = calculate_elix(hosp_dx, hierarchy=True)\n\n# View results\nprint(elix_results.head())\nprint(f\"Elixhauser score range: {elix_results['elix_score'].min()} - {elix_results['elix_score'].max()}\")\n\n# Check condition prevalence\ncondition_prevalence = elix_results.iloc[:, :-1].mean().sort_values(ascending=False)\nprint(\"Most common conditions:\")\nprint(condition_prevalence.head(10))\n</code></pre>"},{"location":"user-guide/comorbidity-index/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/comorbidity-index/#working-with-different-input-types","title":"Working with Different Input Types","text":"<p>CLIFpy's comorbidity functions accept multiple input formats:</p> <pre><code>import pandas as pd\n\n# Option 1: HospitalDiagnosis object (recommended)\ncci_scores = calculate_cci(hosp_dx_table)\n\n# Option 2: pandas DataFrame\ndf = pd.DataFrame({\n    'hospitalization_id': ['H001', 'H001', 'H002'],\n    'diagnosis_code': ['I21.45', 'E10.1', 'K25.5'],\n    'diagnosis_code_format': ['ICD10CM', 'ICD10CM', 'ICD10CM']\n})\ncci_scores = calculate_cci(df)\n\n# Option 3: polars DataFrame\nimport polars as pl\npl_df = pl.from_pandas(df)\ncci_scores = calculate_cci(pl_df)\n</code></pre>"},{"location":"user-guide/comorbidity-index/#hierarchy-logic","title":"Hierarchy Logic","text":"<p>Both indices implement hierarchy logic (assign0) to prevent double-counting of related conditions:</p> <pre><code># With hierarchy (default, recommended)\ncci_with_hierarchy = calculate_cci(hosp_dx, hierarchy=True)\n\n# Without hierarchy (for research comparison)\ncci_without_hierarchy = calculate_cci(hosp_dx, hierarchy=False)\n\n# Compare the difference\nhierarchy_impact = cci_with_hierarchy['cci_score'] - cci_without_hierarchy['cci_score']\nprint(f\"Hierarchy reduces scores by: {hierarchy_impact.mean():.2f} points on average\")\n</code></pre>"},{"location":"user-guide/comorbidity-index/#hierarchy-rules","title":"Hierarchy Rules","text":"<p>CCI Hierarchies: - Severe liver disease supersedes mild liver disease - Diabetes with complications supersedes uncomplicated diabetes - Metastatic cancer supersedes local cancer</p> <p>Elixhauser Hierarchies: - Complicated hypertension supersedes uncomplicated hypertension - Complicated diabetes supersedes uncomplicated diabetes - Metastatic cancer supersedes solid tumor without metastasis</p>"},{"location":"user-guide/comorbidity-index/#data-requirements","title":"Data Requirements","text":""},{"location":"user-guide/comorbidity-index/#input-data-format","title":"Input Data Format","text":"<p>Your hospital diagnosis data must include these columns:</p> <pre><code>required_columns = [\n    'hospitalization_id',      # Unique hospitalization identifier\n    'diagnosis_code',          # ICD diagnosis code (e.g., \"I21.45\")\n    'diagnosis_code_format'    # Code format (must be \"ICD10CM\")\n]\n</code></pre>"},{"location":"user-guide/comorbidity-index/#icd-code-processing","title":"ICD Code Processing","text":"<ul> <li>Decimal handling: Codes like \"I21.45\" are automatically truncated to \"I21\" for mapping</li> <li>Format filtering: Only ICD10CM codes are processed; other formats are ignored</li> <li>Prefix matching: Uses prefix matching (e.g., \"I21\" matches all I21.x codes)</li> </ul>"},{"location":"user-guide/comorbidity-index/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/comorbidity-index/#large-datasets","title":"Large Datasets","text":"<p>For large datasets, consider these optimization strategies:</p> <pre><code># Process in chunks for memory efficiency\nchunk_size = 10000\nresults = []\n\nfor chunk in pd.read_parquet('large_file.parquet', chunksize=chunk_size):\n    chunk_results = calculate_cci(chunk)\n    results.append(chunk_results)\n\nfinal_results = pd.concat(results, ignore_index=True)\n</code></pre>"},{"location":"user-guide/comorbidity-index/#memory-usage","title":"Memory Usage","text":"<ul> <li>Polars backend: Both functions use polars internally for performance</li> <li>Memory efficient: Processes data without loading entire dataset into memory</li> <li>Progress tracking: Built-in progress bars for long-running calculations</li> </ul>"},{"location":"user-guide/comorbidity-index/#clinical-interpretation","title":"Clinical Interpretation","text":""},{"location":"user-guide/comorbidity-index/#cci-score-interpretation","title":"CCI Score Interpretation","text":"Score Range Mortality Risk Typical Patient Population 0 Low Healthy patients, minor procedures 1-2 Moderate Single comorbidity, routine surgery 3-4 High Multiple comorbidities, complex cases \u22655 Very High Severely ill, high-risk procedures"},{"location":"user-guide/comorbidity-index/#elixhauser-score-interpretation","title":"Elixhauser Score Interpretation","text":"<p>Elixhauser scores can be negative due to protective conditions (negative weights). Typical ranges:</p> <ul> <li>\u22640: Low complexity, some protective factors</li> <li>1-4: Moderate complexity</li> <li>5-15: High complexity</li> <li>&gt;15: Very high complexity, multiple severe conditions</li> </ul>"},{"location":"user-guide/comorbidity-index/#research-applications","title":"Research Applications","text":"<pre><code># Mortality risk stratification\ndef risk_category(score, index_type='cci'):\n    if index_type == 'cci':\n        if score == 0:\n            return 'Low'\n        elif score &lt;= 2:\n            return 'Moderate'\n        elif score &lt;= 4:\n            return 'High'\n        else:\n            return 'Very High'\n    # Add Elixhauser categorization as needed\n\n# Apply risk stratification\nresults['risk_category'] = results['cci_score'].apply(\n    lambda x: risk_category(x, 'cci')\n)\n\n# Analyze by risk category\nrisk_summary = results.groupby('risk_category').agg({\n    'cci_score': ['count', 'mean', 'std']\n})\n</code></pre>"},{"location":"user-guide/comorbidity-index/#configuration-and-customization","title":"Configuration and Customization","text":""},{"location":"user-guide/comorbidity-index/#yaml-configuration-files","title":"YAML Configuration Files","text":"<p>Comorbidity mappings are stored in YAML files:</p> <ul> <li><code>clifpy/data/comorbidity/cci.yaml</code> - CCI mappings and weights</li> <li><code>clifpy/data/comorbidity/elixhauser.yaml</code> - Elixhauser mappings and weights</li> </ul>"},{"location":"user-guide/comorbidity-index/#configuration-structure","title":"Configuration Structure","text":"<pre><code># Example CCI configuration structure\nname: \"Charlson Comorbidity Index\"\nversion: \"quan\"\nsupported_formats:\n  - ICD10CM\n\ndiagnosis_code_mappings:\n  ICD10CM:\n    myocardial_infarction:\n      codes: [\"I21\", \"I22\", \"I252\"]\n      description: \"History of definite or probable MI\"\n    # ... other conditions\n\nweights:\n  myocardial_infarction: 1\n  # ... other weights\n\nhierarchies:\n  - higher: \"diabetes_with_complications\"\n    lower: \"diabetes_uncomplicated\"\n  # ... other hierarchies\n</code></pre>"},{"location":"user-guide/comorbidity-index/#custom-mappings","title":"Custom Mappings","text":"<p>For research requiring custom mappings, you can modify the YAML files or create custom versions. Ensure proper validation and testing when using custom configurations.</p>"},{"location":"user-guide/comorbidity-index/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/comorbidity-index/#common-issues","title":"Common Issues","text":"<p>No ICD10CM codes found: <pre><code># Check your data format\nprint(hosp_dx.df['diagnosis_code_format'].value_counts())\n# Ensure codes are marked as 'ICD10CM'\n</code></pre></p> <p>Unexpected zero scores: <pre><code># Check for missing hospitalization_id\nmissing_ids = hosp_dx.df['hospitalization_id'].isna().sum()\nprint(f\"Missing hospitalization IDs: {missing_ids}\")\n\n# Verify diagnosis codes are properly formatted\nprint(hosp_dx.df['diagnosis_code'].head())\n</code></pre></p> <p>Memory errors with large datasets: <pre><code># Monitor memory usage\nimport psutil\nprint(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Consider processing in smaller chunks\n</code></pre></p>"},{"location":"user-guide/comorbidity-index/#validation","title":"Validation","text":"<p>Always validate your results:</p> <pre><code># Basic validation checks\nassert not cci_results['cci_score'].isna().any(), \"CCI scores contain NaN\"\nassert (cci_results['cci_score'] &gt;= 0).all(), \"CCI scores should be non-negative\"\nassert cci_results['hospitalization_id'].is_unique, \"Hospitalization IDs should be unique\"\n\n# Clinical validation\nmax_score = cci_results['cci_score'].max()\nprint(f\"Maximum CCI score: {max_score}\")\nif max_score &gt; 20:\n    print(\"Warning: Unusually high CCI scores detected\")\n</code></pre>"},{"location":"user-guide/comorbidity-index/#references","title":"References","text":"<ul> <li>Quan H, et al. Coding algorithms for defining comorbidities in ICD-9-CM and ICD-10 administrative data. Med Care. 2005;43(11):1130-9.</li> <li>van Walraven C, et al. A modification of the Elixhauser comorbidity measures into a point system for hospital death using administrative data. Med Care. 2009;47(6):626-33.</li> <li>Charlson ME, et al. A new method of classifying prognostic comorbidity in longitudinal studies. J Chronic Dis. 1987;40(5):373-83.</li> </ul>"},{"location":"user-guide/encounter-stitching/","title":"Encounter Stitching","text":"<p>The encounter stitching functionality identifies and groups hospitalizations that occur within a specified time window of each other, treating them as a single continuous encounter. This is particularly useful for handling cases where patients are discharged and quickly readmitted, such as transfers between the emergency department and inpatient units.</p>"},{"location":"user-guide/encounter-stitching/#overview","title":"Overview","text":"<p>In clinical data, what appears as separate hospitalizations may actually represent a single continuous episode of care. Common scenarios include:</p> <ul> <li>ED to inpatient transfers - Patient admitted through ED, then formally admitted to hospital</li> <li>Inter-facility transfers - Patient moved between hospitals within a health system</li> <li>Brief discharges - Patient discharged and readmitted within hours (e.g., for procedures)</li> <li>Administrative separations - Billing or administrative reasons create multiple records</li> </ul> <p>The encounter stitching algorithm links these related hospitalizations using a configurable time window (default: 6 hours) between discharge and subsequent admission.</p>"},{"location":"user-guide/encounter-stitching/#how-it-works","title":"How It Works","text":"<p>The stitching algorithm:</p> <ol> <li>Sorts hospitalizations by patient and admission time</li> <li>Calculates gaps between discharge and next admission for each patient</li> <li>Links encounters when the gap is less than the specified time window</li> <li>Assigns encounter blocks - a unique identifier grouping linked hospitalizations</li> <li>Updates tables in-place - adds <code>encounter_block</code> column to both hospitalization and ADT tables</li> </ol>"},{"location":"user-guide/encounter-stitching/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/encounter-stitching/#quick-start-with-automatic-stitching","title":"Quick Start with Automatic Stitching","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize orchestrator with stitching enabled\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,          # Enable automatic stitching\n    stitch_time_interval=6          # 6-hour window (default)\n)\n\n# Load tables - stitching happens automatically\nclif.initialize(['hospitalization', 'adt'])\n\n# Access the encounter mapping\nmapping = clif.get_encounter_mapping()\nprint(f\"Created {mapping['encounter_block'].nunique()} encounter blocks\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#custom-time-windows","title":"Custom Time Windows","text":"<pre><code># Use a 12-hour window for linking encounters\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,\n    stitch_time_interval=12  # 12-hour window\n)\n\n# Use a 2-hour window for stricter linking\nclif_strict = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,\n    stitch_time_interval=2   # 2-hour window\n)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#direct-function-usage","title":"Direct Function Usage","text":"<p>You can also use the stitching function directly without the orchestrator:</p> <pre><code>from clifpy.utils.stitching_encounters import stitch_encounters\n\n# Load your dataframes\nhospitalization_df = pd.read_parquet('hospitalization.parquet')\nadt_df = pd.read_parquet('adt.parquet')\n\n# Perform stitching\nhosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n    hospitalization=hospitalization_df,\n    adt=adt_df,\n    time_interval=12  # 12-hour window\n)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#parameters","title":"Parameters","text":""},{"location":"user-guide/encounter-stitching/#cliforchestrator-parameters","title":"ClifOrchestrator Parameters","text":"Parameter Type Default Description <code>stitch_encounter</code> bool False Enable automatic encounter stitching during initialization <code>stitch_time_interval</code> int 6 Hours between discharge and next admission to consider encounters linked"},{"location":"user-guide/encounter-stitching/#direct-function-parameters","title":"Direct Function Parameters","text":"Parameter Type Default Description <code>hospitalization</code> pd.DataFrame Required Hospitalization table with required columns <code>adt</code> pd.DataFrame Required ADT table with required columns <code>time_interval</code> int 6 Hours between discharge and next admission to consider encounters linked"},{"location":"user-guide/encounter-stitching/#required-data-columns","title":"Required Data Columns","text":""},{"location":"user-guide/encounter-stitching/#hospitalization-table","title":"Hospitalization Table","text":"<ul> <li><code>patient_id</code></li> <li><code>hospitalization_id</code></li> <li><code>admission_dttm</code></li> <li><code>discharge_dttm</code></li> <li><code>age_at_admission</code></li> <li><code>admission_type_category</code></li> <li><code>discharge_category</code></li> </ul>"},{"location":"user-guide/encounter-stitching/#adt-table","title":"ADT Table","text":"<ul> <li><code>hospitalization_id</code></li> <li><code>in_dttm</code></li> <li><code>out_dttm</code></li> <li><code>location_category</code></li> <li><code>hospital_id</code></li> </ul>"},{"location":"user-guide/encounter-stitching/#output","title":"Output","text":"<p>When stitching is enabled, the process:</p> <ol> <li>Updates hospitalization table - Adds <code>encounter_block</code> column</li> <li>Updates ADT table - Adds <code>encounter_block</code> column</li> <li>Creates encounter mapping - Available via <code>clif.get_encounter_mapping()</code>:</li> <li><code>hospitalization_id</code>: Original hospitalization identifier</li> <li><code>encounter_block</code>: Assigned encounter block number</li> </ol>"},{"location":"user-guide/encounter-stitching/#understanding-encounter-blocks","title":"Understanding Encounter Blocks","text":"<p>Each encounter block represents a continuous episode of care:</p> <pre><code># Access the mapping after initialization\nmapping = clif.get_encounter_mapping()\n\n# Find multi-hospitalization encounters\nmulti_hosp = mapping.groupby('encounter_block').size()\nmulti_hosp_encounters = multi_hosp[multi_hosp &gt; 1]\n\nprint(f\"Encounters with multiple hospitalizations: {len(multi_hosp_encounters)}\")\n\n# Get details for a specific encounter block\nblock_1_hosps = mapping[mapping['encounter_block'] == 1]\nprint(f\"Hospitalizations in encounter block 1: {block_1_hosps['hospitalization_id'].tolist()}\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#practical-examples","title":"Practical Examples","text":""},{"location":"user-guide/encounter-stitching/#calculate-true-length-of-stay","title":"Calculate True Length of Stay","text":"<p>When encounters are stitched, you can calculate the true length of stay across linked hospitalizations:</p> <pre><code># Access stitched hospitalization data\nstitched_df = clif.hospitalization.df\n\n# Calculate encounter-level statistics\nencounter_stats = stitched_df.groupby('encounter_block').agg({\n    'admission_dttm': 'min',  # First admission\n    'discharge_dttm': 'max',  # Last discharge\n    'hospitalization_id': 'count',  # Number of linked hospitalizations\n    'patient_id': 'first'\n})\n\n# Calculate total length of stay\nencounter_stats['total_los_days'] = (\n    (encounter_stats['discharge_dttm'] - encounter_stats['admission_dttm'])\n    .dt.total_seconds() / 86400\n)\n\nprint(encounter_stats[['patient_id', 'hospitalization_id', 'total_los_days']].head())\n</code></pre>"},{"location":"user-guide/encounter-stitching/#analyze-icu-stays-across-encounters","title":"Analyze ICU Stays Across Encounters","text":"<pre><code># Access stitched ADT data\nadt_stitched_df = clif.adt.df\n\n# Find ICU stays by encounter\nicu_by_encounter = adt_stitched_df[\n    adt_stitched_df['location_category'] == 'icu'\n].groupby('encounter_block').agg({\n    'in_dttm': 'min',\n    'out_dttm': 'max',\n    'hospitalization_id': 'nunique'\n})\n\nprint(\"ICU stays by encounter block:\")\nprint(icu_by_encounter.head())\n</code></pre>"},{"location":"user-guide/encounter-stitching/#filter-data-by-encounter-properties","title":"Filter Data by Encounter Properties","text":"<pre><code># Find encounters with ED to inpatient transfers\ned_admits = clif.adt.df[\n    clif.adt.df['location_category'] == 'ed'\n]['encounter_block'].unique()\n\ninpatient_admits = clif.adt.df[\n    clif.adt.df['location_category'].isin(['icu', 'ward'])\n]['encounter_block'].unique()\n\ned_to_inpatient = set(ed_admits) &amp; set(inpatient_admits)\nprint(f\"Encounters with ED to inpatient transfer: {len(ed_to_inpatient)}\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#compare-different-time-windows","title":"Compare Different Time Windows","text":"<pre><code># Test effect of different time windows\nwindows = [3, 6, 12, 24]\nresults = []\n\nfor window in windows:\n    clif_test = ClifOrchestrator(\n        data_directory='/path/to/data',\n        filetype='parquet',\n        timezone='UTC',\n        stitch_encounter=True,\n        stitch_time_interval=window\n    )\n    clif_test.initialize(['hospitalization', 'adt'])\n\n    mapping = clif_test.get_encounter_mapping()\n    results.append({\n        'window_hours': window,\n        'total_encounters': mapping['encounter_block'].nunique(),\n        'multi_hosp_encounters': (mapping.groupby('encounter_block').size() &gt; 1).sum()\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"user-guide/encounter-stitching/#wide-dataset-creation","title":"Wide Dataset Creation","text":"<p>Stitched encounters are automatically used when creating wide datasets:</p> <pre><code># Initialize with stitching\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True\n)\n\n# Load tables (stitching happens automatically)\nclif.initialize(['hospitalization', 'adt', 'labs', 'vitals'])\n\n# Create wide dataset using stitched encounters\nwide_df = clif.create_wide_dataset(\n    start_time='admission_dttm',\n    end_time='discharge_dttm',\n    time_col='charttime'\n)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#validation","title":"Validation","text":"<p>The stitched tables maintain compatibility with validation methods:</p> <pre><code># Validate all loaded tables (including stitched ones)\nvalidation_results = clif.validate_all()\n\n# Check specific tables\nclif.hospitalization.validate()\nclif.adt.validate()\n</code></pre>"},{"location":"user-guide/encounter-stitching/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate time windows:</li> <li>2-4 hours: Strict linking for direct transfers only</li> <li>6 hours (default): Balances capturing related encounters while avoiding over-grouping</li> <li> <p>12-24 hours: Liberal definition, captures day surgery readmissions</p> </li> <li> <p>Validate stitching results:    <pre><code># Check for suspiciously large encounter blocks\nmapping = clif.get_encounter_mapping()\nhosp_counts = mapping.groupby('encounter_block').size()\nsuspicious = hosp_counts[hosp_counts &gt; 5]\nif len(suspicious) &gt; 0:\n    print(f\"Review encounters with &gt;5 hospitalizations: {suspicious.index.tolist()}\")\n</code></pre></p> </li> <li> <p>Consider your analysis goals:</p> </li> <li>Outcome studies: Use stitched encounters to avoid counting transfers as readmissions</li> <li>Resource utilization: May want to keep encounters separate for accurate billing</li> <li> <p>Quality metrics: Check if measure specifications require episode-based analysis</p> </li> <li> <p>Document your choices:    <pre><code># Save stitching parameters for reproducibility\nif clif.encounter_mapping is not None:\n    stitching_info = {\n        'time_interval_hours': clif.stitch_time_interval,\n        'timestamp': pd.Timestamp.now(),\n        'num_encounters_created': clif.encounter_mapping['encounter_block'].nunique(),\n        'num_multi_hosp_encounters': (\n            clif.encounter_mapping.groupby('encounter_block').size().gt(1).sum()\n        )\n    }\n    # Save to file or include in analysis metadata\n</code></pre></p> </li> </ol>"},{"location":"user-guide/encounter-stitching/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/encounter-stitching/#algorithm-implementation","title":"Algorithm Implementation","text":"<p>The stitching algorithm:</p> <ol> <li>Filters required columns from hospitalization and ADT tables</li> <li>Joins hospitalization and ADT data</li> <li>Sorts by patient_id and admission_dttm</li> <li>Calculates hours between discharge and next admission</li> <li>Creates linked flag for gaps &lt; time_interval</li> <li>Iteratively propagates encounter_block IDs through linked chains</li> <li>Updates original dataframes with encounter_block column</li> </ol>"},{"location":"user-guide/encounter-stitching/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Stitching is performed in-memory using pandas operations</li> <li>Performance scales linearly with number of hospitalizations</li> <li>For datasets with &gt;1M hospitalizations, ensure adequate RAM (8GB+ recommended)</li> <li>Processing time is typically seconds to minutes depending on data size</li> </ul>"},{"location":"user-guide/encounter-stitching/#error-handling","title":"Error Handling","text":"<p>The orchestrator handles common issues:</p> <ul> <li>Missing tables: Warns if hospitalization or ADT tables are not loaded</li> <li>Missing columns: Raises ValueError with specific missing columns listed</li> <li>Processing errors: Catches exceptions and reports them without failing initialization</li> </ul>"},{"location":"user-guide/encounter-stitching/#limitations","title":"Limitations","text":"<ul> <li>Currently only links hospitalizations for the same patient</li> <li>Does not consider clinical criteria (purely time-based)</li> <li>Requires both hospitalization and ADT tables to be present</li> <li>Does not link across different hospital systems (requires same patient_id)</li> </ul>"},{"location":"user-guide/encounter-stitching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/encounter-stitching/#common-issues","title":"Common Issues","text":"<p>Issue: \"Encounter stitching requires both hospitalization and ADT tables to be loaded\" - Solution: Include both 'hospitalization' and 'adt' in your <code>initialize()</code> call</p> <p>Issue: \"Missing required columns in hospitalization DataFrame\" - Solution: Ensure your data contains all required columns listed above - Check: Use <code>clif.hospitalization.df.columns</code> to see available columns</p> <p>Issue: No encounters are being stitched despite close admissions - Check: Verify datetime columns are properly parsed and in the same timezone - Check: Ensure discharge_dttm is not null for hospitalizations you expect to link - Try: Increase the time window to see if encounters get linked</p>"},{"location":"user-guide/encounter-stitching/#debugging","title":"Debugging","text":"<pre><code># Enable detailed output during initialization\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,\n    stitch_time_interval=6\n)\n\n# Check if stitching was attempted\nclif.initialize(['hospitalization', 'adt'])\n\n# Verify encounter_block was added\nprint(\"Hospitalization columns:\", clif.hospitalization.df.columns.tolist())\nprint(\"Has encounter_block:\", 'encounter_block' in clif.hospitalization.df.columns)\n\n# Check mapping\nif clif.encounter_mapping is not None:\n    print(f\"Mapping shape: {clif.encounter_mapping.shape}\")\nelse:\n    print(\"No encounter mapping created\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#see-also","title":"See Also","text":"<ul> <li>ClifOrchestrator - Main interface for CLIF data operations</li> <li>Hospitalization Table - Structure of hospitalization data</li> <li>ADT Table - Structure of ADT data</li> <li>Wide Dataset Creation - Creating analysis-ready datasets</li> <li>Examples Notebook - Interactive examples</li> </ul>"},{"location":"user-guide/med-unit-conversion/","title":"Medication Unit Conversion","text":"<p>CLIFpy provides robust medication dose unit conversion functionality to standardize medication dosing across different unit systems. This is essential for clinical data analysis where medications may be recorded in various units across different systems.</p>"},{"location":"user-guide/med-unit-conversion/#standardize-dose-units-by-medication","title":"Standardize dose units by medication","text":"<p>In the most common use cases, we want to standardize dose units by medication and pattern of administration -- all propofol doses to be presented in mcg/kg/min in the continuous table and in mcg in the intermittent table, for example.</p> <p>To achieve this, simply call one of the two <code>convert_dose_units_*</code> functions (one for continuous and one for intermittent) from the CLIF orchestrator and provide a dictionary mapping of medication categories to their preferred units:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\nco = ClifOrchestrator(config_path=\"config/config.yaml\")\n\npreferred_units_cont = {\n    \"propofol\": \"mcg/min\",\n    \"fentanyl\": \"mcg/hr\",\n    \"insulin\": \"u/hr\",\n    \"midazolam\": \"mg/hr\",\n    \"heparin\": \"u/min\"\n}\n\nco.convert_dose_units_for_continuous_meds(preferred_units=preferred_units_cont)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#returns","title":"Returns","text":"<p>Under the hood, this function automatically loads and uses the medication and vitals tables to generate two dataframes that are saved to the corresponding medication table instance by default:</p> <ol> <li><code>co.medication_admin_continuous.df_converted</code> gives the updated medication table with the new columns appended:</li> <li><code>weight_kg</code>: the most recent weight relative to the <code>admin_dttm</code> pulled from the <code>vitals</code> table.</li> <li><code>_clean_unit</code>: cleaned source unit string where both 'U/h' and 'units / hour' would be standardized to 'u/hr', for example.</li> <li><code>_unit_class</code>: distinguishes where the source unit is an amount (e.g. 'mcg'), a 'rate' (e.g. 'mcg/hr'), or 'unrecognized'.</li> <li><code>_convert_status</code>: documents whether the conversion is a \"success\" or, in the case of failure, the reason for failure, e.g. 'cannot convert amount to rate' for rows of propofol in 'mcg' that the users want to convert to 'mcg/kg/min'.</li> <li><code>med_dose_converted</code>, <code>med_dose_unit_converted</code>: the converted results if the <code>_convert_status</code> is 'success', or fall back to the original <code>med_dose</code> and <code>_clean_unit</code> if failure.</li> </ol> <p>Note: the following demo output omits some rows and columns for display purposes</p> <ol> <li><code>co.medication_admin_continuous.conversion_counts</code> shows an aggregated summary of which source units of which <code>med_category</code> are converted to which preferred units -- and their frequency counts. A useful quality check would be to filter for all the <code>_convert_status</code> that are not 'success.'</li> </ol> <p>To access the results directly instead of from the table instance, turn off the <code>save_to_table</code> argument:</p> <pre><code>cont_converted, cont_counts = co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    save_to_table=False\n)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#override-option","title":"Override option","text":"<p>The function automatically parses whether the provided <code>med_categories</code> and preferred units in the dictionary are acceptable and return errors or warnings when they are not. To override any code-breaking error such as an unidentified <code>med_category</code> or preferred unit string, turn on the arg <code>override=True</code>:</p> <pre><code>co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    override=True\n)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#acceptable-unit-formatting","title":"Acceptable unit formatting","text":"<p>The unit strings in <code>preferred_units</code> dictionary need to be formatted a certain way for them to be accepted. (The original source unit strings in <code>med_dose_unit</code> do not face such restrictions. Both 'mL' and 'milliliter' in <code>med_dose_unit</code> can be correctly parsed as 'ml', for example.)</p> <p>For a list of acceptable preferred units:</p> <ul> <li>amount:</li> <li>mass: <code>mcg</code>, <code>mg</code>, <code>ng</code>, <code>g</code></li> <li>volume: <code>ml</code>, <code>l</code></li> <li> <p>unit: <code>mu</code>, <code>u</code></p> </li> <li> <p>weight: <code>/kg</code>, <code>/lb</code></p> </li> <li> <p>time: <code>/hr</code>, <code>/min</code></p> </li> <li> <p>rate: a combination of amount, weight, and time, e.g. 'mcg/kg/min', 'u/hr'.</p> </li> <li>the unit can be either weight-adjusted or not -- that is, both 'mcg/kg/min' and 'mcg/min' are acceptable. When no weight is available from the <code>vitals</code> table to enable conversion between weight-adjusted and weight-less units, an error will be returned.</li> </ul> <p>All strings should be in lower case with no whitespaces in between.</p>"},{"location":"user-guide/med-unit-conversion/#standardize-to-base-units-across-medications","title":"Standardize to base units across medications","text":"<p>In rarer cases, one might prefer all applicable units of the same class be collapsed onto the same scale across medications, e.g. both 'mcg/kg/min' and 'mg/hour' would be converted to the same 'mcg/min' -- referred to here as the \"base unit\" -- across all medications applicable.</p> <p>To enable this, turn on the <code>show_intermediate=True</code> argument:</p> <pre><code>cont_converted_detailed, _ = co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    save_to_table=False,\n    show_intermediate=True\n)\n</code></pre> <p>This would append a series of additional columns that were the intermediate results generated during the conversion, including the <code>_base_dose</code> and <code>_base_unit</code>.</p> <p>The set of base units are:</p> <ul> <li>amount: <code>mcg</code>, <code>ml</code>, <code>u</code></li> <li>time: <code>/min</code></li> <li>rate: a combination of amount and time, e.g. <code>mcg/min</code>, <code>u/min</code>.</li> <li>Note that all base units would be weight-less.</li> </ul>"},{"location":"user-guide/med-unit-conversion/#unit-classification-system","title":"Unit Classification System","text":""},{"location":"user-guide/med-unit-conversion/#unit-classes","title":"Unit Classes","text":"<ul> <li><code>rate</code>: Dose per time units (e.g., mcg/min, ml/hr, u/kg/hr)</li> <li><code>amount</code>: Total dose units (e.g., mcg, ml, u)</li> <li><code>unrecognized</code>: Units that cannot be parsed or converted</li> </ul>"},{"location":"user-guide/med-unit-conversion/#unit-subclasses","title":"Unit Subclasses","text":"<ul> <li><code>mass</code>: Weight-based units (mcg, mg, ng, g)</li> <li><code>volume</code>: Volume-based units (ml, l)</li> <li><code>unit</code>: Unit-based dosing (u, mu)</li> <li><code>unrecognized</code>: Units that don't fit standard categories</li> </ul> <p>Unit class and subclass compatibility determines whether conversions are allowed. For example:</p> <ul> <li> <p>\u2705 <code>rate</code> \u2192 <code>rate</code> (same class)</p> </li> <li> <p>\u2705 <code>mass</code> \u2192 <code>mass</code> (same subclass)</p> </li> <li> <p>\u274c <code>rate</code> \u2192 <code>amount</code> (different class)</p> </li> <li> <p>\u274c <code>mass</code> \u2192 <code>volume</code> (different subclass)</p> </li> </ul>"},{"location":"user-guide/med-unit-conversion/#reference-table","title":"Reference Table","text":"unit class unit subclass _clean_unit acceptable source <code>med_dose_unit</code> examples _base_unit Amount Units amount mass mcg MCG, \u00b5g, \u03bcg, ug mcg amount mass mg MG, milligram mcg amount mass ng NG, nanogram mcg amount mass g G, gram, grams mcg amount volume ml mL, milliliter, milliliters ml amount volume l L, liter, liters, litre, litres ml amount unit u U, unit, units u amount unit mu MU, milliunit, milliunits, milli-unit, milli-units u Rate Units rate mass mcg/min MCG/MIN, \u00b5g/min, \u03bcg/min, mcg/minute, micrograms/minute mcg/min rate mass mcg/hr MCG/HR, \u00b5g/hr, \u03bcg/hr, mcg/hour, micrograms/hour mcg/min rate mass mcg/kg/min MCG/KG/MIN, \u00b5g/kg/min, mcg/kg/minute mcg/min rate mass mcg/kg/hr MCG/KG/HR, \u00b5g/kg/hr, mcg/kg/hour mcg/min rate mass mcg/lb/min MCG/LB/MIN, \u00b5g/lb/min, mcg/lb/minute mcg/min rate mass mcg/lb/hr MCG/LB/HR, \u00b5g/lb/hr, mcg/lb/hour mcg/min rate volume ml/min mL/min, ml/m, milliliter/minute ml/min rate volume ml/hr mL/hr, ml/h, milliliter/hour, milliliters/hour, millilitres/hour ml/min rate volume ml/kg/min mL/kg/min, milliliter/kg/minute ml/min rate volume ml/kg/hr mL/kg/hr, milliliter/kg/hour ml/min rate volume ml/lb/min mL/lb/min, milliliter/lb/minute ml/min rate volume ml/lb/hr mL/lb/hr, milliliter/lb/hour ml/min rate unit u/min U/min, units/minute, unit/minute u/min rate unit u/hr U/hr, units/hour, unit/hour u/min rate unit u/kg/min U/kg/min, units/kg/minute u/min rate unit u/kg/hr U/kg/hr, u/kg/h, units/kg/hour u/min rate unit u/lb/min U/lb/min, units/lb/minute u/min rate unit u/lb/hr U/lb/hr, units/lb/hour, unit/lb/hr u/min"},{"location":"user-guide/med-unit-conversion/#important-notes","title":"Important Notes","text":"<ul> <li>_clean_unit: The exact format you must use when specifying preferred units</li> <li>Acceptable Variations: Raw <code>med_dose_unit</code> strings in your original DataFrame that the converter can detect and clean (these are NOT acceptable formats for preferred units)</li> <li>_base_unit: The standardized unit all conversions target (mcg/min, ml/min, u/min for rates; mcg, ml, u for amounts)</li> </ul>"},{"location":"user-guide/med-unit-conversion/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/med-unit-conversion/#the-_convert_status-column","title":"The <code>_convert_status</code> Column","text":"<p>After conversion, each record includes a <code>_convert_status</code> field indicating the outcome:</p> <ul> <li><code>success</code>: Conversion completed successfully</li> <li><code>original unit is missing</code>: No unit provided in source data</li> <li><code>original unit [unit] is not recognized</code>: Input unit cannot be parsed</li> <li><code>user-preferred unit [unit] is not recognized</code>: Target unit is invalid</li> <li><code>cannot convert [class1] to [class2]</code>: Incompatible unit classes (e.g., rate \u2192 amount)</li> <li><code>cannot convert [subclass1] to [subclass2]</code>: Incompatible unit subclasses (e.g., mass \u2192 volume)</li> <li><code>cannot convert to a weighted unit if weight_kg is missing</code>: Weight-based conversion attempted without patient weight</li> </ul>"},{"location":"user-guide/med-unit-conversion/#failure-handling","title":"Failure Handling","text":"<p>When conversion fails: - <code>med_dose_converted</code> = original <code>_base_dose</code> (or original dose if base conversion failed) - <code>med_dose_unit_converted</code> = <code>_clean_unit</code> (or original unit if cleaning failed)</p>"},{"location":"user-guide/med-unit-conversion/#alternative-direct-unit-converter-usage","title":"Alternative: Direct Unit Converter Usage","text":"<p>For advanced users who need more control or want to use the unit converter directly without the ClifOrchestrator:</p>"},{"location":"user-guide/med-unit-conversion/#primary-function-convert_dose_units_by_med_category","title":"Primary Function: <code>convert_dose_units_by_med_category()</code>","text":"<pre><code>from clifpy.utils.unit_converter import convert_dose_units_by_med_category\nimport pandas as pd\n\n# Load your medication data\nmed_df = pd.read_parquet('clifpy/data/clif_demo/clif_medication_admin_continuous.parquet')\n\n# Define preferred units for each medication\npreferred_units = {\n    'propofol': 'mcg/kg/min',\n    'fentanyl': 'mcg/hr',\n    'insulin': 'u/hr',\n    'midazolam': 'mg/hr'\n}\n\n# Convert units\nconverted_df, summary_df = convert_dose_units_by_med_category(\n    med_df=med_df,\n    preferred_units=preferred_units,\n    override=False\n)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#secondary-function-standardize_dose_to_base_units","title":"Secondary Function: <code>standardize_dose_to_base_units()</code>","text":"<p>This function is for advanced users who need to standardize all units to a base set without medication-specific preferences.</p> <pre><code>from clifpy.utils.unit_converter import standardize_dose_to_base_units\n\n# Standardize to base units only\nbase_df, counts_df = standardize_dose_to_base_units(med_df)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#best-practices","title":"Best Practices","text":"<ol> <li>Check conversion status after processing to identify failed conversions</li> <li>Use exact _clean_unit formats when specifying preferred units</li> <li>Review the conversion counts summary DataFrame to understand conversion patterns and identify data quality issues</li> <li>Test with override=True first to see all potential issues before requiring strict validation</li> <li>Validate your preferred_units dictionary against the acceptable units table above</li> </ol>"},{"location":"user-guide/med-unit-conversion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/med-unit-conversion/#common-issues","title":"Common Issues","text":"<p>Issue: \"Cannot convert rate to amount\"     - Solution: Ensure unit classes match (rate\u2192rate, amount\u2192amount)</p> <p>Issue: \"Cannot convert mass to volume\"     - Solution: Ensure unit subclasses match (mass\u2192mass, volume\u2192volume)</p> <p>Issue: \"User-preferred unit [unit] is not recognized\"     - Solution: Use exact <code>_clean_unit</code> format from the reference table above</p> <p>Issue: Weight-based conversions failing     - Solution: Ensure <code>weight_kg</code> column exists in your DataFrame or is available in vitals data</p> <p>Issue: \"Cannot convert to a weighted unit if weight_kg is missing\"     - Solution: Provide patient weights in the vitals table or med_df</p>"},{"location":"user-guide/med-unit-conversion/#getting-help","title":"Getting Help","text":"<p>If you encounter units not in the reference table or unexpected conversion failures:</p> <ol> <li>Check the <code>_convert_status</code> column for specific error messages</li> <li>Review the summary DataFrame for patterns in failed conversions</li> <li>Use <code>override=True</code> to see warnings instead of stopping on errors</li> <li>Consult the API reference for detailed function documentation</li> </ol>"},{"location":"user-guide/med-unit-conversion/#example-analysis-workflow","title":"Example Analysis Workflow","text":"<pre><code># 1. Basic conversion\nconverted_df, summary_df = co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    save_to_table=False\n)\n\n# 2. Check conversion success\nprint(f\"Total records: {len(converted_df)}\")\nprint(f\"Successful conversions: {(converted_df['_convert_status'] == 'success').sum()}\")\n\n# 3. Analyze conversion patterns\nsummary_analysis = summary_df.groupby(['med_category', '_convert_status'])['count'].sum()\nprint(\"Conversion summary by medication:\")\nprint(summary_analysis)\n\n# 4. Check for problematic units\nproblematic_units = summary_df[summary_df['_convert_status'] != 'success']\nprint(\"\\\\nUnits requiring attention:\")\nprint(problematic_units[['med_dose_unit', '_convert_status', 'count']])\n</code></pre>"},{"location":"user-guide/orchestrator/","title":"CLIF Orchestrator","text":"<p>The <code>ClifOrchestrator</code> class provides a centralized interface for managing multiple CLIF tables with consistent configuration. This guide covers how to use the orchestrator effectively.</p>"},{"location":"user-guide/orchestrator/#overview","title":"Overview","text":"<p>The orchestrator simplifies working with multiple CLIF tables by:</p> <ul> <li>Ensuring consistent configuration across all tables</li> <li>Providing bulk operations (load, validate)</li> <li>Managing shared settings (timezone, file format, output directory)</li> <li>Offering a unified interface for multi-table workflows</li> </ul>"},{"location":"user-guide/orchestrator/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/orchestrator/#initialization","title":"Initialization","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Create orchestrator with your data configuration\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/clif/data',\n    filetype='parquet',  # or 'csv'\n    timezone='US/Central',\n    output_directory='/path/to/outputs'  # Optional\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#loading-tables","title":"Loading Tables","text":"<pre><code># Load specific tables\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Load all available tables\nall_tables = ['patient', 'hospitalization', 'adt', 'labs', 'vitals',\n              'medication_admin_continuous', 'patient_assessments',\n              'respiratory_support', 'position']\norchestrator.initialize(tables=all_tables)\n\n# Load with sampling (useful for testing)\norchestrator.initialize(\n    tables=['patient', 'labs'],\n    sample_size=1000\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#accessing-tables","title":"Accessing Tables","text":"<p>Once loaded, tables are available as attributes:</p> <pre><code># Access individual tables\npatient_data = orchestrator.patient\nlabs_data = orchestrator.labs\nvitals_data = orchestrator.vitals\n\n# Get the underlying DataFrames\npatient_df = orchestrator.patient.df\nlabs_df = orchestrator.labs.df\n</code></pre>"},{"location":"user-guide/orchestrator/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/orchestrator/#selective-column-loading","title":"Selective Column Loading","text":"<p>Load only specific columns to reduce memory usage:</p> <pre><code>orchestrator.initialize(\n    tables=['labs', 'vitals'],\n    columns={\n        'labs': ['hospitalization_id', 'lab_result_dttm', 'lab_value', 'lab_name'],\n        'vitals': ['hospitalization_id', 'recorded_dttm', 'vital_value']\n    }\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#filtered-loading","title":"Filtered Loading","text":"<p>Apply filters during loading:</p> <pre><code># Filter by patient IDs\nhospitalization_ids = ['P001', 'P002', 'P003']\norchestrator.initialize(\n    tables=['labs', 'vitals'],\n    filters={\n        'labs': {'hospitalization_id': patient_ids},\n        'vitals': {'hospitalization_id': patient_ids}\n    }\n)\n\n# Filter by categories\norchestrator.initialize(\n    tables=['labs', 'adt'],\n    filters={\n        'labs': {'lab_category': 'lactate'},\n        'adt': {'location_category': 'icu'}\n    }\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#validation-workflow","title":"Validation Workflow","text":"<pre><code># Validate all loaded tables\norchestrator.validate_all()\n\n# Check which tables are valid\nfor table_name in orchestrator.get_loaded_tables():\n    table = getattr(orchestrator, table_name)\n    if table.isvalid():\n        print(f\"\u2713 {table_name} passed validation\")\n    else:\n        print(f\"\u2717 {table_name} has {len(table.errors)} errors\")\n</code></pre>"},{"location":"user-guide/orchestrator/#utility-methods","title":"Utility Methods","text":""},{"location":"user-guide/orchestrator/#get-loaded-tables","title":"Get Loaded Tables","text":"<pre><code># List of loaded table names\nloaded = orchestrator.get_loaded_tables()\nprint(f\"Loaded tables: {', '.join(loaded)}\")\n\n# Get table objects\ntable_objects = orchestrator.get_tables_obj_list()\nfor table in table_objects:\n    print(f\"{table.table_name}: {len(table.df)} rows\")\n</code></pre>"},{"location":"user-guide/orchestrator/#individual-table-loading","title":"Individual Table Loading","text":"<p>You can also load tables individually:</p> <pre><code># Load tables one at a time\norchestrator.load_patient_data(sample_size=1000)\norchestrator.load_labs_data(\n    columns=['hospitalization_id', 'lab_result_dttm', 'lab_value']\n)\norchestrator.load_vitals_data(\n    filters={'vital_category': ['heart_rate', 'sbp', 'dbp']}\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/orchestrator/#multi-table-analysis","title":"Multi-table Analysis","text":"<pre><code># Load related tables for ICU analysis\norchestrator.initialize(\n    tables=['patient', 'adt', 'labs', 'vitals', 'respiratory_support']\n)\n\n# Get ICU patients\nicu_stays = orchestrator.adt.filter_by_location_category('icu')\nicu_patient_ids = icu_stays['patient_id'].unique()\n\n# Analyze their labs\nicu_labs = orchestrator.labs.df[\n    orchestrator.labs.df['patient_id'].isin(icu_patient_ids)\n]\n\n# Check ventilation status\nvent_patients = orchestrator.respiratory_support.df[\n    orchestrator.respiratory_support.df['device_category'] == 'IMV'\n]['patient_id'].unique()\n</code></pre>"},{"location":"user-guide/orchestrator/#best-practices","title":"Best Practices","text":"<ol> <li>Load only what you need: Use column and filter parameters to reduce memory usage</li> <li>Validate early: Run validation immediately after loading to catch issues</li> <li>Use consistent timezones: The orchestrator ensures all tables use the same timezone</li> <li>Check output directory: Validation reports and logs are saved to the output directory</li> <li>Handle missing tables gracefully: Check if a table is loaded before accessing it</li> </ol>"},{"location":"user-guide/orchestrator/#error-handling","title":"Error Handling","text":"<pre><code># Check if table is loaded\nif orchestrator.labs is not None:\n    labs_data = orchestrator.labs.df\nelse:\n    print(\"Labs table not loaded\")\n\n# Handle validation errors\norchestrator.validate_all()\nfor table_name in orchestrator.get_loaded_tables():\n    table = getattr(orchestrator, table_name)\n    if not table.isvalid():\n        # Save errors for review\n        error_file = f\"{table_name}_errors.csv\"\n        pd.DataFrame(table.errors).to_csv(error_file, index=False)\n        print(f\"Saved {len(table.errors)} errors to {error_file}\")\n</code></pre>"},{"location":"user-guide/orchestrator/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about individual table types</li> <li>Understand data validation</li> <li>See practical examples</li> </ul>"},{"location":"user-guide/outlier-handling/","title":"Outlier Handling","text":"<p>The outlier handling functionality in CLIFpy automatically identifies and removes physiologically implausible values from clinical data. This data cleaning process converts outlier values to NaN while preserving the data structure, ensuring that downstream analysis operates on clinically reasonable values.</p>"},{"location":"user-guide/outlier-handling/#overview","title":"Overview","text":"<p>Outlier handling provides:</p> <ul> <li>Automated detection of values outside clinically reasonable ranges</li> <li>Category-specific ranges for different vital signs, lab tests, medications, and assessments</li> <li>Unit-aware validation for medication dosing based on category and unit combinations</li> <li>Configurable ranges using either CLIF standard ranges or custom configurations</li> <li>Detailed statistics showing the impact of outlier removal</li> <li>Non-destructive preview capability to assess outliers before removal</li> </ul>"},{"location":"user-guide/outlier-handling/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/outlier-handling/#apply_outlier_handling","title":"<code>apply_outlier_handling()</code>","text":"<p>Applies outlier handling by converting out-of-range values to NaN:</p> <pre><code>from clifpy.utils import apply_outlier_handling\n\n# Modify data in-place using CLIF standard ranges\napply_outlier_handling(vitals_table)\n\n# Or use custom configuration\napply_outlier_handling(vitals_table, outlier_config_path=\"/path/to/custom_config.yaml\")\n</code></pre> <p>Parameters: - <code>table_obj</code>: A CLIFpy table object with <code>.df</code> and <code>.table_name</code> attributes - <code>outlier_config_path</code> (optional): Path to custom YAML configuration file</p> <p>Returns: None (modifies table data in-place)</p>"},{"location":"user-guide/outlier-handling/#get_outlier_summary","title":"<code>get_outlier_summary()</code>","text":"<p>Provides a preview of outliers without modifying data:</p> <pre><code>from clifpy.utils import get_outlier_summary\n\n# Get summary without modifying data\nsummary = get_outlier_summary(vitals_table)\nprint(f\"Total rows: {summary['total_rows']}\")\nprint(f\"Config source: {summary['config_source']}\")\n</code></pre> <p>Parameters: - <code>table_obj</code>: A CLIFpy table object with <code>.df</code> and <code>.table_name</code> attributes - <code>outlier_config_path</code> (optional): Path to custom YAML configuration file</p> <p>Returns: Dictionary with outlier analysis summary</p>"},{"location":"user-guide/outlier-handling/#configuration-types","title":"Configuration Types","text":""},{"location":"user-guide/outlier-handling/#internal-clif-standard-configuration","title":"Internal CLIF Standard Configuration","text":"<p>By default, CLIFpy uses internal clinically-validated ranges:</p> <pre><code>from clifpy.utils import apply_outlier_handling\n\n# Uses internal CLIF standard ranges automatically\napply_outlier_handling(vitals_table)\n# Output: \"Using CLIF standard outlier ranges\"\n</code></pre> <p>The internal configuration includes ranges for: - Vitals: Heart rate (0-300), blood pressure (0-300/0-200), temperature (32-44\u00b0C), etc. - Labs: Hemoglobin (2-25), sodium (90-210), glucose (0-2000), lactate (0-30), etc. - Medications: Unit-specific dosing ranges (e.g., norepinephrine 0-3 mcg/kg/min) - Assessments: Scale-specific ranges (e.g., GCS 3-15, RASS -5 to +4)</p>"},{"location":"user-guide/outlier-handling/#custom-yaml-configuration","title":"Custom YAML Configuration","text":"<p>Create custom configurations for specific research needs:</p> <pre><code># Apply custom ranges\napply_outlier_handling(vitals_table, outlier_config_path=\"/path/to/custom_ranges.yaml\")\n# Output: \"Using custom outlier ranges from: /path/to/custom_ranges.yaml\"\n</code></pre>"},{"location":"user-guide/outlier-handling/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/outlier-handling/#example-1-basic-usage-with-standard-ranges","title":"Example 1: Basic Usage with Standard Ranges","text":"<pre><code>from clifpy import Vitals\nfrom clifpy.utils import apply_outlier_handling\n\n# Load vitals data\nvitals = Vitals.from_file('/data', 'parquet', 'UTC')\n\nprint(f\"Before: {vitals.df['vital_value'].notna().sum()} non-null values\")\n\n# Apply outlier handling with CLIF standard ranges\napply_outlier_handling(vitals)\n\nprint(f\"After: {vitals.df['vital_value'].notna().sum()} non-null values\")\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-2-preview-outliers-before-removal","title":"Example 2: Preview Outliers Before Removal","text":"<pre><code>from clifpy.utils import get_outlier_summary, apply_outlier_handling\n\n# Get summary without modifying data\nsummary = get_outlier_summary(vitals)\nprint(\"Outlier Analysis Summary:\")\nprint(f\"- Table: {summary['table_name']}\")\nprint(f\"- Total rows: {summary['total_rows']}\")\nprint(f\"- Configuration: {summary['config_source']}\")\n\n# Apply outlier handling after review\napply_outlier_handling(vitals)\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-3-custom-configuration-for-research","title":"Example 3: Custom Configuration for Research","text":"<pre><code># Create custom configuration file\ncustom_config = \"\"\"\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 40    # More restrictive than standard (0)\n        max: 180   # More restrictive than standard (300)\n      temp_c:\n        min: 35.0  # More restrictive than standard (32)\n        max: 42.0  # More restrictive than standard (44)\n\"\"\"\n\nwith open('research_config.yaml', 'w') as f:\n    f.write(custom_config)\n\n# Apply custom ranges\napply_outlier_handling(vitals, outlier_config_path='research_config.yaml')\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-4-multiple-tables-with-different-configurations","title":"Example 4: Multiple Tables with Different Configurations","text":"<pre><code>from clifpy import Labs, Vitals, MedicationAdminContinuous\nfrom clifpy.utils import apply_outlier_handling\n\n# Load tables\nvitals = Vitals.from_file('/data', 'parquet', 'UTC')\nlabs = Labs.from_file('/data', 'parquet', 'UTC')\nmeds = MedicationAdminContinuous.from_file('/data', 'parquet', 'UTC')\n\n# Apply outlier handling to each table\nfor table in [vitals, labs, meds]:\n    print(f\"\\n=== Processing {table.table_name} ===\")\n    apply_outlier_handling(table)\n</code></pre>"},{"location":"user-guide/outlier-handling/#table-specific-handling","title":"Table-Specific Handling","text":""},{"location":"user-guide/outlier-handling/#simple-range-columns","title":"Simple Range Columns","text":"<p>For columns with straightforward min/max ranges:</p> <pre><code># Example: Age at admission (0-120 years)\n# Configuration:\nhospitalization:\n  age_at_admission:\n    min: 0\n    max: 120\n\n# Output statistics:\n# age_at_admission              :   5432 values \u2192     23 nullified ( 0.4%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#category-dependent-columns","title":"Category-Dependent Columns","text":"<p>For vitals, labs, and assessments where ranges depend on the category:</p> <pre><code># Example: Vital signs with different ranges per category\n# Configuration:\nvitals:\n  vital_value:\n    heart_rate:\n      min: 0\n      max: 300\n    temp_c:\n      min: 32\n      max: 44\n\n# Output statistics:\n# Vitals Table - Category Statistics:\n#   heart_rate        :  15234 values \u2192    156 nullified ( 1.0%)\n#   temp_c           :   8765 values \u2192     23 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#unit-dependent-medication-dosing","title":"Unit-Dependent Medication Dosing","text":"<p>For medications where ranges depend on both category and unit:</p> <pre><code># Example: Norepinephrine dosing with different units\n# Configuration:\nmedication_admin_continuous:\n  med_dose:\n    norepinephrine:\n      \"mcg/kg/min\":\n        min: 0.0\n        max: 3.0\n      \"mcg/min\":\n        min: 0.0\n        max: 200.0\n\n# Output statistics:\n# Medication Table - Category/Unit Statistics:\n#   norepinephrine (mcg/kg/min)  :   2341 values \u2192     12 nullified ( 0.5%)\n#   norepinephrine (mcg/min)     :    876 values \u2192      4 nullified ( 0.5%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#custom-yaml-configuration-examples","title":"Custom YAML Configuration Examples","text":""},{"location":"user-guide/outlier-handling/#example-1-research-specific-vitals-ranges","title":"Example 1: Research-Specific Vitals Ranges","text":"<pre><code># custom_vitals_config.yaml\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 50     # More restrictive for adults\n        max: 150    # Exclude extreme tachycardia\n      sbp:\n        min: 70     # Focus on hypotension\n        max: 200    # Exclude severe hypertension\n      temp_c:\n        min: 36.0   # Normothermic range\n        max: 39.0   # Exclude extreme hyperthermia\n      spo2:\n        min: 88     # Allow mild hypoxemia\n        max: 100    # Standard upper bound\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-2-pediatric-specific-ranges","title":"Example 2: Pediatric-Specific Ranges","text":"<pre><code># pediatric_config.yaml\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 60     # Pediatric range\n        max: 200    # Higher for children\n      sbp:\n        min: 60     # Lower for pediatrics\n        max: 140\n\n  hospitalization:\n    age_at_admission:\n      min: 0\n      max: 18     # Pediatric patients only\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-3-icu-specific-lab-ranges","title":"Example 3: ICU-Specific Lab Ranges","text":"<pre><code># icu_lab_config.yaml\ntables:\n  labs:\n    lab_value_numeric:\n      lactate:\n        min: 0.5    # Minimum detectable\n        max: 20.0   # ICU-relevant range\n      hemoglobin:\n        min: 4.0    # Severe anemia threshold\n        max: 20.0   # Exclude transfusion artifacts\n      creatinine:\n        min: 0.3    # Physiologic minimum\n        max: 15.0   # Include severe AKI\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-4-complete-custom-configuration-template","title":"Example 4: Complete Custom Configuration Template","text":"<pre><code># complete_custom_config.yaml\ntables:\n  # Simple range columns\n  hospitalization:\n    age_at_admission:\n      min: 18      # Adult patients only\n      max: 100     # Exclude very elderly\n\n  respiratory_support:\n    fio2_set:\n      min: 0.21    # Room air minimum\n      max: 1.0     # 100% oxygen maximum\n    peep_set:\n      min: 0       # No PEEP\n      max: 25      # High PEEP limit\n\n  # Category-dependent columns\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 40\n        max: 200\n      sbp:\n        min: 60\n        max: 250\n      temp_c:\n        min: 35.0\n        max: 42.0\n\n  labs:\n    lab_value_numeric:\n      hemoglobin:\n        min: 5.0\n        max: 18.0\n      sodium:\n        min: 120\n        max: 160\n\n  # Unit-dependent medication dosing\n  medication_admin_continuous:\n    med_dose:\n      norepinephrine:\n        \"mcg/kg/min\":\n          min: 0.01\n          max: 2.0\n        \"mcg/min\":\n          min: 1.0\n          max: 150.0\n      propofol:\n        \"mg/hr\":\n          min: 1.0\n          max: 300.0\n\n  # Assessment-specific ranges\n  patient_assessments:\n    numerical_value:\n      gcs_total:\n        min: 3\n        max: 15\n      RASS:\n        min: -5\n        max: 4\n</code></pre>"},{"location":"user-guide/outlier-handling/#understanding-output-statistics","title":"Understanding Output Statistics","text":"<p>The outlier handling provides detailed statistics showing the impact of data cleaning:</p>"},{"location":"user-guide/outlier-handling/#category-dependent-statistics","title":"Category-Dependent Statistics","text":"<pre><code>Vitals Table - Category Statistics:\n  heart_rate        :  15234 values \u2192    156 nullified ( 1.0%)\n  sbp              :  12876 values \u2192     45 nullified ( 0.3%)\n  temp_c           :   8765 values \u2192     23 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#medication-unit-dependent-statistics","title":"Medication Unit-Dependent Statistics","text":"<pre><code>Medication Table - Category/Unit Statistics:\n  norepinephrine (mcg/kg/min)  :   2341 values \u2192     12 nullified ( 0.5%)\n  propofol (mg/hr)            :   1876 values \u2192      8 nullified ( 0.4%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#simple-range-statistics","title":"Simple Range Statistics","text":"<pre><code>age_at_admission              :   5432 values \u2192     23 nullified ( 0.4%)\nfio2_set                     :   3456 values \u2192     12 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#integration-with-cliforchestrator","title":"Integration with ClifOrchestrator","text":"<p>The outlier handling integrates seamlessly with the ClifOrchestrator workflow:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\nfrom clifpy.utils import apply_outlier_handling\n\n# Initialize orchestrator and load tables\nco = ClifOrchestrator('/data', 'parquet', 'UTC')\nco.initialize(['vitals', 'labs', 'medication_admin_continuous'])\n\n# Apply outlier handling to all loaded tables\nfor table_name in co.get_loaded_tables():\n    table_obj = getattr(co, table_name)\n    print(f\"\\n=== Cleaning {table_name} ===\")\n    apply_outlier_handling(table_obj)\n\n# Validate after outlier handling\nco.validate_all()\n\n# Create wide dataset with clean data\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin', 'sodium']\n    }\n)\n</code></pre>"},{"location":"user-guide/outlier-handling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/outlier-handling/#1-preview-before-application","title":"1. Preview Before Application","text":"<pre><code># Always preview outliers first\nsummary = get_outlier_summary(table)\nprint(f\"Will affect {summary['total_rows']} rows\")\n\n# Apply after review\napply_outlier_handling(table)\n</code></pre>"},{"location":"user-guide/outlier-handling/#2-keep-original-data","title":"2. Keep Original Data","text":"<pre><code># Make backup before outlier handling\noriginal_df = vitals.df.copy()\n\n# Apply outlier handling\napply_outlier_handling(vitals)\n\n# Compare results\nprint(f\"Original: {original_df['vital_value'].notna().sum()} values\")\nprint(f\"Cleaned:  {vitals.df['vital_value'].notna().sum()} values\")\nprint(f\"Removed:  {original_df['vital_value'].notna().sum() - vitals.df['vital_value'].notna().sum()} values\")\n</code></pre>"},{"location":"user-guide/outlier-handling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/outlier-handling/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>No Configuration Found <pre><code># Error: \"No outlier configuration found for table: custom_table\"\n# Solution: Add table configuration to your custom YAML\n\ntables:\n  custom_table:\n    numeric_column:\n      min: 0\n      max: 100\n</code></pre></p> <p>Missing Columns <pre><code># Warning: Configuration references columns not in data\n# Solution: Check column names in your data vs. configuration\nprint(vitals.df.columns.tolist())  # Check actual column names\n</code></pre></p> <p>No Outliers Detected <pre><code># All values are within range - this is normal for clean data\n# The statistics will show \"0 nullified\" for all categories\n</code></pre></p>"},{"location":"user-guide/outlier-handling/#internal-clif-standard-ranges","title":"Internal CLIF Standard Ranges","text":"<p>The internal configuration includes clinically-validated ranges for:</p>"},{"location":"user-guide/outlier-handling/#vitals","title":"Vitals","text":"<ul> <li>Heart rate: 0-300 bpm</li> <li>Blood pressure: SBP 0-300, DBP 0-200, MAP 0-250 mmHg  </li> <li>Temperature: 32-44\u00b0C</li> <li>SpO2: 50-100%</li> <li>Respiratory rate: 0-60/min</li> <li>Height: 76-255 cm, Weight: 30-1100 kg</li> </ul>"},{"location":"user-guide/outlier-handling/#common-labs","title":"Common Labs","text":"<ul> <li>Hemoglobin: 2.0-25.0 g/dL</li> <li>Sodium: 90-210 mEq/L</li> <li>Potassium: 0-15 mEq/L</li> <li>Creatinine: 0-20 mg/dL</li> <li>Glucose: 0-2000 mg/dL</li> <li>Lactate: 0-30 mmol/L</li> </ul>"},{"location":"user-guide/outlier-handling/#medication-dosing-examples","title":"Medication Dosing (examples)","text":"<ul> <li>Norepinephrine: 0-3 mcg/kg/min, 0-200 mcg/min</li> <li>Propofol: 0-400 mg/hr, 0-200 mcg/kg/min</li> <li>Fentanyl: 0-500 mcg/hr, 0-10 mcg/kg/hr</li> </ul>"},{"location":"user-guide/outlier-handling/#clinical-assessments","title":"Clinical Assessments","text":"<ul> <li>GCS Total: 3-15</li> <li>RASS: -5 to +4</li> <li>Richmond Agitation Sedation Scale: 1-7</li> <li>Braden Total: 6-23</li> </ul>"},{"location":"user-guide/outlier-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about data validation to ensure data quality after outlier removal</li> <li>Explore wide dataset creation with cleaned data</li> <li>Review individual table guides for table-specific considerations</li> <li>See the orchestrator guide for workflow integration</li> </ul>"},{"location":"user-guide/sofa/","title":"SOFA Score Computation","text":"<p>Compute Sequential Organ Failure Assessment (SOFA) scores from CLIF data.</p>"},{"location":"user-guide/sofa/#quick-start","title":"Quick Start","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\nco = ClifOrchestrator(config_path='config/config.yaml')\nsofa_scores = co.compute_sofa_scores()\n</code></pre>"},{"location":"user-guide/sofa/#parameters","title":"Parameters","text":"<ul> <li><code>wide_df</code>: Optional pre-computed wide dataset</li> <li><code>cohort_df</code>: Optional time windows for filtering</li> <li><code>id_name</code>: Grouping column (default: 'encounter_block')</li> <li><code>extremal_type</code>: 'worst' (default) or 'latest' (future)</li> <li><code>fill_na_scores_with_zero</code>: Handle missing data (default: True)</li> </ul>"},{"location":"user-guide/sofa/#encounter-block-vs-hospitalization-id","title":"Encounter Block vs Hospitalization ID","text":"<p>By default, SOFA scores are computed per <code>encounter_block</code>, which groups related hospitalizations:</p> <pre><code># Initialize with encounter stitching\nco = ClifOrchestrator(\n    config_path='config/config.yaml',\n    stitch_encounter=True,\n    stitch_time_interval=6  # hours between admissions\n)\n\n# Default: scores per encounter block (may span multiple hospitalizations)\nsofa_by_encounter = co.compute_sofa_scores()  # uses encounter_block\n\n# Alternative: scores per individual hospitalization\nsofa_by_hosp = co.compute_sofa_scores(id_name='hospitalization_id')\n</code></pre> <p>What happens when using encounter_block:</p> <ul> <li>If encounter mapping doesn't exist, it's created automatically via <code>run_stitch_encounters()</code></li> <li>Multiple hospitalizations within the time interval are grouped as one encounter</li> <li>SOFA score represents the worst values across the entire encounter</li> <li>Result has one row per encounter_block instead of per hospitalization</li> </ul> <p>Example encounter mapping: <pre><code>hospitalization_id | encounter_block\n-------------------|----------------\n12345             | E001\n12346             | E001  # Same encounter (readmit &lt; 6 hours)\n12347             | E002  # Different encounter\n</code></pre></p>"},{"location":"user-guide/sofa/#required-data","title":"Required Data","text":"<p>SOFA requires these variables:</p> <ul> <li>Labs: creatinine, platelet_count, po2_arterial, bilirubin_total</li> <li>Vitals: map, spo2</li> <li>Assessments: gcs_total</li> <li>Medications: norepinephrine, epinephrine, dopamine, dobutamine (pre-converted to mcg/kg/min)</li> <li>Respiratory: device_category, fio2_set</li> </ul>"},{"location":"user-guide/sofa/#missing-data","title":"Missing Data","text":"<ul> <li>Missing values default to score of 0</li> <li>P/F ratio uses PaO2 or imputed from SpO2</li> <li>Medications must be pre-converted to standard units</li> </ul>"},{"location":"user-guide/sofa/#example-with-time-filtering","title":"Example with Time Filtering","text":"<pre><code>import pandas as pd\n\n# Define cohort with time windows\ncohort_df = pd.DataFrame({\n    'encounter_block': ['E001', 'E002'],  # or 'hospitalization_id'\n    'start_time': pd.to_datetime(['2024-01-01', '2024-01-02']),\n    'end_time': pd.to_datetime(['2024-01-03', '2024-01-04'])\n})\n\nsofa_scores = co.compute_sofa_scores(\n    cohort_df=cohort_df,\n    id_name='encounter_block'  # must match cohort_df column\n)\n</code></pre>"},{"location":"user-guide/sofa/#output","title":"Output","text":"<p>Returns DataFrame with:</p> <ul> <li>One row per <code>id_name</code> (encounter_block or hospitalization_id)</li> <li>Individual component scores (sofa_cv_97, sofa_coag, sofa_liver, sofa_resp, sofa_cns, sofa_renal)</li> <li>Total SOFA score (sofa_total)</li> <li>Intermediate calculations (p_f, p_f_imputed)</li> </ul>"},{"location":"user-guide/sofa/#sofa-components","title":"SOFA Components","text":"Component Based on Score Range Cardiovascular Vasopressor doses, MAP 0-4 Coagulation Platelet count 0-4 Liver Bilirubin levels 0-4 Respiratory P/F ratio, respiratory support 0-4 CNS GCS score 0-4 Renal Creatinine levels 0-4 <p>Higher scores indicate worse organ dysfunction. Total score ranges from 0-24.</p>"},{"location":"user-guide/sofa/#notes","title":"Notes","text":"<ul> <li>Medication units: Ensure medications are pre-converted to mcg/kg/min using the unit converter</li> <li>PaO2 imputation: When PaO2 is missing but SpO2 &lt; 97%, PaO2 is estimated using the Severinghaus equation</li> <li>Missing data philosophy: Absence of monitoring data suggests the organ wasn't failing enough to warrant close observation (score = 0)</li> </ul>"},{"location":"user-guide/timezones/","title":"Working with Timezones","text":"<p>Proper timezone handling is critical when working with ICU data from multiple sources. This guide explains how CLIFpy manages timezones and best practices for your data.</p>"},{"location":"user-guide/timezones/#overview","title":"Overview","text":"<p>CLIFpy ensures all datetime columns are timezone-aware to: - Prevent ambiguity in timestamp interpretation - Enable accurate time-based calculations - Support data from multiple time zones - Maintain consistency across tables</p>"},{"location":"user-guide/timezones/#timezone-specification","title":"Timezone Specification","text":""},{"location":"user-guide/timezones/#when-loading-data","title":"When Loading Data","text":"<p>Always specify the timezone when loading data:</p> <pre><code># Specify source data timezone\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'  # Source data timezone\n)\n\n# Common US timezones\n# 'US/Eastern', 'US/Central', 'US/Mountain', 'US/Pacific'\n# 'America/New_York', 'America/Chicago', 'America/Denver', 'America/Los_Angeles'\n</code></pre>"},{"location":"user-guide/timezones/#using-orchestrator","title":"Using Orchestrator","text":"<p>The orchestrator ensures consistent timezone across all tables:</p> <pre><code>orchestrator = ClifOrchestrator(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'  # Applied to all tables\n)\n</code></pre>"},{"location":"user-guide/timezones/#timezone-conversion","title":"Timezone Conversion","text":""},{"location":"user-guide/timezones/#during-loading","title":"During Loading","text":"<p>CLIFpy automatically converts datetime columns to the specified timezone:</p> <pre><code># Original data in UTC\ntable = TableClass.from_file('/data', 'parquet', timezone='UTC')\n\n# Convert to Central time during loading\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#after-loading","title":"After Loading","text":"<p>Convert between timezones using pandas:</p> <pre><code># Convert to different timezone\ndf = table.df.copy()\ndf['lab_datetime'] = df['lab_datetime'].dt.tz_convert('US/Eastern')\n\n# Localize timezone-naive data (not recommended)\n# df['datetime'] = df['datetime'].dt.tz_localize('US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#common-timezone-issues","title":"Common Timezone Issues","text":""},{"location":"user-guide/timezones/#issue-1-timezone-naive-data","title":"Issue 1: Timezone-Naive Data","text":"<p>Problem: Source data lacks timezone information</p> <pre><code># This will fail validation\ntable.validate()\n# Error: \"Datetime column 'admission_date' is not timezone-aware\"\n</code></pre> <p>Solution: Specify timezone during loading</p> <pre><code># CLIFpy will localize to specified timezone\ntable = TableClass.from_file(\n    '/data', \n    'parquet', \n    timezone='US/Central'  # Assumes data is in Central time\n)\n</code></pre>"},{"location":"user-guide/timezones/#issue-2-mixed-timezones","title":"Issue 2: Mixed Timezones","text":"<p>Problem: Different tables from different timezones</p> <pre><code># Hospital A in Eastern time\nlabs_a = Labs.from_file('/hospital_a/data', 'parquet', timezone='US/Eastern')\n\n# Hospital B in Pacific time  \nlabs_b = Labs.from_file('/hospital_b/data', 'parquet', timezone='US/Pacific')\n</code></pre> <p>Solution: Convert to common timezone</p> <pre><code># Convert both to UTC for analysis\nlabs_a.df['lab_datetime'] = labs_a.df['lab_datetime'].dt.tz_convert('UTC')\nlabs_b.df['lab_datetime'] = labs_b.df['lab_datetime'].dt.tz_convert('UTC')\n\n# Combine datasets\ncombined_labs = pd.concat([labs_a.df, labs_b.df])\n</code></pre>"},{"location":"user-guide/timezones/#issue-3-daylight-saving-time","title":"Issue 3: Daylight Saving Time","text":"<p>Problem: Ambiguous times during DST transitions</p> <pre><code># Fall back: 2:00 AM occurs twice\n# Spring forward: 2:00 AM doesn't exist\n</code></pre> <p>Solution: Use pytz-aware timezone names</p> <pre><code># Good - handles DST automatically\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n\n# Avoid - doesn't handle DST\n# table = TableClass.from_file('/data', 'parquet', timezone='CST6CDT')\n</code></pre>"},{"location":"user-guide/timezones/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/timezones/#1-know-your-source-timezone","title":"1. Know Your Source Timezone","text":"<pre><code># Document source timezone\n\"\"\"\nData extracted from Hospital EHR\nTimezone: US/Central (America/Chicago)\nIncludes DST adjustments\n\"\"\"\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#2-use-consistent-timezones","title":"2. Use Consistent Timezones","text":"<pre><code># Use orchestrator for consistency\norchestrator = ClifOrchestrator('/data', 'parquet', timezone='US/Central')\norchestrator.initialize(tables=['labs', 'vitals', 'medications'])\n\n# All tables now use same timezone\n</code></pre>"},{"location":"user-guide/timezones/#3-validate-timezone-handling","title":"3. Validate Timezone Handling","text":"<pre><code># Check timezone after loading\nprint(f\"Lab datetime timezone: {table.df['lab_datetime'].dt.tz}\")\n\n# Verify reasonable time ranges\nprint(f\"Earliest: {table.df['lab_datetime'].min()}\")\nprint(f\"Latest: {table.df['lab_datetime'].max()}\")\n</code></pre>"},{"location":"user-guide/timezones/#4-document-timezone-conversions","title":"4. Document Timezone Conversions","text":"<pre><code># Keep audit trail of conversions\nmetadata = {\n    'original_timezone': 'US/Eastern',\n    'converted_timezone': 'UTC',\n    'conversion_date': datetime.now(),\n    'conversion_method': 'pandas dt.tz_convert'\n}\n</code></pre>"},{"location":"user-guide/timezones/#time-based-calculations","title":"Time-based Calculations","text":""},{"location":"user-guide/timezones/#duration-calculations","title":"Duration Calculations","text":"<p>Timezone-aware datetimes ensure accurate duration calculations:</p> <pre><code># Calculate length of stay\nlos = adt.df['out_dttm'] - adt.df['in_dttm']\nlos_hours = los.dt.total_seconds() / 3600\n\n# Time since admission\ncurrent_time = pd.Timestamp.now(tz='US/Central')\ntime_since = current_time - hosp.df['admission_dttm']\n</code></pre>"},{"location":"user-guide/timezones/#filtering-by-time","title":"Filtering by Time","text":"<pre><code># Get data from last 24 hours\ncutoff = pd.Timestamp.now(tz='US/Central') - pd.Timedelta(hours=24)\nrecent = table.df[table.df['datetime_column'] &gt;= cutoff]\n\n# Filter to specific date (timezone-aware)\ndate = pd.Timestamp('2023-01-01', tz='US/Central')\nday_data = table.df[table.df['datetime_column'].dt.date == date.date()]\n</code></pre>"},{"location":"user-guide/timezones/#aggregating-by-time","title":"Aggregating by Time","text":"<pre><code># Hourly aggregation\nhourly = table.df.set_index('datetime_column').resample('H').mean()\n\n# Daily aggregation (timezone affects day boundaries!)\ndaily = table.df.set_index('datetime_column').resample('D').count()\n</code></pre>"},{"location":"user-guide/timezones/#multi-site-considerations","title":"Multi-site Considerations","text":"<p>When combining data from multiple sites:</p> <pre><code># Strategy 1: Convert all to UTC\nsites = ['site_a', 'site_b', 'site_c']\nsite_timezones = {\n    'site_a': 'US/Eastern',\n    'site_b': 'US/Central', \n    'site_c': 'US/Pacific'\n}\n\nall_data = []\nfor site in sites:\n    table = Labs.from_file(f'/data/{site}', 'parquet', \n                          timezone=site_timezones[site])\n    # Convert to UTC\n    table.df['lab_datetime'] = table.df['lab_datetime'].dt.tz_convert('UTC')\n    table.df['site'] = site\n    all_data.append(table.df)\n\ncombined = pd.concat(all_data)\n</code></pre> <pre><code># Strategy 2: Use site's local time with site column\n# Keep original timezone but track source\nfor site in sites:\n    table = Labs.from_file(f'/data/{site}', 'parquet',\n                          timezone=site_timezones[site])\n    table.df['site'] = site\n    table.df['source_timezone'] = site_timezones[site]\n</code></pre>"},{"location":"user-guide/timezones/#timezone-reference","title":"Timezone Reference","text":"<p>Common medical facility timezones:</p> <pre><code>US_TIMEZONES = {\n    'Eastern': 'US/Eastern',     # NYC, Boston, Atlanta\n    'Central': 'US/Central',     # Chicago, Houston, Dallas\n    'Mountain': 'US/Mountain',   # Denver, Phoenix\n    'Pacific': 'US/Pacific',     # LA, Seattle, San Francisco\n    'Alaska': 'US/Alaska',       # Anchorage\n    'Hawaii': 'US/Hawaii'        # Honolulu\n}\n\n# International\nINTL_TIMEZONES = {\n    'London': 'Europe/London',\n    'Paris': 'Europe/Paris',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney',\n    'Toronto': 'America/Toronto'\n}\n</code></pre>"},{"location":"user-guide/timezones/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/timezones/#check-current-timezone","title":"Check Current Timezone","text":"<pre><code># For a datetime column\nprint(table.df['datetime_column'].dt.tz)\n\n# For a single timestamp\nprint(table.df['datetime_column'].iloc[0].tzinfo)\n</code></pre>"},{"location":"user-guide/timezones/#fix-timezone-issues","title":"Fix Timezone Issues","text":"<pre><code># If validation fails due to timezone\nif not table.isvalid():\n    tz_errors = [e for e in table.errors if 'timezone' in str(e)]\n    if tz_errors:\n        # Reload with proper timezone\n        table = TableClass.from_file('/data', 'parquet', \n                                   timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#next-steps","title":"Next Steps","text":"<ul> <li>Review validation guide for timezone validation</li> <li>See examples of timezone handling</li> <li>Learn about multi-site analysis</li> </ul>"},{"location":"user-guide/validation/","title":"Data Validation","text":"<p>CLIFpy provides comprehensive validation to ensure your data conforms to CLIF standards. This guide explains the validation process and how to interpret results.</p>"},{"location":"user-guide/validation/#overview","title":"Overview","text":"<p>Validation in CLIFpy operates at multiple levels:</p> <ol> <li>Schema Validation - Ensures required columns exist with correct data types</li> <li>Category Validation - Verifies values match standardized categories</li> <li>Range Validation - Checks values fall within clinically reasonable ranges</li> <li>Timezone Validation - Ensures datetime columns are timezone-aware</li> <li>Duplicate Detection - Identifies duplicate records based on composite keys</li> <li>Completeness Checks - Analyzes missing data patterns</li> </ol>"},{"location":"user-guide/validation/#running-validation","title":"Running Validation","text":""},{"location":"user-guide/validation/#basic-validation","title":"Basic Validation","text":"<pre><code># Load and validate a table\ntable = TableClass.from_file('/data', 'parquet')\ntable.validate()\n\n# Check if valid\nif table.isvalid():\n    print(\"Validation passed!\")\nelse:\n    print(f\"Found {len(table.errors)} validation errors\")\n</code></pre>"},{"location":"user-guide/validation/#bulk-validation-with-orchestrator","title":"Bulk Validation with Orchestrator","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\norchestrator = ClifOrchestrator('/data', 'parquet')\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Validate all tables\norchestrator.validate_all()\n</code></pre>"},{"location":"user-guide/validation/#understanding-validation-results","title":"Understanding Validation Results","text":""},{"location":"user-guide/validation/#error-types","title":"Error Types","text":"<p>Validation errors are stored in the <code>errors</code> attribute:</p> <pre><code># Review errors\nfor error in table.errors[:10]:  # First 10 errors\n    print(f\"Type: {error['type']}\")\n    print(f\"Message: {error['message']}\")\n    print(f\"Details: {error.get('details', 'N/A')}\")\n    print(\"-\" * 50)\n</code></pre> <p>Common error types: - <code>missing_column</code> - Required column not found - <code>invalid_category</code> - Value not in permissible list - <code>out_of_range</code> - Value outside acceptable range - <code>invalid_timezone</code> - Datetime column not timezone-aware - <code>duplicate_rows</code> - Duplicate records found</p>"},{"location":"user-guide/validation/#validation-reports","title":"Validation Reports","text":"<p>Validation results are automatically saved to the output directory:</p> <pre><code># Set custom output directory\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    output_directory='/path/to/reports'\n)\n\n# After validation, these files are created:\n# - validation_log_[table_name].log\n# - validation_errors_[table_name].csv\n# - missing_data_stats_[table_name].csv\n</code></pre>"},{"location":"user-guide/validation/#schema-validation","title":"Schema Validation","text":"<p>Each table has a YAML schema defining its structure:</p> <pre><code># Example from patient_schema.yaml\ncolumns:\n  - name: patient_id\n    data_type: VARCHAR\n    required: true\n    is_category_column: false\n  - name: sex_category\n    data_type: VARCHAR\n    required: true\n    is_category_column: true\n    permissible_values:\n      - Male\n      - Female\n      - Unknown\n</code></pre>"},{"location":"user-guide/validation/#required-columns","title":"Required Columns","text":"<pre><code># Check which required columns are missing\nif not table.isvalid():\n    missing_cols = [e for e in table.errors if e['type'] == 'missing_column']\n    for error in missing_cols:\n        print(f\"Missing required column: {error['column']}\")\n</code></pre>"},{"location":"user-guide/validation/#data-types","title":"Data Types","text":"<p>CLIFpy validates that columns have appropriate data types: - <code>VARCHAR</code> - String/text data - <code>DATETIME</code> - Timezone-aware datetime - <code>NUMERIC</code> - Numeric values (int or float)</p>"},{"location":"user-guide/validation/#category-validation","title":"Category Validation","text":"<p>Standardized categories ensure consistency across institutions:</p> <pre><code># Example: Validating location categories in ADT\nvalid_locations = ['ed', 'ward', 'stepdown', 'icu', 'procedural', \n                   'l&amp;d', 'hospice', 'psych', 'rehab', 'radiology', \n                   'dialysis', 'other']\n\n# Check for invalid categories\ncategory_errors = [e for e in table.errors \n                   if e['type'] == 'invalid_category']\n</code></pre>"},{"location":"user-guide/validation/#range-validation","title":"Range Validation","text":"<p>Clinical values are checked against reasonable ranges:</p> <pre><code># Example: Vital signs ranges\nranges = {\n    'heart_rate': (0, 300),\n    'sbp': (0, 300),\n    'dbp': (0, 200),\n    'temp_c': (25, 44),\n    'spo2': (50, 100)\n}\n\n# Identify out-of-range values\nrange_errors = [e for e in table.errors \n                if e['type'] == 'out_of_range']\n</code></pre>"},{"location":"user-guide/validation/#timezone-validation","title":"Timezone Validation","text":"<p>All datetime columns must be timezone-aware:</p> <pre><code># Check timezone issues\ntz_errors = [e for e in table.errors \n             if 'timezone' in e.get('message', '').lower()]\n\nif tz_errors:\n    print(\"Datetime columns must be timezone-aware\")\n    print(\"Consider reloading with explicit timezone:\")\n    print(\"table = TableClass.from_file('/data', 'parquet', timezone='US/Central')\")\n</code></pre>"},{"location":"user-guide/validation/#duplicate-detection","title":"Duplicate Detection","text":"<p>Duplicates are identified based on composite keys:</p> <pre><code># Check for duplicates\nduplicate_errors = [e for e in table.errors \n                    if e['type'] == 'duplicate_rows']\n\nif duplicate_errors:\n    for error in duplicate_errors:\n        print(f\"Found {error['count']} duplicate rows\")\n        print(f\"Composite keys: {error['keys']}\")\n</code></pre>"},{"location":"user-guide/validation/#missing-data-analysis","title":"Missing Data Analysis","text":"<p>CLIFpy analyzes missing data patterns:</p> <pre><code># Get missing data statistics\nsummary = table.get_summary()\nif 'missing_data' in summary:\n    print(\"Columns with missing data:\")\n    for col, count in summary['missing_data'].items():\n        pct = (count / summary['num_rows']) * 100\n        print(f\"  {col}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"user-guide/validation/#custom-validation","title":"Custom Validation","text":"<p>Tables may include specific validation logic:</p> <pre><code># Example: Labs table validates reference ranges\n# Example: Medications validates dose units match drug\n# Example: Respiratory support validates device/mode combinations\n</code></pre>"},{"location":"user-guide/validation/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate after loading - Catch issues early</li> <li>Review all error types - Don't just check if valid</li> <li>Save validation reports - Keep audit trail</li> <li>Fix data at source - Update extraction/ETL process</li> <li>Document exceptions - Some errors may be acceptable</li> </ol>"},{"location":"user-guide/validation/#handling-validation-errors","title":"Handling Validation Errors","text":""},{"location":"user-guide/validation/#option-1-fix-and-reload","title":"Option 1: Fix and Reload","text":"<pre><code># Identify issues\ntable.validate()\nerrors_df = pd.DataFrame(table.errors)\nerrors_df.to_csv('validation_errors.csv', index=False)\n\n# Fix source data based on errors\n# Then reload\ntable = TableClass.from_file('/fixed_data', 'parquet')\ntable.validate()\n</code></pre>"},{"location":"user-guide/validation/#option-2-filter-invalid-records","title":"Option 2: Filter Invalid Records","text":"<pre><code># Remove records with invalid categories\nvalid_categories = ['Male', 'Female', 'Unknown']\ncleaned_df = table.df[table.df['sex_category'].isin(valid_categories)]\n\n# Create new table instance with cleaned data\ntable = TableClass(data=cleaned_df, timezone='US/Central')\n</code></pre>"},{"location":"user-guide/validation/#option-3-document-and-proceed","title":"Option 3: Document and Proceed","text":"<pre><code># For acceptable validation errors\nif not table.isvalid():\n    # Document why proceeding despite errors\n    with open('validation_notes.txt', 'w') as f:\n        f.write(f\"Proceeding with {len(table.errors)} known issues:\\n\")\n        f.write(\"- Missing optional columns\\n\")\n        f.write(\"- Historical data outside current ranges\\n\")\n</code></pre>"},{"location":"user-guide/validation/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about timezone handling</li> <li>Explore table-specific guides</li> <li>See practical examples</li> </ul>"},{"location":"user-guide/waterfall/","title":"Respiratory Support Waterfall Processing","text":"<p>The waterfall method provides a sophisticated data cleaning and imputation pipeline for respiratory support data, ensuring continuous and complete ventilator records for analysis.</p>"},{"location":"user-guide/waterfall/#overview","title":"Overview","text":"<p>The waterfall processing transforms raw, sparse respiratory support data into a dense, analysis-ready dataset by: - Creating hourly scaffolds for continuous timelines - Inferring missing device and mode information - Forward-filling numeric values within ventilation episodes - Applying clinical logic and heuristics</p> <p></p>"},{"location":"user-guide/waterfall/#usage","title":"Usage","text":""},{"location":"user-guide/waterfall/#basic-usage","title":"Basic Usage","text":"<pre><code>from clifpy.tables.respiratory_support import RespiratorySupport\n\n# Load your respiratory support data\nresp_support = RespiratorySupport.from_file(\n    data_directory=\"/path/to/data\",\n    filetype=\"parquet\"\n)\n\n# Apply waterfall processing\nprocessed = resp_support.waterfall()\n\n# The result is a new RespiratorySupport instance\nprocessed.validate()  # Validate the processed data\ndf = processed.df     # Access the DataFrame\n</code></pre>"},{"location":"user-guide/waterfall/#advanced-options","title":"Advanced Options","text":"<pre><code># Enable backward fill for numeric values\nprocessed = resp_support.waterfall(bfill=True)\n\n# Use a different ID column for grouping\nprocessed = resp_support.waterfall(id_col=\"patient_id\")\n\n# Get just the DataFrame\ndf = resp_support.waterfall(return_dataframe=True)\n\n# Silent mode (no progress messages)\nprocessed = resp_support.waterfall(verbose=False)\n</code></pre>"},{"location":"user-guide/waterfall/#timezone-handling","title":"Timezone Handling","text":"<p>The waterfall function expects data in UTC timezone. If your data is in a different timezone, it will be automatically converted:</p> <pre><code># Your data is in US/Eastern\nprocessed = resp_support.waterfall(verbose=True)\n# Output: \"Converting timezone from US/Eastern to UTC for waterfall processing\"\n</code></pre>"},{"location":"user-guide/waterfall/#processing-pipeline","title":"Processing Pipeline","text":"<p>The waterfall processing consists of five phases:</p>"},{"location":"user-guide/waterfall/#phase-0-initialize-build-hourly-scaffold","title":"Phase 0: Initialize &amp; Build Hourly Scaffold","text":"<ul> <li>Lower-case all text columns - Standardizes device and mode names for consistent matching</li> <li>Coerce numeric setters to floats - Ensures all numeric columns have proper data types</li> <li>Scale FiO\u2082 if needed - Corrects common documentation errors (e.g., 40 \u2192 0.40)</li> <li>Create hourly scaffold - Inserts synthetic rows at HH:59:59 for each hour</li> <li>Ensures every patient has a dense, regular timeline</li> <li>Vital for plots, per-hour metrics, and length-of-stay calculations</li> <li>These scaffold rows serve as \"landing spots\" for forward-fill operations</li> </ul>"},{"location":"user-guide/waterfall/#phase-1-devicemode-heuristics","title":"Phase 1: Device/Mode Heuristics","text":"<p>Applies intelligent rules to repair missing device and mode labels:</p> <ul> <li>IMV from mode strings - Infers invasive mechanical ventilation from mode categories</li> <li>Look-ahead/behind logic - Uses surrounding context to fill gaps</li> <li>Device-specific repairs:</li> <li>BiPAP device name standardization</li> <li>Nasal cannula PEEP guards</li> <li>Mode category inference</li> <li>Data cleaning:</li> <li>Removes rows with no usable information</li> <li>Handles timestamp duplicates</li> <li>Prioritizes non-NIPPV entries when duplicates exist</li> </ul>"},{"location":"user-guide/waterfall/#phase-2-hierarchical-ids","title":"Phase 2: Hierarchical IDs","text":"<p>Creates four nested run-length identifiers within each encounter:</p> <pre><code>device_cat_id \u2192 device_id \u2192 mode_cat_id \u2192 mode_name_id\n</code></pre> <ul> <li>Each ID increments when its label or parent ID changes</li> <li>Enables tracking of ventilation episodes and mode transitions</li> <li>Provides grouping keys for the numeric fill phase</li> </ul> <p>Example progression: <pre><code>Time  Device_Category  Device_Cat_ID  Mode_Category  Mode_Cat_ID\n10:00 IMV             1              AC/VC          1\n11:00 IMV             1              AC/VC          1\n12:00 IMV             1              SIMV           2  \u2190 mode change\n13:00 NIPPV           2              CPAP           3  \u2190 device change\n</code></pre></p>"},{"location":"user-guide/waterfall/#phase-3-numeric-waterfall","title":"Phase 3: Numeric Waterfall","text":"<p>Performs intelligent filling of numeric values within each <code>mode_name_id</code> block:</p> <ul> <li>Forward-fill by default - Carries last known settings forward</li> <li>Optional backward-fill - When <code>bfill=True</code>, also fills backwards</li> <li>Special handling:</li> <li>FiO\u2082 defaults to 0.21 for room air</li> <li>Tidal volume blanked for pressure support modes</li> <li>Trach collar acts as a \"breaker\" for fills</li> <li>Preserves clinical logic - Respects mode transitions and device changes</li> </ul>"},{"location":"user-guide/waterfall/#phase-4-final-tidy-up","title":"Phase 4: Final Tidy-up","text":"<ul> <li>De-duplicate rows - Ensures one row per timestamp</li> <li>Sort chronologically - Orders by encounter and time</li> <li>Forward-fill tracheostomy flag - Carries trach status through entire encounter</li> <li>Clean up helper columns - Removes temporary calculation fields</li> <li>Preserve scaffold indicator - <code>is_scaffold</code> column marks synthetic rows</li> </ul>"},{"location":"user-guide/waterfall/#output-format","title":"Output Format","text":"<p>The processed DataFrame includes:</p>"},{"location":"user-guide/waterfall/#original-columns-cleaned-and-filled","title":"Original Columns (cleaned and filled)","text":"<ul> <li>All original respiratory support columns</li> <li>Numeric values filled within appropriate contexts</li> <li>Categorical values standardized and inferred</li> </ul>"},{"location":"user-guide/waterfall/#new-columns","title":"New Columns","text":"<ul> <li><code>device_cat_id</code> - Device category episode ID</li> <li><code>device_id</code> - Device instance episode ID  </li> <li><code>mode_cat_id</code> - Mode category episode ID</li> <li><code>mode_name_id</code> - Mode instance episode ID</li> <li><code>is_scaffold</code> - Boolean flag for synthetic hourly rows</li> </ul>"},{"location":"user-guide/waterfall/#example","title":"Example","text":"<pre><code>import pandas as pd\nfrom clifpy.tables.respiratory_support import RespiratorySupport\n\n# Sample data with gaps\ndata = pd.DataFrame({\n    'hospitalization_id': ['H001', 'H001', 'H001'],\n    'recorded_dttm': pd.to_datetime([\n        '2023-01-01 10:30', \n        '2023-01-01 14:15',  # 4-hour gap\n        '2023-01-01 15:00'\n    ]).tz_localize('UTC'),\n    'device_category': ['imv', None, 'imv'],  # Missing value\n    'fio2_set': [0.5, None, 0.4],\n    'peep_set': [8, None, 10],\n    # ... other columns\n})\n\n# Create instance and process\nrs = RespiratorySupport(data=data)\nprocessed = rs.waterfall()\n\n# Result will have:\n# - Hourly rows at 11:59:59, 12:59:59, 13:59:59\n# - Device category filled for 14:15 row\n# - FiO\u2082 and PEEP carried forward through gaps\n# - Hierarchical IDs tracking ventilation episodes\n</code></pre>"},{"location":"user-guide/waterfall/#clinical-considerations","title":"Clinical Considerations","text":"<ol> <li>Scaffold rows are synthetic - Filter by <code>is_scaffold == False</code> for actual observations</li> <li>Fills respect clinical boundaries - Values don't cross mode/device transitions</li> <li>Room air defaults - FiO\u2082 set to 0.21 (21%) for room air observations</li> <li>Tracheostomy persistence - Once documented, carries through admission</li> </ol>"},{"location":"user-guide/waterfall/#performance-notes","title":"Performance Notes","text":"<ul> <li>Processing time scales with number of encounters and data density</li> <li>Memory usage increases due to hourly scaffold creation</li> <li>Consider processing in batches for very large datasets</li> </ul>"},{"location":"user-guide/waterfall/#see-also","title":"See Also","text":"<ul> <li>Respiratory Support Table - Table schema and validation</li> <li>Wide Dataset Creation - Creating analysis-ready datasets</li> <li>Data Validation - Understanding validation errors</li> <li>Timezone Handling - Working with different timezones</li> </ul>"},{"location":"user-guide/wide-dataset/","title":"Wide Dataset Creation","text":"<p>The wide dataset functionality enables you to create comprehensive time-series dataset by joining multiple CLIF tables with automatic pivoting and high-performance processing using DuckDB. This feature transforms narrow, category-based data (like vitals and labs) into wide format suitable for machine learning and analysis.</p>"},{"location":"user-guide/wide-dataset/#overview","title":"Overview","text":"<p>Wide dataset creation through the ClifOrchestrator provides:</p> <ul> <li>Automatic table joining across patient, hospitalization, ADT, and optional tables</li> <li>Intelligent pivoting of category-based data (vitals, labs, medications, assessments) </li> <li>High-performance processing using DuckDB for large datasets</li> <li>Memory-efficient batch processing to handle datasets of any size</li> <li>Flexible filtering by hospitalization IDs, time windows, and categories</li> <li>System resource optimization with configurable memory and thread settings</li> </ul> <p>Important: Wide dataset functionality is only available through the <code>ClifOrchestrator</code> and requires specific tables to be loaded.</p>"},{"location":"user-guide/wide-dataset/#quick-start","title":"Quick Start","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize orchestrator\nco = ClifOrchestrator(\n    data_directory='/path/to/clif/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Create wide dataset with sample data\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['hemoglobin', 'sodium', 'glucose']\n    },\n    sample=True  # Use 20 random hospitalizations for testing\n)\n\n# Access the created wide dataset\nwide_df = co.wide_df\n</code></pre>"},{"location":"user-guide/wide-dataset/#accessing-wide-dataset-results","title":"Accessing Wide Dataset Results","text":"<p>The <code>create_wide_dataset()</code> method stores the resulting DataFrame in the orchestrator's <code>wide_df</code> property rather than returning it directly. This approach provides several benefits:</p> <ul> <li>Persistent storage: The wide dataset remains accessible throughout your session</li> <li>Memory management: Avoids creating multiple copies of large DataFrames</li> <li>Consistent access pattern: Aligns with other orchestrator table properties</li> </ul> <pre><code># Create the wide dataset (no return value)\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={'vitals': ['heart_rate'], 'labs': ['hemoglobin']}\n)\n\n# Access the result via the property\nwide_df = co.wide_df\n\n# Check if wide dataset exists\nif co.wide_df is not None:\n    print(f\"Wide dataset shape: {co.wide_df.shape}\")\nelse:\n    print(\"No wide dataset has been created yet\")\n\n# Direct access to the DataFrame\nprint(f\"Hospitalizations: {co.wide_df['hospitalization_id'].nunique()}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#function-parameters","title":"Function Parameters","text":"<p>The <code>create_wide_dataset()</code> method accepts the following parameters:</p> Parameter Type Default Description <code>tables_to_load</code> List[str] None Tables to include: 'vitals', 'labs', 'medication_admin_continuous', 'patient_assessments', 'respiratory_support' <code>category_filters</code> Dict[str, List[str]] None Specific categories to pivot for each table <code>sample</code> bool False If True, randomly select 20 hospitalizations for testing <code>hospitalization_ids</code> List[str] None Specific hospitalization IDs to process <code>cohort_df</code> DataFrame None DataFrame with time windows (requires 'hospitalization_id', 'start_time', 'end_time' columns) <code>output_format</code> str 'dataframe' Output format: 'dataframe', 'csv', or 'parquet' <code>save_to_data_location</code> bool False Save output to data directory <code>output_filename</code> str None Custom filename (auto-generated if None) <code>return_dataframe</code> bool True Store DataFrame in <code>wide_df</code> property even when saving to file <code>batch_size</code> int 1000 Number of hospitalizations per batch (use -1 to disable batching) <code>memory_limit</code> str None DuckDB memory limit (e.g., '8GB', '16GB') <code>threads</code> int None Number of threads for DuckDB processing <code>show_progress</code> bool True Show progress bars for long operations"},{"location":"user-guide/wide-dataset/#best-practices-system-resource-management","title":"Best Practices: System Resource Management","text":"<p>Always check your system resources before running wide dataset creation on large datasets:</p> <pre><code># Check system resources first\nresources = co.get_sys_resource_info()\nprint(f\"Available RAM: {resources['memory_available_gb']:.1f} GB\")\nprint(f\"Recommended threads: {resources['max_recommended_threads']}\")\n\n# Configure based on available resources\nmemory_limit = f\"{int(resources['memory_available_gb'] * 0.7)}GB\"  # Use 70% of available RAM\nthreads = resources['max_recommended_threads']\n\n# Create wide dataset with optimized settings\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['hemoglobin', 'sodium']\n    },\n    memory_limit=memory_limit,\n    threads=threads,\n    batch_size=1000  # Adjust based on dataset size\n)\n\n# Access the created dataset\nwide_df = co.wide_df\n</code></pre>"},{"location":"user-guide/wide-dataset/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/wide-dataset/#example-1-development-and-testing","title":"Example 1: Development and Testing","text":"<p>Use sampling for initial development and testing:</p> <pre><code># Sample mode for development\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'dbp', 'spo2', 'respiratory_rate'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium', 'creatinine']\n    },\n    sample=True,  # Only 20 random hospitalizations\n    show_progress=True\n)\n\n# Access the created dataset\nwide_df = co.wide_df\nprint(f\"Sample dataset shape: {wide_df.shape}\")\nprint(f\"Unique hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-2-production-use-with-resource-optimization","title":"Example 2: Production Use with Resource Optimization","text":"<p>For production use with large datasets:</p> <pre><code># Get system info and configure accordingly\nresources = co.get_sys_resource_info(print_summary=False)\n\n# Production settings\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs', 'medication_admin_continuous'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'dbp', 'spo2', 'temp_c'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium'],\n        'medication_admin_continuous': ['norepinephrine', 'propofol', 'fentanyl']\n    },\n    batch_size=500,  # Smaller batches for large datasets\n    memory_limit=\"12GB\",\n    threads=resources['max_recommended_threads'],\n    save_to_data_location=True,\n    output_format='parquet',\n    output_filename='wide_dataset_production'\n)\n\n# Access the created dataset\nwide_df = co.wide_df\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-3-targeted-analysis-with-specific-ids","title":"Example 3: Targeted Analysis with Specific IDs","text":"<p>Process specific hospitalizations:</p> <pre><code># Analyze specific patient cohort\ntarget_ids = ['12345', '67890', '11111', '22222']\n\nco.create_wide_dataset(\n    hospitalization_ids=target_ids,\n    tables_to_load=['vitals', 'labs', 'patient_assessments'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['lactate', 'hemoglobin'],\n        'patient_assessments': ['gcs_total', 'rass']\n    }\n)\n\n# Access the created dataset\nwide_df = co.wide_df\nprint(f\"Analyzed {len(target_ids)} specific hospitalizations\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-4-time-window-filtering-with-cohort-dataframe","title":"Example 4: Time Window Filtering with Cohort DataFrame","text":"<p>Filter data to specific time windows:</p> <pre><code>import pandas as pd\n\n# Define cohort with time windows\ncohort_df = pd.DataFrame({\n    'hospitalization_id': ['12345', '67890', '11111'],\n    'start_time': ['2023-01-01 08:00:00', '2023-01-05 12:00:00', '2023-01-10 06:00:00'],\n    'end_time': ['2023-01-03 18:00:00', '2023-01-07 20:00:00', '2023-01-12 14:00:00']\n})\n\n# Convert to datetime\ncohort_df['start_time'] = pd.to_datetime(cohort_df['start_time'])\ncohort_df['end_time'] = pd.to_datetime(cohort_df['end_time'])\n\n# Create wide dataset with time filtering\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin', 'sodium']\n    },\n    cohort_df=cohort_df  # Only include data within specified time windows\n)\n\n# Access the created dataset\nwide_df = co.wide_df\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-5-no-batch-processing-for-small-datasets","title":"Example 5: No Batch Processing for Small Datasets","text":"<p>Disable batching for small datasets:</p> <pre><code># Small dataset - process all at once\nco.create_wide_dataset(\n    hospitalization_ids=small_id_list,  # &lt; 100 hospitalizations\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin']\n    },\n    batch_size=-1  # Disable batching\n)\n\n# Access the created dataset\nwide_df = co.wide_df\n</code></pre>"},{"location":"user-guide/wide-dataset/#memory-management-and-batch-processing","title":"Memory Management and Batch Processing","text":""},{"location":"user-guide/wide-dataset/#understanding-batch-processing","title":"Understanding Batch Processing","text":"<p>Batch processing divides hospitalizations into smaller groups to prevent memory (larger-than-memory, OOM) issues:</p> <pre><code># Large dataset - use batching\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    batch_size=1000,  # Process 1000 hospitalizations at a time\n    memory_limit=\"8GB\"\n)\nwide_df = co.wide_df\n\n# Small dataset - disable batching for better performance\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    batch_size=-1  # Process all at once\n)\nwide_df = co.wide_df\n</code></pre>"},{"location":"user-guide/wide-dataset/#memory-optimization-guidelines","title":"Memory Optimization Guidelines","text":"Dataset Size Batch Size Memory Limit Threads &lt; 1,000 hospitalizations -1 (no batching) 4GB 2-4 1,000 - 10,000 hospitalizations 1000 8GB 4-8 &gt; 10,000 hospitalizations 500 16GB+ 6-12"},{"location":"user-guide/wide-dataset/#hourly-aggregation","title":"Hourly Aggregation","text":"<p>Convert the wide dataset to hourly aggregation. The <code>convert_wide_to_hourly</code> method automatically uses the stored wide dataset from <code>create_wide_dataset()</code>:</p> <pre><code># Create aggregation configuration\naggregation_config = {\n    'max': ['sbp', 'map'],\n    'mean': ['heart_rate', 'respiratory_rate'],\n    'min': ['spo2'],\n    'median': ['temp_c'],\n    'first': ['gcs_total', 'rass'],\n    'last': ['assessment_value'],\n    'boolean': ['norepinephrine', 'propofol'],  # 1 if present, 0 if absent\n}\n\n# Convert to hourly (automatically uses stored wide_df)\nhourly_df = co.convert_wide_to_hourly(\n    aggregation_config=aggregation_config,\n    memory_limit='8GB'\n)\n\nprint(f\"Hourly dataset: {hourly_df.shape}\")\nprint(f\"Hour range: {hourly_df['nth_hour'].min()} to {hourly_df['nth_hour'].max()}\")\n\n# Alternative: Explicitly provide wide_df (useful for custom datasets)\n# hourly_df = co.convert_wide_to_hourly(\n#     wide_df=custom_wide_df,\n#     aggregation_config=aggregation_config,\n#     memory_limit='8GB'\n# )\n</code></pre>"},{"location":"user-guide/wide-dataset/#output-structure","title":"Output Structure","text":"<p>The wide dataset includes:</p>"},{"location":"user-guide/wide-dataset/#core-columns","title":"Core Columns","text":"<ul> <li><code>patient_id</code>: Patient identifier</li> <li><code>hospitalization_id</code>: Hospitalization identifier  </li> <li><code>event_time</code>: Timestamp for each event</li> <li><code>day_number</code>: Sequential day within hospitalization</li> <li><code>hosp_id_day_key</code>: Unique hospitalization-daily identifier</li> </ul>"},{"location":"user-guide/wide-dataset/#patient-demographics","title":"Patient Demographics","text":"<ul> <li><code>age_at_admission</code>: Patient age</li> <li>Additional patient table columns</li> </ul>"},{"location":"user-guide/wide-dataset/#adt-information","title":"ADT Information","text":"<ul> <li>Location and transfer data from ADT table</li> </ul>"},{"location":"user-guide/wide-dataset/#pivoted-data-columns","title":"Pivoted Data Columns","text":"<ul> <li>Individual columns for each category (e.g., <code>heart_rate</code>, <code>hemoglobin</code>, <code>norepinephrine</code>) as per use provided in <code>category_filters</code></li> <li>Values aligned by timestamp and hospitalization</li> </ul>"},{"location":"user-guide/wide-dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/wide-dataset/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Memory Errors <pre><code># Solution: Reduce batch size and set memory limit\nco.create_wide_dataset(\n    batch_size=250,  # Smaller batches\n    memory_limit=\"4GB\",  # Conservative limit\n    sample=True  # Test with sample first\n)\nwide_df = co.wide_df\n</code></pre></p> <p>System Crashes <pre><code># Solution: Check resources first and configure accordingly\nresources = co.get_sys_resource_info()\nif resources['memory_available_gb'] &lt; 8:\n    print(\"Warning: Low memory available. Consider using smaller batch_size.\")\n    batch_size = 250\nelse:\n    batch_size = 1000\n</code></pre></p> <p>Empty Results <pre><code># Check if tables and categories exist\nprint(\"Available tables:\", co.get_loaded_tables())\nif hasattr(co, 'vitals') and co.vitals is not None:\n    print(\"Vital categories:\", co.vitals.df['vital_category'].unique())\n</code></pre></p> <p>Slow Performance <pre><code># Use optimize settings\nco.create_wide_dataset(\n    tables_to_load=['vitals'],  # Start with one table\n    category_filters={\n        'vitals': ['heart_rate', 'sbp']  # Limit categories\n    },\n    threads=co.get_sys_resource_info(print_summary=False)['max_recommended_threads']\n)\nwide_df = co.wide_df\n</code></pre></p>"},{"location":"user-guide/wide-dataset/#error-messages","title":"Error Messages","text":"Error Cause Solution \"Memory limit exceeded\" Dataset too large for available RAM Reduce batch_size, set memory_limit \"No event times found\" No data in specified tables/categories Check table data and category filters \"Missing required columns\" cohort_df missing required columns Ensure 'hospitalization_id', 'start_time', 'end_time' columns exist"},{"location":"user-guide/wide-dataset/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>Wide dataset creation integrates seamlessly with existing ClifOrchestrator workflows:</p> <pre><code># Traditional approach\nco = ClifOrchestrator('/path/to/data', 'parquet', 'UTC')\nco.initialize(['patient', 'hospitalization', 'adt', 'vitals', 'labs'])\n\n# Enhanced with wide dataset\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin']\n    }\n)\n\n# Continue with validation and analysis\nco.validate_all()\nanalysis_results = your_analysis_function(co.wide_df)\n</code></pre>"},{"location":"user-guide/wide-dataset/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about data validation to ensure data quality</li> <li>Explore individual table guides for detailed table documentation</li> <li>See the orchestrator guide for advanced orchestrator features</li> <li>Review timezone handling for multi-site data</li> </ul>"},{"location":"user-guide/tables/","title":"CLIF Tables Overview","text":"<p>CLIFpy implements all 9 tables defined in the CLIF 2.0.0 specification. Each table represents a specific aspect of ICU patient data, with standardized columns and validation rules. Detailed CLIF Data Dictionary is available here</p>"},{"location":"user-guide/tables/#data-standards","title":"Data Standards","text":"<p>Each table follows CLIF standards for:</p> <ul> <li> <p>\ud83c\udff7\ufe0f Standardized Categories</p> <p>Consistent values across institutions, validated against permissible value lists, and mapped from institution-specific values.</p> </li> <li> <p>\ud83c\udfe5 Source Preservation</p> <p>Original EHR data elements maintained alongside standardized mappings for institutional transparency.</p> </li> <li> <p>\ud83d\udd52 Timezone Handling</p> <p>All datetime columns are timezone-aware with consistent timezone across all tables and automatic conversion during loading.</p> </li> <li> <p>\ud83d\udd11 Composite Keys</p> <p>Unique record identification with duplicate detection and data integrity validation.</p> </li> </ul>"},{"location":"user-guide/tables/#available-tables","title":"Available Tables","text":"<pre><code>graph TD\n    Patient[Patient] --&gt; |patient_id| Hospitalization[Hospitalization]\n    Hospitalization --&gt; |hospitalization_id| ADT[ADT]\n    Hospitalization --&gt; |hospitalization_id| Labs[Labs]\n    Hospitalization --&gt; |hospitalization_id| Vitals[Vitals]\n    Hospitalization --&gt; |hospitalization_id| Meds[Medications]\n    Hospitalization --&gt; |hospitalization_id| Assess[Assessments]\n    Hospitalization --&gt; |hospitalization_id| Resp[Respiratory Support]\n    Hospitalization --&gt; |hospitalization_id| Pos[Position]</code></pre>"},{"location":"user-guide/tables/#patient","title":"Patient","text":"<p>Core demographic information including birth date, sex, race, ethnicity, and language. This is the primary table that links <code>patient_id</code> field to the <code>hospitalization_id</code> field in the Hospitalization table. For detailed API documentation, see Patient API</p>"},{"location":"user-guide/tables/#adt","title":"ADT","text":"<p>Admission, Discharge, and Transfer events tracking patient movement through different hospital locations (ICU, ward, ED, etc.).</p>"},{"location":"user-guide/tables/#hospitalization","title":"Hospitalization","text":"<p>Hospital admission and discharge information, including admission source, discharge disposition, and length of stay.</p>"},{"location":"user-guide/tables/#labs","title":"Labs","text":"<p>Laboratory test results with standardized categories (chemistry, hematology, etc.) and reference ranges.</p>"},{"location":"user-guide/tables/#microbiology-culture","title":"Microbiology Culture","text":"<p>Microbiology Culture data includes order and result times of microbiology culture tests, the type of fluid collected, the component of the test, and the organism identified.</p>"},{"location":"user-guide/tables/#vitals","title":"Vitals","text":"<p>Vital signs measurements including temperature, heart rate, blood pressure, respiratory rate, and oxygen saturation.</p>"},{"location":"user-guide/tables/#position","title":"Position","text":"<p>Patient positioning data, particularly important for prone positioning in ARDS management.</p>"},{"location":"user-guide/tables/#respiratory-support","title":"Respiratory Support","text":"<p>Ventilation and oxygen therapy data, including device types, settings, and observed values.</p>"},{"location":"user-guide/tables/#medications","title":"Medications","text":"<p>Continuous medication infusions with standardized drug categories and dosing information.</p>"},{"location":"user-guide/tables/#patient-assessments","title":"Patient Assessments","text":"<p>Clinical assessment scores including GCS, RASS, CAM-ICU, pain scores, and other standardized assessments.</p>"},{"location":"user-guide/tables/#common-table-features","title":"Common Table Features","text":"<p>All tables inherit from <code>BaseTable</code> and share these features:</p>"},{"location":"user-guide/tables/#data-loading","title":"Data Loading","text":"<pre><code>table = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"user-guide/tables/#validation","title":"Validation","text":"<pre><code>table.validate()\nif table.isvalid():\n    print(\"Validation passed\")\n</code></pre>"},{"location":"user-guide/tables/#summary-statistics","title":"Summary Statistics","text":"<pre><code>summary = table.get_summary()\nprint(f\"Rows: {summary['num_rows']}\")\nprint(f\"Memory: {summary['memory_usage_mb']} MB\")\n</code></pre>"},{"location":"user-guide/tables/#choosing-tables-for-your-analysis","title":"Choosing Tables for Your Analysis","text":""},{"location":"user-guide/tables/#for-patient-cohort-building","title":"For Patient Cohort Building","text":"<ul> <li>Start with <code>Patient</code> for demographics</li> <li>Add <code>Hospitalization</code> for admission criteria</li> <li>Include <code>ADT</code> for ICU/location-specific cohorts</li> </ul>"},{"location":"user-guide/tables/#for-clinical-outcomes","title":"For Clinical Outcomes","text":"<ul> <li>Use <code>Labs</code> for laboratory markers</li> <li>Add <code>Vitals</code> for physiological parameters</li> <li>Include <code>PatientAssessments</code> for severity scores</li> </ul>"},{"location":"user-guide/tables/#for-treatment-analysis","title":"For Treatment Analysis","text":"<ul> <li>Use <code>RespiratorySupport</code> for ventilation data</li> <li>Add <code>MedicationAdminContinuous</code> for drug therapy</li> <li>Include <code>Position</code> for positioning interventions</li> </ul>"},{"location":"user-guide/tables/#next-steps","title":"Next Steps","text":"<ul> <li>Explore individual table guides for detailed usage</li> <li>Learn about data validation</li> <li>See practical examples</li> <li>Review the API reference</li> </ul>"},{"location":"user-guide/tables/patient/","title":"Patient Table","text":"<p>The Patient table contains core demographic information and serves as the primary reference for all other CLIF tables through the <code>patient_id</code> field.</p>"},{"location":"user-guide/tables/patient/#overview","title":"Overview","text":"<p>The Patient table includes: - Unique patient identifiers - Birth and death dates - Demographics (sex, race, ethnicity) - Primary language</p>"},{"location":"user-guide/tables/patient/#required-columns","title":"Required Columns","text":"Column Type Description patient_id VARCHAR Unique patient identifier birth_date DATETIME Date of birth death_dttm DATETIME Date/time of death (null if alive) race_name VARCHAR Free-text race description race_category VARCHAR Standardized race category ethnicity_name VARCHAR Free-text ethnicity description ethnicity_category VARCHAR Standardized ethnicity category sex_name VARCHAR Free-text sex description sex_category VARCHAR Standardized sex category language_name VARCHAR Primary language language_category VARCHAR Standardized language category"},{"location":"user-guide/tables/patient/#standardized-categories","title":"Standardized Categories","text":""},{"location":"user-guide/tables/patient/#race-categories","title":"Race Categories","text":"<ul> <li>Black or African American</li> <li>White</li> <li>American Indian or Alaska Native</li> <li>Asian</li> <li>Native Hawaiian or Other Pacific Islander</li> <li>Unknown</li> <li>Other</li> </ul>"},{"location":"user-guide/tables/patient/#ethnicity-categories","title":"Ethnicity Categories","text":"<ul> <li>Hispanic</li> <li>Non-Hispanic</li> <li>Unknown</li> </ul>"},{"location":"user-guide/tables/patient/#sex-categories","title":"Sex Categories","text":"<ul> <li>Male</li> <li>Female</li> <li>Unknown</li> </ul>"},{"location":"user-guide/tables/patient/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/tables/patient/#loading-patient-data","title":"Loading Patient Data","text":"<pre><code>from clifpy.tables import Patient\n\n# Load from file\npatient = Patient.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Validate the data\npatient.validate()\n</code></pre>"},{"location":"user-guide/tables/patient/#basic-analysis","title":"Basic Analysis","text":"<pre><code># Get summary statistics\nsummary = patient.get_summary()\nprint(f\"Total patients: {summary['num_rows']}\")\n\n# Demographics distribution\ndemographics = patient.df.groupby(['sex_category', 'race_category']).size()\nprint(demographics)\n\n# Age calculation (if needed)\npatient.df['age'] = (\n    pd.Timestamp.now() - patient.df['birth_date']\n).dt.days / 365.25\n\n# Find elderly patients\nelderly = patient.df[patient.df['age'] &gt;= 65]\n</code></pre>"},{"location":"user-guide/tables/patient/#cohort-building","title":"Cohort Building","text":"<pre><code># Female patients over 65\ncohort = patient.df[\n    (patient.df['sex_category'] == 'Female') &amp; \n    (patient.df['age'] &gt;= 65)\n]\n\n# Living patients\nalive = patient.df[patient.df['death_dttm'].isna()]\n\n# Specific ethnicity\nhispanic = patient.df[patient.df['ethnicity_category'] == 'Hispanic']\n</code></pre>"},{"location":"user-guide/tables/patient/#joining-with-other-tables","title":"Joining with Other Tables","text":"<pre><code># Get patient demographics for lab results\nlabs_with_demographics = labs.df.merge(\n    patient.df[['patient_id', 'age', 'sex_category']],\n    on='patient_id',\n    how='left'\n)\n\n# Analyze by demographic groups\nlab_by_sex = labs_with_demographics.groupby('sex_category')['lab_value'].mean()\n</code></pre>"},{"location":"user-guide/tables/patient/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Check for missing demographics\nmissing_sex = patient.df[patient.df['sex_category'].isna()]\nmissing_race = patient.df[patient.df['race_category'].isna()]\n\n# Validate age ranges\npatient.df['age'] = (pd.Timestamp.now() - patient.df['birth_date']).dt.days / 365.25\ninvalid_age = patient.df[(patient.df['age'] &lt; 0) | (patient.df['age'] &gt; 120)]\n\n# Check death date consistency\ninvalid_death = patient.df[\n    patient.df['death_dttm'] &lt; patient.df['birth_date']\n]\n</code></pre>"},{"location":"user-guide/tables/patient/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate demographic categories against standardized values</li> <li>Handle missing data appropriately for demographic fields</li> <li>Calculate age at time of admission, not current date</li> <li>Protect PHI by using only de-identified patient_ids</li> <li>Document any demographic data transformations</li> </ol>"},{"location":"user-guide/tables/patient/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see Patient API</p>"}]}