{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>CLIFpy is a Python package that implements the Common Longitudinal ICU data Format (CLIF) specification. It provides a standardized interface for working with critical care data in the CLIF format, enabling healthcare researchers and data scientists to analyze ICU data across different healthcare systems.</p> <ul> <li> <p>\ud83d\ude80 Getting Started</p> <p>Install with <code>pip</code> and get started with CLIFpy with these tutorials.</p> </li> <li> <p>\ud83d\udcd6 User Guide</p> <p>In depth explanation and discussion of the concepts and working of different features available in CLIFpy.</p> </li> <li> <p>\ud83d\udd28 How-to Guides</p> <p>Practical guides to help you achieve specific goals. Take a look at these guides to learn how to use CLIFpy to solve real-world problems.</p> </li> <li> <p>\ud83d\udcc4 API Reference</p> <p>Technical descriptions of how CLIFpy classes and methods work.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>CLIFpy is released under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing to CLIFpy","text":"<p>We welcome contributions to CLIFpy! This guide will help you get started.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/CLIFpy.git\ncd CLIFpy\n</code></pre></li> <li>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></li> <li>Install in development mode with all dependencies:    <pre><code>pip install -e \".[docs]\"\n</code></pre></li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a new branch for your feature or fix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and ensure:    - Code follows the existing style    - All tests pass    - New features include tests    - Documentation is updated</p> </li> <li> <p>Run tests:    <pre><code>pytest tests/\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"feat: add new feature\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add type hints where appropriate</li> <li>Include docstrings for all public functions and classes</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update docstrings for any API changes</li> <li>Add examples to docstrings where helpful</li> <li>Update user guide if adding new features</li> <li>Build docs locally to verify:   <pre><code>mkdocs serve\n</code></pre></li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure all tests pass before submitting PR</li> <li>Aim for high test coverage</li> <li>Use pytest fixtures for common test data</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits format: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation changes - <code>test:</code> - Test additions or changes - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance tasks</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue for bugs or feature requests</li> <li>Join discussions in existing issues</li> <li>Reach out to maintainers if you need help</li> </ul> <p>Thank you for contributing to CLIFpy!</p>"},{"location":"logging/","title":"Logging System Documentation","text":""},{"location":"logging/#overview","title":"Overview","text":"<p>The <code>clifpy</code> package uses a centralized logging system that provides:</p> <ul> <li>Dual log files: Separate files for all events and errors-only</li> <li>Console output: Maintains familiar print()-like user experience</li> <li>Emoji formatting: Visual indicators for quick log level identification</li> <li>Automatic setup: Logging initializes automatically when using ClifOrchestrator</li> <li>Hierarchical loggers: Organized namespace (<code>clifpy.*</code>) for all modules</li> </ul>"},{"location":"logging/#how-it-works-two-key-functions","title":"How It Works: Two Key Functions","text":"<p>The logging system uses two functions with distinct purposes:</p>"},{"location":"logging/#setup_logging-configure-the-system-call-once","title":"<code>setup_logging()</code> - Configure the System (Call Once)","text":"<p>This function configures the entire logging infrastructure: - Creates log files (<code>clifpy_all.log</code>, <code>clifpy_errors.log</code>) - Sets up console output - Configures emoji formatting - Determines where and how logs are saved</p> <p>Call this once at your application's entry point.</p>"},{"location":"logging/#get_logger-get-a-logger-call-anywhere","title":"<code>get_logger()</code> - Get a Logger (Call Anywhere)","text":"<p>This function simply retrieves a logger instance. It does not configure anything.</p> <p>Where Each Is Used:</p> Called In Function Used Purpose <code>ClifOrchestrator.__init__()</code> <code>setup_logging()</code> Entry point - configures logging <code>BaseTable.__init__()</code> <code>setup_logging()</code> Entry point - configures logging <code>utils/wide_dataset.py</code> <code>get_logger()</code> Utility - just needs a logger <code>utils/sofa.py</code> <code>get_logger()</code> Utility - just needs a logger <code>utils/io.py</code> <code>get_logger()</code> Utility - just needs a logger <code>utils/config.py</code> <code>get_logger()</code> Utility - just needs a logger <p>Why utility modules don't call <code>setup_logging()</code>:</p> <p>When you create a <code>ClifOrchestrator</code>, it calls <code>setup_logging()</code> internally. By the time utility modules like <code>wide_dataset.py</code> run, logging is already configured. They just need to retrieve their logger with <code>get_logger()</code>.</p> <pre><code># User's script\nclif = ClifOrchestrator(...)  # \u2190 setup_logging() called here\n\n# Later, when you call this:\nwide_df = clif.create_wide_dataset(...)  # \u2190 wide_dataset.py just uses get_logger()\n</code></pre>"},{"location":"logging/#logger-namespaces-and-why-they-matter","title":"Logger Namespaces and Why They Matter","text":""},{"location":"logging/#what-are-logger-names","title":"What Are Logger Names?","text":"<p>Each logger has a hierarchical name (like <code>clifpy.utils.sofa</code> or <code>clifpy.tables.labs</code>). Think of it like a file path - the dots create a parent-child relationship.</p>"},{"location":"logging/#the-logger-hierarchy","title":"The Logger Hierarchy","text":"<pre><code>clifpy                              \u2190 Root logger (configured by setup_logging)\n\u251c\u2500\u2500 clifpy.ClifOrchestrator         \u2190 Inherits config from parent\n\u251c\u2500\u2500 clifpy.tables                   \u2190 Inherits config from parent\n\u2502   \u251c\u2500\u2500 clifpy.tables.labs          \u2190 Inherits config from grandparent\n\u2502   \u251c\u2500\u2500 clifpy.tables.vitals        \u2190 Inherits config from grandparent\n\u2502   \u2514\u2500\u2500 clifpy.tables.meds          \u2190 Inherits config from grandparent\n\u2514\u2500\u2500 clifpy.utils                    \u2190 Inherits config from parent\n    \u251c\u2500\u2500 clifpy.utils.wide_dataset   \u2190 Inherits config from grandparent\n    \u251c\u2500\u2500 clifpy.utils.sofa           \u2190 Inherits config from grandparent\n    \u2514\u2500\u2500 clifpy.utils.io             \u2190 Inherits config from grandparent\n</code></pre> <p>Key concept: When <code>setup_logging()</code> configures the <code>'clifpy'</code> root logger, all child loggers (<code>clifpy.*</code>) automatically inherit that configuration. This is why utility modules don't need to call <code>setup_logging()</code> - they inherit everything from the parent.</p>"},{"location":"logging/#why-logger-names-matter","title":"Why Logger Names Matter","text":"<p>Logger names provide several practical benefits:</p>"},{"location":"logging/#1-identify-where-logs-come-from","title":"1. Identify Where Logs Come From","text":"<p>Each log message shows exactly which module generated it:</p> <pre><code>2025-10-13 10:30:15 | \ud83d\udce2 INFO | clifpy.ClifOrchestrator | Starting analysis\n2025-10-13 10:30:16 | \ud83d\udce2 INFO | clifpy.utils.wide_dataset | Loading tables\n2025-10-13 10:30:17 | \u274c ERROR | clifpy.tables.labs | Missing column: creatinine\n2025-10-13 10:30:18 | \ud83d\udce2 INFO | clifpy.utils.sofa | Computing SOFA scores\n</code></pre> <p>Without names, you'd just see messages with no indication of which file or module has a problem.</p>"},{"location":"logging/#2-control-verbosity-for-specific-modules","title":"2. Control Verbosity for Specific Modules","text":"<p>You can make specific parts of your code more or less verbose:</p> <pre><code>import logging\n\n# Make ONLY sofa.py show detailed debug messages\nlogging.getLogger('clifpy.utils.sofa').setLevel(logging.DEBUG)\n\n# Quiet down the noisy wide_dataset.py (warnings only)\nlogging.getLogger('clifpy.utils.wide_dataset').setLevel(logging.WARNING)\n\n# Everything else stays at INFO level (default)\n</code></pre> <p>Real scenario: You're debugging SOFA calculations but don't care about all the verbose wide dataset processing logs. Just make SOFA verbose and keep everything else quiet.</p>"},{"location":"logging/#3-control-entire-subsystems","title":"3. Control Entire Subsystems","text":"<p>Hierarchical names let you control entire groups at once:</p> <pre><code>import logging\n\n# Silence ALL table-related logging\nlogging.getLogger('clifpy.tables').setLevel(logging.ERROR)\n\n# This automatically affects all child loggers:\n#   - clifpy.tables.labs\n#   - clifpy.tables.vitals\n#   - clifpy.tables.medication_admin_continuous\n#   - All other tables\n</code></pre>"},{"location":"logging/#4-search-and-filter-logs","title":"4. Search and Filter Logs","text":"<p>Use grep to find logs from specific modules:</p> <pre><code># Find all SOFA calculation logs\ngrep \"clifpy.utils.sofa\" output/logs/clifpy_all.log\n\n# Find all table loading issues\ngrep \"clifpy.tables\" output/logs/clifpy_errors.log\n\n# Find all wide dataset processing\ngrep \"clifpy.utils.wide_dataset\" output/logs/clifpy_all.log\n</code></pre>"},{"location":"logging/#5-trace-execution-flow","title":"5. Trace Execution Flow","text":"<p>Follow your code's execution path across multiple modules:</p> <pre><code>10:30:15 | clifpy.ClifOrchestrator      | Starting wide dataset creation\n10:30:16 | clifpy.utils.wide_dataset    | Loading labs table\n10:30:17 | clifpy.tables.labs           | Loaded 50,000 records\n10:30:18 | clifpy.utils.wide_dataset    | Pivoting labs data\n10:30:19 | clifpy.utils.wide_dataset    | ERROR: Pivot failed\n</code></pre> <p>You can see the execution flow: orchestrator \u2192 wide_dataset \u2192 labs \u2192 back to wide_dataset \u2192 error.</p>"},{"location":"logging/#log-file-structure","title":"Log File Structure","text":"<p>All logs are stored in the <code>output/logs/</code> subdirectory:</p> <pre><code>output/\n\u251c\u2500\u2500 logs/\n\u2502   \u251c\u2500\u2500 clifpy_all.log          # All INFO+ messages\n\u2502   \u251c\u2500\u2500 clifpy_errors.log        # Only WARNING+ messages\n\u2502   \u251c\u2500\u2500 validation_log_*.log     # Per-table validation logs (supplementary)\n</code></pre>"},{"location":"logging/#log-file-contents","title":"Log File Contents","text":""},{"location":"logging/#clifpy_alllog","title":"<code>clifpy_all.log</code>","text":"<p>Contains all informational messages, warnings, and errors. Use this for: - Debugging processing steps - Understanding data flow - Tracking what operations were performed - Performance analysis</p> <p>Example: <pre><code>2025-10-13 10:30:15 | \u2705 INFO     | clifpy.ClifOrchestrator | [create_wide_dataset:520] | \ud83d\ude80 WIDE DATASET CREATION STARTED\n2025-10-13 10:30:16 | \u2705 INFO     | clifpy.tables.labs | [validate:145] | \u2713 All required columns present\n2025-10-13 10:30:17 | \u26a0\ufe0f WARNING  | clifpy.utils.io | [convert_datetime_columns_to_site_tz:191] | event_dttm: Naive datetime localized to US/Central\n</code></pre></p>"},{"location":"logging/#clifpy_errorslog","title":"<code>clifpy_errors.log</code>","text":"<p>Contains only warnings and errors. Use this to: - Quickly identify problems - Review issues without reading through info logs - Troubleshoot failures</p> <p>Example: <pre><code>2025-10-13 10:30:17 | \u26a0\ufe0f WARNING  | clifpy.utils.io | [convert_datetime_columns_to_site_tz:191] | event_dttm: Naive datetime localized to US/Central\n2025-10-13 10:31:45 | \u274c ERROR    | clifpy.tables.vitals | [validate:152] | Missing required columns: ['heart_rate']\n</code></pre></p>"},{"location":"logging/#emoji-legend","title":"Emoji Legend","text":"<p>Logs use emojis for quick visual parsing:</p> Level Emoji When Used DEBUG \ud83d\udc1b Detailed internal operations, variable values INFO \ud83d\udce2 Normal operations, progress updates WARNING \u26a0\ufe0f Potential issues, missing optional data ERROR \u274c Failures, validation errors CRITICAL \ud83c\udd98 Severe failures requiring immediate attention"},{"location":"logging/#usage","title":"Usage","text":""},{"location":"logging/#automatic-setup-recommended","title":"Automatic Setup (Recommended)","text":"<p>When using <code>ClifOrchestrator</code>, logging is configured automatically:</p> <pre><code>from clifpy import ClifOrchestrator\n\n# Logging automatically initializes when creating orchestrator\nclif = ClifOrchestrator(\n    data_directory=\"./data\",\n    filetype=\"parquet\",\n    timezone=\"US/Central\",\n    output_directory=\"./output\"  # Logs go to ./output/logs/\n)\n\n# All operations now log automatically\nclif.load_table(\"labs\")\nclif.create_wide_dataset(tables_to_include=[\"labs\", \"vitals\"])\n</code></pre>"},{"location":"logging/#manual-setup","title":"Manual Setup","text":"<p>For standalone scripts or custom workflows:</p> <pre><code>from clifpy.utils.logging_config import setup_logging, get_logger\n\n# Initialize centralized logging\nsetup_logging(output_directory=\"./output\")\n\n# Get a logger for your module\nlogger = get_logger('my_analysis')\n\n# Use the logger\nlogger.info(\"Starting custom analysis\")\nlogger.warning(\"Missing optional parameter, using default\")\nlogger.error(\"Failed to process data\")\n</code></pre>"},{"location":"logging/#understanding-get_logger-vs-logginggetlogger","title":"Understanding <code>get_logger()</code> vs <code>logging.getLogger()</code>","text":"<p>The <code>get_logger()</code> function is a convenience wrapper that ensures your logger inherits the centralized configuration. Here's what it does:</p> <pre><code># Using our wrapper (recommended) \u2705\nfrom clifpy.utils.logging_config import get_logger\nlogger = get_logger('my_analysis')\n# \u2192 Creates logger named 'clifpy.my_analysis' (automatically prefixed!)\n# \u2192 Inherits all configuration (log files, console, emojis)\n\n# Direct call with full prefix \u2705\nimport logging\nlogger = logging.getLogger('clifpy.my_analysis')\n# \u2192 Creates logger named 'clifpy.my_analysis' (manual prefix)\n# \u2192 Inherits all configuration\n\n# Direct call WITHOUT prefix \u274c\nimport logging\nlogger = logging.getLogger('my_analysis')\n# \u2192 Creates logger named 'my_analysis' (root level, no prefix)\n# \u2192 Does NOT inherit clifpy configuration\n# \u2192 Uses Python's default logging (no files, no emojis)\n</code></pre> <p>Why the prefix matters:</p> <p>Only loggers whose names start with <code>'clifpy.'</code> inherit the centralized configuration. The <code>get_logger()</code> wrapper automatically adds this prefix, so you don't have to remember it.</p> <p>Best practice: Use <code>get_logger()</code> to ensure your logger is properly configured.</p>"},{"location":"logging/#configuration-options","title":"Configuration Options","text":"<pre><code>setup_logging(\n    output_directory=\"./output\",      # Base directory (logs go in output/logs/)\n    level=logging.INFO,                # Minimum level to capture\n    console_output=True,               # Show messages in console\n    separate_error_log=True            # Create separate error log file\n)\n</code></pre>"},{"location":"logging/#log-levels-guide","title":"Log Levels Guide","text":""},{"location":"logging/#when-to-use-each-level","title":"When to Use Each Level","text":"<p>DEBUG - Detailed diagnostics for development: <pre><code>logger.debug(f\"Processing batch {i} of {total_batches}\")\nlogger.debug(f\"Query: {sql_query}\")\nlogger.debug(f\"Intermediate result shape: {df.shape}\")\n</code></pre></p> <p>INFO - Normal operation progress: <pre><code>logger.info(\"Loading patient data\")\nlogger.info(f\"Loaded {len(df)} records\")\nlogger.info(\"\u2705 Validation complete\")\n</code></pre></p> <p>WARNING - Potential issues that don't stop execution: <pre><code>logger.warning(\"Missing optional column 'weight_kg', using defaults\")\nlogger.warning(\"No data found for hospitalization_id=12345\")\nlogger.warning(f\"Outlier values detected: {outlier_count} records\")\n</code></pre></p> <p>ERROR - Failures that prevent operation completion: <pre><code>logger.error(f\"Missing required columns: {missing_cols}\")\nlogger.error(\"File not found: {file_path}\")\nlogger.error(\"Data validation failed\")\n</code></pre></p> <p>CRITICAL - Severe failures requiring immediate action: <pre><code>logger.critical(\"Database connection lost\")\nlogger.critical(\"Insufficient memory to process dataset\")\n</code></pre></p>"},{"location":"logging/#common-workflows","title":"Common Workflows","text":""},{"location":"logging/#reviewing-processing-results","title":"Reviewing Processing Results","text":"<p>After running data processing:</p> <ol> <li>Check console output for high-level progress and warnings</li> <li>Review <code>clifpy_errors.log</code> for any issues</li> <li>Check <code>clifpy_all.log</code> if you need detailed processing steps</li> </ol>"},{"location":"logging/#debugging-issues","title":"Debugging Issues","text":"<p>When something goes wrong:</p> <ol> <li> <p>Start with <code>clifpy_errors.log</code>:    <pre><code>cat output/logs/clifpy_errors.log\n</code></pre></p> </li> <li> <p>Search for specific patterns:    <pre><code>grep \"ERROR\" output/logs/clifpy_all.log\ngrep \"hospitalization_id=12345\" output/logs/clifpy_all.log\n</code></pre></p> </li> <li> <p>Check table-specific validation:    <pre><code>cat output/logs/validation_log_labs.log\n</code></pre></p> </li> </ol>"},{"location":"logging/#adjusting-log-verbosity","title":"Adjusting Log Verbosity","text":"<p>For more detailed logs during development:</p> <pre><code>import logging\nfrom clifpy import ClifOrchestrator\n\nclif = ClifOrchestrator(\n    data_directory=\"./data\",\n    filetype=\"parquet\",\n    timezone=\"US/Central\",\n    output_directory=\"./output\"\n)\n\n# Enable DEBUG level for more details\nsetup_logging(output_directory=\"./output\", level=logging.DEBUG)\n</code></pre> <p>For quieter logs (warnings/errors only):</p> <pre><code>setup_logging(\n    output_directory=\"./output\",\n    level=logging.WARNING,\n    console_output=True  # Still show warnings in console\n)\n</code></pre>"},{"location":"logging/#per-table-validation-logs","title":"Per-Table Validation Logs","text":"<p>In addition to the centralized logs, each table creates a supplementary validation log:</p> <pre><code>output/logs/validation_log_labs.log\noutput/logs/validation_log_vitals.log\noutput/logs/validation_log_medication_admin_continuous.log\n</code></pre> <p>These logs contain: - Column validation results - Data type checks - Required field presence - Table-specific validation rules</p> <p>Note: These are supplementary - validation messages also appear in the main <code>clifpy_all.log</code> and <code>clifpy_errors.log</code> files.</p>"},{"location":"logging/#best-practices","title":"Best Practices","text":""},{"location":"logging/#1-use-appropriate-log-levels","title":"1. Use Appropriate Log Levels","text":"<ul> <li>Don't overuse ERROR for warnings</li> <li>Use DEBUG for verbose internal details</li> <li>INFO should tell the \"story\" of what's happening</li> </ul>"},{"location":"logging/#2-include-context-in-messages","title":"2. Include Context in Messages","text":"<pre><code># Good - includes context\nlogger.info(f\"Processing {len(df)} records for {table_name}\")\n\n# Less helpful\nlogger.info(\"Processing records\")\n</code></pre>"},{"location":"logging/#3-log-important-parameters","title":"3. Log Important Parameters","text":"<pre><code>logger.info(f\"Starting SOFA calculation with extremal_type='{extremal_type}'\")\nlogger.info(f\"Cohort filtering: {len(cohort_df)} hospitalizations\")\n</code></pre>"},{"location":"logging/#4-use-structured-sections","title":"4. Use Structured Sections","text":"<pre><code>logger.info(\"=\" * 50)\nlogger.info(\"\ud83d\ude80 ANALYSIS STARTED\")\nlogger.info(\"=\" * 50)\n# ... processing ...\nlogger.info(\"\u2705 ANALYSIS COMPLETED\")\n</code></pre>"},{"location":"logging/#5-clean-up-logs-between-runs","title":"5. Clean Up Logs Between Runs","text":"<p>Log files are overwritten on each run (mode='w'), so previous runs are automatically cleaned up.</p>"},{"location":"logging/#integration-with-existing-code","title":"Integration with Existing Code","text":"<p>The logging system integrates with all existing <code>clifpy</code> modules:</p> Module Logger Name Purpose <code>ClifOrchestrator</code> <code>clifpy.ClifOrchestrator</code> High-level workflow orchestration <code>tables.*</code> <code>clifpy.tables.{table_name}</code> Table loading and validation <code>utils.wide_dataset</code> <code>clifpy.utils.wide_dataset</code> Wide dataset creation <code>utils.sofa</code> <code>clifpy.utils.sofa</code> SOFA score calculation <code>utils.io</code> <code>clifpy.utils.io</code> File I/O operations <code>utils.config</code> <code>clifpy.utils.config</code> Configuration loading <p>All modules use the same centralized configuration and write to the same log files.</p>"},{"location":"logging/#troubleshooting","title":"Troubleshooting","text":""},{"location":"logging/#logs-not-appearing","title":"Logs Not Appearing","text":"<p>Issue: No log files created</p> <p>Solution: Ensure <code>output_directory</code> is writable: <pre><code>import os\noutput_dir = \"./output/logs\"\nos.makedirs(output_dir, exist_ok=True)\n</code></pre></p>"},{"location":"logging/#console-output-missing","title":"Console Output Missing","text":"<p>Issue: Not seeing messages in terminal</p> <p>Solution: Ensure <code>console_output=True</code>: <pre><code>setup_logging(output_directory=\"./output\", console_output=True)\n</code></pre></p>"},{"location":"logging/#too-verbose-too-quiet","title":"Too Verbose / Too Quiet","text":"<p>Issue: Too many/few messages</p> <p>Solution: Adjust the log level: <pre><code>import logging\n\n# More verbose\nsetup_logging(level=logging.DEBUG)\n\n# Less verbose\nsetup_logging(level=logging.WARNING)\n</code></pre></p>"},{"location":"logging/#duplicate-log-messages","title":"Duplicate Log Messages","text":"<p>Issue: Same message appears multiple times</p> <p>Solution: Avoid calling <code>setup_logging()</code> multiple times in custom code. The system is designed to be idempotent, but it's best to call it once at the start of your script.</p>"},{"location":"logging/#examples","title":"Examples","text":""},{"location":"logging/#example-1-basic-analysis-script","title":"Example 1: Basic Analysis Script","text":"<pre><code>from clifpy import ClifOrchestrator, setup_logging, get_logger\n\n# Initialize logging\nsetup_logging(output_directory=\"./my_analysis/output\")\n\n# Get a custom logger for your script\nlogger = get_logger('my_analysis')\n\nlogger.info(\"Starting sepsis analysis\")\n\n# Create orchestrator (inherits logging configuration)\nclif = ClifOrchestrator(\n    data_directory=\"./data\",\n    filetype=\"parquet\",\n    timezone=\"US/Central\",\n    output_directory=\"./my_analysis/output\"\n)\n\n# All operations are logged automatically\nlogger.info(\"Loading clinical tables\")\nclif.load_table(\"labs\")\nclif.load_table(\"vitals\")\n\nlogger.info(\"Creating wide dataset\")\nwide_df = clif.create_wide_dataset(\n    tables_to_include=[\"labs\", \"vitals\"]\n)\n\nlogger.info(f\"\u2705 Analysis complete - processed {len(wide_df)} records\")\n</code></pre>"},{"location":"logging/#example-2-custom-processing-with-detailed-logging","title":"Example 2: Custom Processing with Detailed Logging","text":"<pre><code>import logging\nfrom clifpy import setup_logging, get_logger\nfrom clifpy.utils.sofa import compute_sofa\n\n# Enable DEBUG level for detailed tracking\nsetup_logging(output_directory=\"./output\", level=logging.DEBUG)\n\nlogger = get_logger('sofa_analysis')\n\nlogger.info(\"=\" * 50)\nlogger.info(\"SOFA Score Calculation\")\nlogger.info(\"=\" * 50)\n\nlogger.debug(f\"Input dataset shape: {wide_df.shape}\")\nlogger.debug(f\"Columns: {wide_df.columns.tolist()}\")\n\n# Compute SOFA scores\nsofa_df = compute_sofa(\n    wide_df,\n    id_name='hospitalization_id',\n    extremal_type='worst'\n)\n\nlogger.info(f\"Computed SOFA scores for {len(sofa_df)} hospitalizations\")\nlogger.debug(f\"SOFA score distribution:\\n{sofa_df['sofa_total'].describe()}\")\n\n# Check results\nlogger.info(f\"Logs saved to: output/logs/clifpy_all.log\")\nlogger.info(f\"Error log: output/logs/clifpy_errors.log\")\n</code></pre>"},{"location":"logging/#example-3-quiet-mode-errors-only","title":"Example 3: Quiet Mode (Errors Only)","text":"<pre><code>import logging\nfrom clifpy import ClifOrchestrator, setup_logging\n\n# Only show warnings and errors\nsetup_logging(\n    output_directory=\"./output\",\n    level=logging.WARNING,\n    console_output=True\n)\n\nclif = ClifOrchestrator(\n    data_directory=\"./data\",\n    filetype=\"parquet\",\n    timezone=\"US/Central\",\n    output_directory=\"./output\"\n)\n\n# Console will only show warnings/errors\n# All info messages still go to clifpy_all.log\nclif.load_table(\"labs\")\nclif.create_wide_dataset(tables_to_include=[\"labs\"])\n</code></pre>"},{"location":"logging/#summary","title":"Summary","text":"<p>The <code>clifpy</code> logging system provides:</p> <ul> <li>\u2705 Automatic logging for all operations</li> <li>\u2705 Dual log files (all events + errors-only)</li> <li>\u2705 Console output for real-time feedback</li> <li>\u2705 Emoji formatting for readability</li> <li>\u2705 Organized structure in <code>output/logs/</code> directory</li> <li>\u2705 Flexible configuration for different use cases</li> </ul> <p>No additional setup required when using <code>ClifOrchestrator</code> - just review the logs in <code>output/logs/</code> after running your analysis!</p>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the complete API documentation for CLIFpy, automatically generated from the source code docstrings.</p>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#cliforchestrator","title":"ClifOrchestrator","text":"<p>The main orchestration class for managing multiple CLIF tables with consistent configuration.</p>"},{"location":"api/#basetable","title":"BaseTable","text":"<p>The base class that all CLIF table implementations inherit from, providing common functionality for data loading, validation, and reporting.</p>"},{"location":"api/#table-classes","title":"Table Classes","text":""},{"location":"api/#tables-overview","title":"Tables Overview","text":"<p>Complete API documentation for all CLIF table implementations:</p> <ul> <li>Patient - Patient demographics and identification</li> <li>Adt - Admission, discharge, and transfer events  </li> <li>Hospitalization - Hospital stay information</li> <li>Labs - Laboratory test results</li> <li>Vitals - Vital signs measurements</li> <li>RespiratorySupport - Ventilation and oxygen therapy</li> <li>MedicationAdminContinuous - Continuous medication infusions</li> <li>PatientAssessments - Clinical assessment scores</li> <li>Position - Patient positioning data</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":""},{"location":"api/#utility-functions","title":"Utility Functions","text":"<p>Helper functions for data processing, validation, and specialized operations:</p> <ul> <li>stitch_encounters - Link related hospitalizations within time windows</li> <li>process_resp_support_waterfall - Respiratory support waterfall algorithm</li> <li>io - Data loading and sample creation utilities</li> <li>config - Configuration management functions</li> <li>validator - Data validation functions</li> <li>outlier_handler - Outlier detection and handling</li> <li>wide_dataset - Wide dataset creation utilities</li> </ul>"},{"location":"api/#quick-links","title":"Quick Links","text":"<ul> <li>ClifOrchestrator API - Multi-table management</li> <li>BaseTable API - Common table functionality</li> <li>Table Classes API - Individual table implementations</li> <li>Utilities API - Helper functions</li> </ul>"},{"location":"api/#usage-example","title":"Usage Example","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\nfrom clifpy.tables import Patient, Labs, Vitals\n\n# Using the orchestrator\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Using individual tables\npatient = Patient.from_file('/path/to/data', 'parquet')\npatient.validate()\n</code></pre>"},{"location":"api/base-table/","title":"BaseTable","text":""},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable","title":"clifpy.tables.base_table.BaseTable","text":"<pre><code>BaseTable(\n    data_directory,\n    filetype,\n    timezone,\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>Base class for all pyCLIF table classes.</p> <p>Provides common functionality for loading data, running validations, and generating reports. All table-specific classes should inherit from this.</p> <p>Attributes:</p> Name Type Description <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>table_name</code> <code>str</code> <p>Name of the table (from class name)</p> <code>df</code> <code>DataFrame</code> <p>The loaded data</p> <code>schema</code> <code>dict</code> <p>The YAML schema for this table</p> <code>errors</code> <code>List[dict]</code> <p>Validation errors from last validation run</p> <code>logger</code> <code>Logger</code> <p>Logger for this table</p> <p>Initialize the BaseTable.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> required <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> required <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> required <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not provided, creates an 'output' directory in the current working directory.</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def __init__(\n    self, \n    data_directory: str,\n    filetype: str,\n    timezone: str,\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the BaseTable.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs.\n        If not provided, creates an 'output' directory in the current working directory.\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Store configuration\n    self.data_directory = data_directory\n    self.filetype = filetype\n    self.timezone = timezone\n\n    # Set output directory\n    if output_directory is None:\n        output_directory = os.path.join(os.getcwd(), 'output')\n    self.output_directory = output_directory\n    os.makedirs(self.output_directory, exist_ok=True)\n\n    # Initialize centralized logging\n    setup_logging(output_directory=self.output_directory)\n\n    # Derive snake_case table name from PascalCase class name\n    # Example: Adt -&gt; adt, RespiratorySupport -&gt; respiratory_support\n    self.table_name = ''.join(['_' + c.lower() if c.isupper() else c for c in self.__class__.__name__]).lstrip('_')\n\n    # Initialize data and validation state\n    self.df: Optional[pd.DataFrame] = data\n    self.errors: List[Dict[str, Any]] = []\n    self.schema: Optional[Dict[str, Any]] = None\n    self.outlier_config: Optional[Dict[str, Any]] = None\n    self._validated: bool = False\n\n    # Setup table-specific logging\n    self._setup_logging()\n\n    # Load schema\n    self._load_schema()\n\n    # Load outlier config\n    self._load_outlier_config()\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.analyze_categorical_distributions","title":"analyze_categorical_distributions","text":"<pre><code>analyze_categorical_distributions(save=True)\n</code></pre> <p>Analyze distributions of categorical variables.</p> <p>For each categorical variable, returns the distribution of categories based on unique hospitalization_id (or patient_id if hospitalization_id is not present).</p> <p>Parameters:</p> Name Type Description Default <code>save</code> <code>bool</code> <p>If True, saves distribution data to CSV files in the output directory.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dictionary where keys are categorical column names and values are DataFrames with category distributions (unique ID counts and %).</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def analyze_categorical_distributions(self, save: bool = True) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Analyze distributions of categorical variables.\n\n    For each categorical variable, returns the distribution of categories\n    based on unique hospitalization_id (or patient_id if hospitalization_id is not present).\n\n    Parameters\n    ----------\n    save : bool, default=True\n        If True, saves distribution data to CSV files in the output directory.\n\n    Returns\n    -------\n    Dict[str, pd.DataFrame]\n        Dictionary where keys are categorical column names and values are\n        DataFrames with category distributions (unique ID counts and %).\n    \"\"\"\n    if self.df is None:\n        self.logger.warning(\"No dataframe to analyze\")\n        return {}\n\n    if not self.schema:\n        self.logger.warning(\"No schema available for categorical analysis\")\n        return {}\n\n    # Determine ID column to use (prefer hospitalization_id)\n    if 'hospitalization_id' in self.df.columns:\n        id_col = 'hospitalization_id'\n    elif 'patient_id' in self.df.columns:\n        id_col = 'patient_id'\n    else:\n        self.logger.warning(\"No hospitalization_id or patient_id column found\")\n        return {}\n\n    # Get categorical columns from schema\n    categorical_columns = [\n        col['name'] for col in self.schema.get('columns', [])\n        if col.get('is_category_column', False) and col['name'] in self.df.columns\n    ]\n\n    if not categorical_columns:\n        self.logger.info(\"No categorical columns found in schema\")\n        return {}\n\n    results = {}\n\n    for col in categorical_columns:\n        try:\n            # Count unique IDs per category\n            id_counts = self.df.groupby(col, dropna=False)[id_col].nunique().sort_values(ascending=False)\n            # Calculate % as (unique IDs in category) / (total unique IDs in entire table)\n            total_unique_ids = self.df[id_col].nunique()\n            percent = (id_counts / total_unique_ids * 100).round(2)\n\n            distribution_df = pd.DataFrame({\n                'category': id_counts.index,\n                'count': id_counts.values,\n                '%': percent.values\n            })\n\n            results[col] = distribution_df\n\n            # Save to CSV if requested\n            if save:\n                csv_filename = f'categorical_dist_{self.table_name}_{col}.csv'\n                csv_path = os.path.join(self.output_directory, csv_filename)\n                distribution_df.to_csv(csv_path, index=False)\n                self.logger.info(f\"Saved distribution data to {csv_path}\")\n\n            self.logger.info(f\"Analyzed categorical distribution for {col}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error analyzing categorical distribution for {col}: {str(e)}\")\n            continue\n\n    return results\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.calculate_stratified_ecdf","title":"calculate_stratified_ecdf","text":"<pre><code>calculate_stratified_ecdf(\n    value_column,\n    category_column,\n    category_values=None,\n    save=True,\n)\n</code></pre> <p>Calculate ECDF for a continuous variable stratified by categories using loaded DataFrame (self.df).</p> <p>Parameters:</p> Name Type Description Default <code>value_column</code> <code>str</code> <p>Name of the continuous/numeric column to calculate ECDF for.</p> required <code>category_column</code> <code>str</code> <p>Name of the categorical column to stratify by.</p> required <code>category_values</code> <code>List[str]</code> <p>Specific category values to include. If None, uses permissible values from schema, or all unique values in the data if schema doesn't specify permissible values.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, saves stratified ECDF data to CSV files (one per category).</p> <code>True</code> <p>Returns:</p> Type Description <code>List[DataFrame] or None</code> <p>List of DataFrames (one per category), each with x-values and their corresponding cumulative probabilities. If save=True, saves the resulting DataFrame to CSV.</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def calculate_stratified_ecdf(\n    self,\n    value_column: str,\n    category_column: str,\n    category_values: Optional[List[str]] = None,\n    save: bool = True\n) -&gt; Optional[List['pl.DataFrame']]:\n    \"\"\"\n    Calculate ECDF for a continuous variable stratified by categories using loaded DataFrame (self.df).\n\n    Parameters\n    ----------\n    value_column : str\n        Name of the continuous/numeric column to calculate ECDF for.\n    category_column : str\n        Name of the categorical column to stratify by.\n    category_values : List[str], optional\n        Specific category values to include. If None, uses permissible values from schema,\n        or all unique values in the data if schema doesn't specify permissible values.\n    save : bool, default=True\n        If True, saves stratified ECDF data to CSV files (one per category).\n\n    Returns\n    -------\n    List[pl.DataFrame] or None\n        List of DataFrames (one per category), each with x-values and their corresponding cumulative probabilities.\n        If save=True, saves the resulting DataFrame to CSV.\n    \"\"\"\n    import polars as pl\n\n    # Check if self.df is loaded\n    if self.df is None:\n        self.logger.error(\"Loaded dataframe (self.df) is not available.\")\n        return None\n\n    # Convert to Polars DataFrame if it's not already\n    if not isinstance(self.df, pl.DataFrame):\n        try:\n            df_pl = pl.from_pandas(self.df)\n        except Exception as e:\n            self.logger.error(f\"Could not convert self.df to Polars DataFrame: {str(e)}\")\n            return None\n    else:\n        df_pl = self.df\n\n    # Check if columns exist\n    columns = df_pl.columns\n    if value_column not in columns:\n        self.logger.error(f\"Value column '{value_column}' not found in dataframe\")\n        return None\n    if category_column not in columns:\n        self.logger.error(f\"Category column '{category_column}' not found in dataframe\")\n        return None\n\n    # Determine which category values to use\n    if category_values is None:\n        # Try permissible values from schema\n        category_values = None\n        if self.schema:\n            for col_def in self.schema.get('columns', []):\n                if col_def.get('name') == category_column:\n                    category_values = col_def.get('permissible_values')\n                    if category_values:\n                        self.logger.info(f\"Using permissible values from schema for {category_column}\")\n                    break\n        # Otherwise use all unique values from data\n        if not category_values:\n            category_values = (\n                df_pl\n                .select(pl.col(category_column).drop_nulls().unique())\n                .to_series()\n                .to_list()\n            )\n            self.logger.info(f\"Using all unique values from data for {category_column}\")\n\n    all_ecdf_rows = []\n\n    for category in category_values:\n        try:\n            # Filter data for this category\n            cat_df = (\n                df_pl\n                .filter(pl.col(category_column) == category)\n                .select([pl.col(value_column)])\n                .drop_nulls()\n                .sort(value_column)\n            )\n\n            n = cat_df.shape[0]\n            if n == 0:\n                self.logger.warning(f\"No valid data for category '{category}'\")\n                continue\n\n            # Calculate ECDF: each value gets rank = position, cumulative_probability = rank/n\n            ecdf_df = cat_df.with_columns([\n                (pl.arange(1, n + 1) / n).alias('cumulative_probability'),\n            ])\n            # Add category for later clarity\n            ecdf_df = ecdf_df.with_columns([\n                pl.lit(category).alias(category_column)\n            ])\n\n            all_ecdf_rows.append(ecdf_df)\n\n            self.logger.info(f\"Calculated ECDF for {category_column}={category} with {n} measurements\")\n\n        except Exception as e:\n            self.logger.error(f\"Error calculating ECDF for category '{category}': {str(e)}\")\n            continue\n\n    if not all_ecdf_rows:\n        self.logger.warning(\"No valid ECDF data for any category.\")\n        return None\n\n    # Concatenate all\n    all_ecdf_pl = pl.concat(all_ecdf_rows)\n\n    if save:\n        csv_filename = f'ecdf_{self.table_name}_{value_column}_by_{category_column}.csv'\n        csv_path = os.path.join(self.output_directory, csv_filename)\n        try:\n            all_ecdf_pl.write_csv(csv_path)\n            self.logger.info(f\"Saved ECDF data for all categories to {csv_path}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to save ECDF CSV: {str(e)}\")\n\n    return all_ecdf_rows\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    data_directory=None,\n    filetype=None,\n    timezone=None,\n    config_path=None,\n    output_directory=None,\n    sample_size=None,\n    columns=None,\n    filters=None,\n    verbose=False,\n)\n</code></pre> <p>Load data from file and create a table instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>None</code> <code>config_path</code> <code>str</code> <p>Path to configuration JSON file</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of rows to load</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>Specific columns to load</p> <code>None</code> <code>filters</code> <code>Dict</code> <p>Filters to apply when loading</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, show detailed loading messages. Default is False</p> <code>False</code> Notes <p>Loading priority:     1. If all required params provided \u2192 use them     2. If config_path provided \u2192 load from that path, allow param overrides     3. If no params and no config_path \u2192 auto-detect config.json     4. Parameters override config file values when both are provided</p> <p>Returns:</p> Type Description <code>BaseTable</code> <p>Instance of the table class with loaded data</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    data_directory: Optional[str] = None,\n    filetype: Optional[str] = None,\n    timezone: Optional[str] = None,\n    config_path: Optional[str] = None,\n    output_directory: Optional[str] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None,\n    verbose: bool = False\n) -&gt; 'BaseTable':\n    \"\"\"\n    Load data from file and create a table instance.\n\n    Parameters\n    ----------\n    data_directory : str, optional\n        Path to the directory containing data files\n    filetype : str, optional\n        Type of data file (csv, parquet, etc.)\n    timezone : str, optional\n        Timezone for datetime columns\n    config_path : str, optional\n        Path to configuration JSON file\n    output_directory : str, optional\n        Directory for saving output files and logs\n    sample_size : int, optional\n        Number of rows to load\n    columns : List[str], optional\n        Specific columns to load\n    filters : Dict, optional\n        Filters to apply when loading\n    verbose : bool, optional\n        If True, show detailed loading messages. Default is False\n\n    Notes\n    -----\n    Loading priority:\n        1. If all required params provided \u2192 use them\n        2. If config_path provided \u2192 load from that path, allow param overrides\n        3. If no params and no config_path \u2192 auto-detect config.json\n        4. Parameters override config file values when both are provided\n\n    Returns\n    -------\n    BaseTable\n        Instance of the table class with loaded data\n    \"\"\"\n    # Get configuration from config file or parameters\n    config = get_config_or_params(\n        config_path=config_path,\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory\n    )\n\n    # Derive snake_case table name from PascalCase class name\n    table_name = ''.join(['_' + c.lower() if c.isupper() else c for c in cls.__name__]).lstrip('_')\n\n    # Load data using existing io utility\n    data = load_data(\n        table_name,\n        config['data_directory'],\n        config['filetype'],\n        sample_size=sample_size,\n        columns=columns,\n        filters=filters,\n        site_tz=config['timezone'],\n        verbose=verbose\n    )\n\n    # Create instance with loaded data\n    return cls(\n        data_directory=config['data_directory'],\n        filetype=config['filetype'],\n        timezone=config['timezone'],\n        output_directory=config.get('output_directory', output_directory),\n        data=data\n    )\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.get_summary","title":"get_summary","text":"<pre><code>get_summary()\n</code></pre> <p>Get a summary of the table data.</p> <p>Returns:     dict: Summary statistics and information about the table</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get a summary of the table data.\n\n    Returns:\n        dict: Summary statistics and information about the table\n    \"\"\"\n    if self.df is None:\n        return {\"status\": \"No data loaded\"}\n\n    summary = {\n        \"table_name\": self.table_name,\n        \"num_rows\": len(self.df),\n        \"num_columns\": len(self.df.columns),\n        \"columns\": list(self.df.columns),\n        \"memory_usage_mb\": self.df.memory_usage(deep=True).sum() / 1024 / 1024,\n        \"validation_run\": self._validated,\n        \"validation_errors\": len(self.errors) if self._validated else None,\n        \"is_valid\": self.isvalid()\n    }\n\n    # Add basic statistics for numeric columns\n    numeric_cols = self.df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) &gt; 0:\n        summary[\"numeric_columns\"] = list(numeric_cols)\n        summary[\"numeric_stats\"] = self.df[numeric_cols].describe().to_dict()\n\n    # Add missing data summary\n    missing_counts = self.df.isnull().sum()\n    if missing_counts.any():\n        summary[\"missing_data\"] = missing_counts[missing_counts &gt; 0].to_dict()\n\n    return summary\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Check if the data is valid based on the last validation run.</p> <p>Returns:     bool: True if validation has been run and no errors were found,           False if validation found errors or hasn't been run yet</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"\n    Check if the data is valid based on the last validation run.\n\n    Returns:\n        bool: True if validation has been run and no errors were found,\n              False if validation found errors or hasn't been run yet\n    \"\"\"\n    if not self._validated:\n        self.logger.warning(\"Validation has not been run yet. Please call validate() first.\")\n        return False\n    return not self.errors\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.plot_categorical_distributions","title":"plot_categorical_distributions","text":"<pre><code>plot_categorical_distributions(\n    columns=None, figsize=(10, 6), save=True, dpi=300\n)\n</code></pre> <p>Create bar plots for categorical variable distributions.</p> <p>Counts unique hospitalization_id (or patient_id if hospitalization_id is not present) for each category.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[str]</code> <p>Specific categorical columns to plot. If None, plots all categorical columns.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for each plot (width, height).</p> <code>(10, 6)</code> <code>save</code> <code>bool</code> <p>If True, saves plots to output directory as PNG files.</p> <code>True</code> <code>dpi</code> <code>int</code> <p>Resolution for saved plots (dots per inch).</p> <code>300</code> <p>Returns:</p> Type Description <code>Dict[str, Figure]</code> <p>Dictionary where keys are categorical column names and values are matplotlib Figure objects.</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def plot_categorical_distributions(self, columns: Optional[List[str]] = None, figsize: Tuple[int, int] = (10, 6), save: bool = True, dpi: int = 300):\n    \"\"\"\n    Create bar plots for categorical variable distributions.\n\n    Counts unique hospitalization_id (or patient_id if hospitalization_id is not present)\n    for each category.\n\n    Parameters\n    ----------\n    columns : List[str], optional\n        Specific categorical columns to plot. If None, plots all categorical columns.\n    figsize : Tuple[int, int], default=(10, 6)\n        Figure size for each plot (width, height).\n    save : bool, default=True\n        If True, saves plots to output directory as PNG files.\n    dpi : int, default=300\n        Resolution for saved plots (dots per inch).\n\n    Returns\n    -------\n    Dict[str, Figure]\n        Dictionary where keys are categorical column names and values are\n        matplotlib Figure objects.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if self.df is None:\n        self.logger.warning(\"No dataframe to plot\")\n        return {}\n\n    if not self.schema:\n        self.logger.warning(\"No schema available for categorical plotting\")\n        return {}\n\n    # Determine ID column to use (prefer hospitalization_id)\n    if 'hospitalization_id' in self.df.columns:\n        id_col = 'hospitalization_id'\n    elif 'patient_id' in self.df.columns:\n        id_col = 'patient_id'\n    else:\n        self.logger.warning(\"No hospitalization_id or patient_id column found\")\n        return {}\n\n    # Get categorical columns from schema\n    categorical_columns = [\n        col['name'] for col in self.schema.get('columns', [])\n        if col.get('is_category_column', False) and col['name'] in self.df.columns\n    ]\n\n    if not categorical_columns:\n        self.logger.info(\"No categorical columns found in schema\")\n        return {}\n\n    # Filter to requested columns if specified\n    if columns is not None:\n        categorical_columns = [col for col in categorical_columns if col in columns]\n\n    if not categorical_columns:\n        self.logger.warning(\"No matching categorical columns found\")\n        return {}\n\n    plots = {}\n\n    for col in categorical_columns:\n        try:\n            # Count unique IDs per category\n            id_counts = self.df.groupby(col, dropna=False)[id_col].nunique().sort_values(ascending=False)\n\n            # Create modern bar plot\n            fig, ax = plt.subplots(figsize=figsize, facecolor='white')\n\n            # Use colorblind-friendly color palette (cividis)\n            colors = plt.cm.cividis(np.linspace(0.3, 0.9, len(id_counts)))\n            bars = ax.bar(range(len(id_counts)), id_counts.values, color=colors, edgecolor='white', linewidth=1.5)\n\n            # Styling\n            ax.set_xlabel('Category', fontsize=12, fontweight='bold', color='#333333')\n            ax.set_ylabel(f'Unique {id_col} counts', fontsize=12, fontweight='bold', color='#333333')\n            ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold', pad=20, color='#1a1a1a')\n            ax.set_xticks(range(len(id_counts)))\n            ax.set_xticklabels([str(x) for x in id_counts.index], rotation=45, ha='right', fontsize=10)\n\n            # Remove top and right spines\n            ax.spines['top'].set_visible(False)\n            ax.spines['right'].set_visible(False)\n            ax.spines['left'].set_color('#cccccc')\n            ax.spines['bottom'].set_color('#cccccc')\n\n            # Add grid for readability\n            ax.yaxis.grid(True, linestyle='--', alpha=0.3, color='#cccccc')\n            ax.set_axisbelow(True)\n\n            # Add value labels on top of bars (adjust font size and rotation based on number of categories)\n            num_categories = len(id_counts)\n            if num_categories &lt;= 10:\n                label_fontsize = 9\n                label_rotation = 0\n            elif num_categories &lt;= 20:\n                label_fontsize = 7\n                label_rotation = 45\n            else:\n                label_fontsize = 6\n                label_rotation = 90\n\n            for i, (bar, value) in enumerate(zip(bars, id_counts.values)):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                       f'{int(value)}',\n                       ha='center', va='bottom', fontsize=label_fontsize,\n                       color='#333333', rotation=label_rotation)\n\n            plt.tight_layout()\n\n            # Save plot if requested\n            if save:\n                plot_filename = f'categorical_dist_{self.table_name}_{col}.png'\n                plot_path = os.path.join(self.output_directory, plot_filename)\n                fig.savefig(plot_path, dpi=dpi, bbox_inches='tight')\n                self.logger.info(f\"Saved plot to {plot_path}\")\n\n            plots[col] = fig\n\n            self.logger.info(f\"Created plot for {col}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error creating plot for {col}: {str(e)}\")\n            continue\n\n    return plots\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.save_summary","title":"save_summary","text":"<pre><code>save_summary()\n</code></pre> <p>Save table summary to a JSON file.</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def save_summary(self):\n    \"\"\"Save table summary to a JSON file.\"\"\"\n    try:\n        import json\n\n        summary = self.get_summary()\n\n        # Save to JSON\n        summary_file = os.path.join(\n            self.output_directory,\n            f'summary_{self.table_name}.json'\n        )\n\n        with open(summary_file, 'w') as f:\n            json.dump(summary, f, indent=2, default=str)\n\n        self.logger.info(f\"Saved summary to {summary_file}\")\n\n    except Exception as e:\n        self.logger.error(f\"Error saving summary: {str(e)}\")\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Run comprehensive validation on the data.</p> <p>This method runs all validation checks including:</p> <ul> <li>Schema validation (required columns, data types, categories)</li> <li>Missing data analysis</li> <li>Duplicate checking</li> <li>Statistical analysis</li> <li>Table-specific validations (if overridden in child class)</li> </ul> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Run comprehensive validation on the data.\n\n    This method runs all validation checks including:\n\n    - Schema validation (required columns, data types, categories)\n    - Missing data analysis\n    - Duplicate checking\n    - Statistical analysis\n    - Table-specific validations (if overridden in child class)\n    \"\"\"\n    if self.df is None:\n        self.logger.warning(\"No dataframe to validate\")\n        return\n\n    self.logger.info(\"Starting validation\")\n    self.errors = []\n    self._validated = True\n\n    try:\n        # Run basic schema validation\n        if self.schema:\n            self.logger.info(\"Running schema validation\")\n            schema_errors = validator.validate_dataframe(self.df, self.schema)\n            self.errors.extend(schema_errors)\n\n            if schema_errors:\n                self.logger.warning(f\"Schema validation found {len(schema_errors)} errors\")\n            else:\n                self.logger.info(\"Schema validation passed\")\n\n        # Run enhanced validations (these will be implemented in Phase 3)\n        self._run_enhanced_validations()\n\n        # Run table-specific validations (can be overridden in child classes)\n        self._run_table_specific_validations()\n\n        # Log validation results\n        if not self.errors:\n            self.logger.info(\"Validation completed successfully\")\n        else:\n            self.logger.warning(f\"Validation completed with {len(self.errors)} error(s). See `errors` attribute.\")\n\n            # Save errors to CSV\n            self._save_validation_errors()\n\n    except Exception as e:\n        self.logger.error(f\"Error during validation: {str(e)}\")\n        self.errors.append({\n            \"type\": \"validation_error\",\n            \"message\": str(e)\n        })\n</code></pre>"},{"location":"api/orchestrator/","title":"ClifOrchestrator","text":""},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator","title":"clifpy.clif_orchestrator.ClifOrchestrator","text":"<pre><code>ClifOrchestrator(\n    config_path=None,\n    data_directory=None,\n    filetype=None,\n    timezone=None,\n    output_directory=None,\n    stitch_encounter=False,\n    stitch_time_interval=6,\n)\n</code></pre> <p>Orchestrator class for managing multiple CLIF table objects.</p> <p>This class provides a centralized interface for loading, managing, and validating multiple CLIF tables with consistent configuration.</p> <p>Attributes:</p> Name Type Description <code>config_path</code> <code>(str, optional)</code> <p>Path to configuration JSON file</p> <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>stitch_encounter</code> <code>bool</code> <p>Whether to stitch encounters within time interval</p> <code>stitch_time_interval</code> <code>int</code> <p>Hours between discharge and next admission to consider encounters linked</p> <code>encounter_mapping</code> <code>DataFrame</code> <p>Mapping of hospitalization_id to encounter_block (after stitching)</p> <code>patient</code> <code>Patient</code> <p>Patient table object</p> <code>hospitalization</code> <code>Hospitalization</code> <p>Hospitalization table object</p> <code>adt</code> <code>Adt</code> <p>ADT table object</p> <code>labs</code> <code>Labs</code> <p>Labs table object</p> <code>vitals</code> <code>Vitals</code> <p>Vitals table object</p> <code>medication_admin_continuous</code> <code>MedicationAdminContinuous</code> <p>Medication administration continuous table object</p> <code>medication_admin_intermittent</code> <code>MedicationAdminIntermittent</code> <p>Medication administration intermittent table object</p> <code>patient_assessments</code> <code>PatientAssessments</code> <p>Patient assessments table object</p> <code>respiratory_support</code> <code>RespiratorySupport</code> <p>Respiratory support table object</p> <code>position</code> <code>Position</code> <p>Position table object</p> <code>hospital_diagnosis</code> <code>HospitalDiagnosis</code> <p>Hospital diagnosis table object</p> <code>microbiology_culture</code> <code>MicrobiologyCulture</code> <p>Microbiology culture table object</p> <code>crrt_therapy</code> <code>CrrtTherapy</code> <p>CRRT therapy table object</p> <code>patient_procedures</code> <code>PatientProcedures</code> <p>Patient procedures table object</p> <code>microbiology_susceptibility</code> <code>MicrobiologySusceptibility</code> <p>Microbiology susceptibility table object</p> <code>ecmo_mcs</code> <code>EcmoMcs</code> <p>ECMO/MCS table object</p> <code>microbiology_nonculture</code> <code>MicrobiologyNonculture</code> <p>Microbiology non-culture table object</p> <code>code_status</code> <code>CodeStatus</code> <p>Code status table object</p> <code>wide_df</code> <code>DataFrame</code> <p>Wide dataset with time-series data (populated by create_wide_dataset)</p> <p>Initialize the ClifOrchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to configuration JSON file</p> <code>None</code> <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not provided, creates an 'output' directory in the current working directory.</p> <code>None</code> <code>stitch_encounter</code> <code>bool</code> <p>Whether to stitch encounters within time interval. Default False.</p> <code>False</code> <code>stitch_time_interval</code> <code>int</code> <p>Hours between discharge and next admission to consider  encounters linked. Default 6 hours.</p> <code>6</code> Notes <p>Loading priority:</p> <ol> <li>If all required params provided \u2192 use them</li> <li>If config_path provided \u2192 load from that path, allow param overrides</li> <li>If no params and no config_path \u2192 auto-detect config.json</li> <li>Parameters override config file values when both are provided</li> </ol> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def __init__(\n    self,\n    config_path: Optional[str] = None,\n    data_directory: Optional[str] = None,\n    filetype: Optional[str] = None,\n    timezone: Optional[str] = None,\n    output_directory: Optional[str] = None,\n    stitch_encounter: bool = False,\n    stitch_time_interval: int = 6\n):\n    \"\"\"\n    Initialize the ClifOrchestrator.\n\n    Parameters\n    ----------\n    config_path : str, optional\n        Path to configuration JSON file\n    data_directory : str, optional\n        Path to the directory containing data files\n    filetype : str, optional\n        Type of data file (csv, parquet, etc.)\n    timezone : str, optional\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs.\n        If not provided, creates an 'output' directory in the current working directory.\n    stitch_encounter : bool, optional\n        Whether to stitch encounters within time interval. Default False.\n    stitch_time_interval : int, optional\n        Hours between discharge and next admission to consider \n        encounters linked. Default 6 hours.\n\n    Notes\n    -----\n    Loading priority:\n\n    1. If all required params provided \u2192 use them\n    2. If config_path provided \u2192 load from that path, allow param overrides\n    3. If no params and no config_path \u2192 auto-detect config.json\n    4. Parameters override config file values when both are provided\n    \"\"\"\n    # Get configuration from config file or parameters\n    config = get_config_or_params(\n        config_path=config_path,\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory\n    )\n\n    self.data_directory = config['data_directory']\n    self.filetype = config['filetype']\n    self.timezone = config['timezone']\n\n    # Set output directory\n    self.output_directory = config.get('output_directory')\n    if self.output_directory is None:\n        self.output_directory = os.path.join(os.getcwd(), 'output')\n    os.makedirs(self.output_directory, exist_ok=True)\n\n    # Initialize centralized logging\n    setup_logging(output_directory=self.output_directory)\n\n    # Get logger for orchestrator\n    self.logger = logging.getLogger('clifpy.ClifOrchestrator')\n\n    # Set stitching parameters\n    self.stitch_encounter = stitch_encounter\n    self.stitch_time_interval = stitch_time_interval\n    self.encounter_mapping = None\n\n    # Initialize all table attributes to None\n    self.patient: Patient = None\n    self.hospitalization: Hospitalization = None\n    self.adt: Adt = None\n    self.labs: Labs = None\n    self.vitals: Vitals = None\n    self.medication_admin_continuous: MedicationAdminContinuous = None\n    self.medication_admin_intermittent: MedicationAdminIntermittent = None\n    self.patient_assessments: PatientAssessments = None\n    self.respiratory_support: RespiratorySupport = None\n    self.position: Position = None\n    self.hospital_diagnosis: HospitalDiagnosis = None\n    self.microbiology_culture: MicrobiologyCulture = None\n    self.crrt_therapy: CrrtTherapy = None\n    self.patient_procedures: PatientProcedures = None\n    self.microbiology_susceptibility: MicrobiologySusceptibility = None\n    self.ecmo_mcs: EcmoMcs = None\n    self.microbiology_nonculture: MicrobiologyNonculture = None\n    self.code_status: CodeStatus = None\n\n    # Initialize wide dataset property\n    self.wide_df: Optional[pd.DataFrame] = None\n\n    self.logger.info('ClifOrchestrator initialized')\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.compute_sofa_scores","title":"compute_sofa_scores","text":"<pre><code>compute_sofa_scores(\n    wide_df=None,\n    cohort_df=None,\n    extremal_type=\"worst\",\n    id_name=\"encounter_block\",\n    fill_na_scores_with_zero=True,\n    remove_outliers=True,\n    create_new_wide_df=True,\n)\n</code></pre> <p>Compute SOFA (Sequential Organ Failure Assessment) scores.</p> <p>Parameters:     wide_df: Optional wide dataset. If not provided, uses self.wide_df or creates one     cohort_df: Optional DataFrame with columns [id_name, 'start_time', 'end_time']               to further filter observations by time windows     extremal_type: 'worst' (default) or 'latest' (future feature)     id_name: Column name for grouping (default: 'encounter_block')             - 'encounter_block': Groups related hospitalizations (requires encounter stitching)             - 'hospitalization_id': Individual hospitalizations     fill_na_scores_with_zero: If True, missing component scores default to 0     remove_outliers: If True, overwrite the df of the table object associated with the orchestrator with outliers nullified     create_new_wide_df: If True, create a new wide dataset for SOFA computation and save it at .wide_df_sofa.          If False, use the existing .wide_df.</p> <p>Returns:     DataFrame with SOFA component scores and total score for each ID.     Results are stored in self.sofa_df.</p> <p>Notes:     - Medication units should be pre-converted (e.g., 'norepinephrine_mcg_kg_min')     - When id_name='encounter_block' and encounter mapping doesn't exist,       it will be created automatically via run_stitch_encounters()     - Missing data defaults to score of 0 (normal organ function)</p> <p>Examples:     Basic usage:     &gt;&gt;&gt; co = ClifOrchestrator(config_path='config/config.yaml')     &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores()</p> <pre><code>Per hospitalization instead of encounter:\n&gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(id_name='hospitalization_id')\n\nWith time filtering:\n&gt;&gt;&gt; cohort_df = pd.DataFrame({\n...     'encounter_block': ['E001', 'E002'],\n...     'start_time': pd.to_datetime(['2024-01-01', '2024-01-02']),\n...     'end_time': pd.to_datetime(['2024-01-03', '2024-01-04'])\n... })\n&gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(cohort_df=cohort_df)\n</code></pre> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def compute_sofa_scores(\n    self,\n    wide_df: Optional[pd.DataFrame] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    extremal_type: str = 'worst',\n    id_name: str = 'encounter_block',\n    fill_na_scores_with_zero: bool = True,\n    remove_outliers: bool = True,\n    create_new_wide_df: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute SOFA (Sequential Organ Failure Assessment) scores.\n\n    Parameters:\n        wide_df: Optional wide dataset. If not provided, uses self.wide_df or creates one\n        cohort_df: Optional DataFrame with columns [id_name, 'start_time', 'end_time']\n                  to further filter observations by time windows\n        extremal_type: 'worst' (default) or 'latest' (future feature)\n        id_name: Column name for grouping (default: 'encounter_block')\n                - 'encounter_block': Groups related hospitalizations (requires encounter stitching)\n                - 'hospitalization_id': Individual hospitalizations\n        fill_na_scores_with_zero: If True, missing component scores default to 0\n        remove_outliers: If True, overwrite the df of the table object associated with the orchestrator with outliers nullified\n        create_new_wide_df: If True, create a new wide dataset for SOFA computation and save it at .wide_df_sofa. \n            If False, use the existing .wide_df.\n\n    Returns:\n        DataFrame with SOFA component scores and total score for each ID.\n        Results are stored in self.sofa_df.\n\n    Notes:\n        - Medication units should be pre-converted (e.g., 'norepinephrine_mcg_kg_min')\n        - When id_name='encounter_block' and encounter mapping doesn't exist,\n          it will be created automatically via run_stitch_encounters()\n        - Missing data defaults to score of 0 (normal organ function)\n\n    Examples:\n        Basic usage:\n        &gt;&gt;&gt; co = ClifOrchestrator(config_path='config/config.yaml')\n        &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores()\n\n        Per hospitalization instead of encounter:\n        &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(id_name='hospitalization_id')\n\n        With time filtering:\n        &gt;&gt;&gt; cohort_df = pd.DataFrame({\n        ...     'encounter_block': ['E001', 'E002'],\n        ...     'start_time': pd.to_datetime(['2024-01-01', '2024-01-02']),\n        ...     'end_time': pd.to_datetime(['2024-01-03', '2024-01-04'])\n        ... })\n        &gt;&gt;&gt; sofa_scores = co.compute_sofa_scores(cohort_df=cohort_df)\n    \"\"\"\n    from .utils.sofa import compute_sofa, REQUIRED_SOFA_CATEGORIES_BY_TABLE\n\n    self.logger.info(f\"Computing SOFA scores with extremal_type='{extremal_type}', id_name='{id_name}'\")\n\n    if (cohort_df is not None) and (id_name not in cohort_df.columns):\n        raise ValueError(f\"id_name '{id_name}' not found in cohort_df columns\")\n\n    # Determine which wide_df to use\n    if wide_df is not None:\n        self.logger.debug(\"Using provided wide_df\")\n        df = wide_df\n    elif create_new_wide_df:\n        self.logger.info(\"Ignoring any existing .wide_df and creating a new wide dataset for SOFA computation\")\n        df = self.create_wide_dataset(\n            tables_to_load=list(REQUIRED_SOFA_CATEGORIES_BY_TABLE.keys()),\n            category_filters=REQUIRED_SOFA_CATEGORIES_BY_TABLE,\n            cohort_df=cohort_df,\n            return_dataframe=True\n        )\n        df = self.wide_df if df is None else df\n        self.wide_df_sofa = df\n    elif hasattr(self, 'wide_df') and self.wide_df is not None:\n        self.logger.debug(\"Using existing self.wide_df\")\n        df = self.wide_df\n    else:\n        self.logger.info(\"No wide dataset available, creating one...\")\n        # Create wide dataset with required categories for SOFA\n\n        self.create_wide_dataset(\n            tables_to_load=list(REQUIRED_SOFA_CATEGORIES_BY_TABLE.keys()),\n            category_filters=REQUIRED_SOFA_CATEGORIES_BY_TABLE,\n            cohort_df=cohort_df\n        )\n        df = self.wide_df\n        self.logger.debug(f\"Created wide dataset with shape: {df.shape}\")\n\n    if id_name not in df.columns:\n        if self.encounter_mapping is None:\n            self.logger.info(\"Encounter mapping not found, running stitch_encounters()\")\n            try:\n                self.run_stitch_encounters()\n            except Exception as e:\n                self.logger.error(f\"Error during encounter stitching: {e}\")\n                raise ValueError(\"Encounter stitching failed. Please run stitch_encounters() manually.\")\n        df = df.merge(self.encounter_mapping, on='hospitalization_id', how='left')\n        self.wide_df = df\n        self.logger.debug(f\"Mapped {id_name} to wide_df via encounter_mapping, with shape: {df.shape}\")\n\n    # Compute SOFA scores\n    self.logger.debug(\"Calling compute_sofa function\")\n    sofa_scores = compute_sofa(\n        wide_df=df,\n        cohort_df=cohort_df,\n        extremal_type=extremal_type,\n        id_name=id_name,\n        fill_na_scores_with_zero=fill_na_scores_with_zero,\n        remove_outliers=remove_outliers\n    )\n\n    # Store results in orchestrator\n    self.sofa_df = sofa_scores\n    self.logger.info(f\"SOFA computation completed. Results stored in self.sofa_df with shape: {sofa_scores.shape}\")\n\n    return sofa_scores\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_dose_units_for_continuous_meds","title":"convert_dose_units_for_continuous_meds","text":"<pre><code>convert_dose_units_for_continuous_meds(\n    preferred_units,\n    vitals_df=None,\n    hospitalization_ids=None,\n    show_intermediate=False,\n    override=False,\n    save_to_table=True,\n)\n</code></pre> <p>Convert dose units for continuous medication data.</p> <p>Parameters:</p> Name Type Description Default <code>preferred_units</code> <code>Dict[str, str]</code> <p>Dict of preferred units for each medication category</p> required <code>vitals_df</code> <code>DataFrame</code> <p>Vitals DataFrame for extracting patient weights</p> <code>None</code> <code>hospitalization_ids</code> <code>List[str]</code> <p>List of specific hospitalization IDs to filter and process. When provided, only medication and vitals data for these hospitalizations will be loaded and processed, improving performance for targeted analyses.</p> <code>None</code> <code>show_intermediate</code> <code>bool</code> <p>If True, includes intermediate calculation columns in output</p> <code>False</code> <code>override</code> <code>bool</code> <p>If True, continues processing with warnings for unacceptable units</p> <code>False</code> <code>save_to_table</code> <code>bool</code> <p>If True, saves the converted DataFrame to the table's df_converted property and stores conversion_counts as a table property. If False, returns the converted data without updating the table.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame] or None</code> <p>(converted_df, counts_df) when save_to_table=False, None otherwise</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_dose_units_for_continuous_meds(\n    self,\n    preferred_units: Dict[str, str],\n    vitals_df: pd.DataFrame = None,\n    hospitalization_ids: Optional[List[str]] = None,\n    show_intermediate: bool = False,\n    override: bool = False,\n    save_to_table: bool = True\n) -&gt; Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"\n    Convert dose units for continuous medication data.\n\n    Parameters\n    ----------\n    preferred_units : Dict[str, str]\n        Dict of preferred units for each medication category\n    vitals_df : pd.DataFrame, optional\n        Vitals DataFrame for extracting patient weights\n    hospitalization_ids : List[str], optional\n        List of specific hospitalization IDs to filter and process. When provided,\n        only medication and vitals data for these hospitalizations will be loaded\n        and processed, improving performance for targeted analyses.\n    show_intermediate : bool, default=False\n        If True, includes intermediate calculation columns in output\n    override : bool, default=False\n        If True, continues processing with warnings for unacceptable units\n    save_to_table : bool, default=True\n        If True, saves the converted DataFrame to the table's df_converted\n        property and stores conversion_counts as a table property. If False,\n        returns the converted data without updating the table.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame] or None\n        (converted_df, counts_df) when save_to_table=False, None otherwise\n    \"\"\"\n    from .utils.unit_converter import convert_dose_units_by_med_category\n\n    # Log function entry with parameters\n    self.logger.info(f\"Starting dose unit conversion for continuous medications with parameters: \"\n                    f\"preferred_units={preferred_units}, hospitalization_ids={'provided' if hospitalization_ids else 'None'}, \"\n                    f\"show_intermediate={show_intermediate}, override={override}, save_to_table={save_to_table}\")\n\n    # Load medication table with optional hospitalization_ids filter\n    if self.medication_admin_continuous is None:\n        self.logger.info(\"Loading medication_admin_continuous table...\")\n        if hospitalization_ids is not None:\n            self.logger.info(f\"Filtering for {len(hospitalization_ids)} hospitalization(s)\")\n            self.load_table('medication_admin_continuous', filters={'hospitalization_id': hospitalization_ids})\n        else:\n            self.load_table('medication_admin_continuous')\n        self.logger.debug(\"medication_admin_continuous table loaded successfully\")\n\n    # Determine hospitalization_ids for vitals loading if not provided\n    if hospitalization_ids is None and self.medication_admin_continuous is not None:\n        hospitalization_ids = self.medication_admin_continuous.df['hospitalization_id'].unique().tolist()\n        self.logger.debug(f\"Extracted {len(hospitalization_ids)} unique hospitalization_id(s) from medication data\")\n\n    # Load vitals df with filters for weight_kg only\n    if vitals_df is None:\n        self.logger.debug(\"No vitals_df provided, loading filtered vitals table\")\n        if (self.vitals is None) or (self.vitals.df is None):\n            self.logger.info(f\"Loading vitals table for {len(hospitalization_ids)} hospitalization(s), vital_category='weight_kg'\")\n            self.load_table('vitals', filters={'hospitalization_id': hospitalization_ids, 'vital_category': ['weight_kg']})\n        vitals_df = self.vitals.df\n        self.logger.debug(f\"Using vitals data with shape: {vitals_df.shape}\")\n    else:\n        self.logger.debug(f\"Using provided vitals_df with shape: {vitals_df.shape}\")\n\n    # Call the conversion function with all parameters\n    self.logger.info(\"Starting dose unit conversion\")\n    self.logger.debug(f\"Input DataFrame shape: {self.medication_admin_continuous.df.shape}\")\n\n    converted_df, counts_df = convert_dose_units_by_med_category(\n        self.medication_admin_continuous.df,\n        vitals_df=vitals_df,\n        preferred_units=preferred_units,\n        show_intermediate=show_intermediate,\n        override=override\n    )\n\n    self.logger.info(\"Dose unit conversion completed\")\n    self.logger.debug(f\"Output DataFrame shape: {converted_df.shape}\")\n    self.logger.debug(f\"Conversion counts summary: {len(counts_df)} conversions tracked\")\n\n    # If overwrite_raw_df is True, update the table's df and store conversion_counts\n    if save_to_table:\n        self.logger.info(\"Updating medication_admin_continuous table with converted data\")\n        self.medication_admin_continuous.df_converted = converted_df\n        self.medication_admin_continuous.conversion_counts = counts_df\n        self.logger.debug(\"Conversion counts stored as table property\")\n    else:\n        self.logger.info(\"Returning converted data without updating table\")\n        return converted_df, counts_df\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_dose_units_for_intermittent_meds","title":"convert_dose_units_for_intermittent_meds","text":"<pre><code>convert_dose_units_for_intermittent_meds(\n    preferred_units,\n    vitals_df=None,\n    hospitalization_ids=None,\n    show_intermediate=False,\n    override=False,\n    save_to_table=True,\n)\n</code></pre> <p>Convert dose units for intermittent medication data.</p> <p>Parameters:</p> Name Type Description Default <code>preferred_units</code> <code>Dict[str, str]</code> <p>Dict of preferred units for each medication category</p> required <code>vitals_df</code> <code>DataFrame</code> <p>Vitals DataFrame for extracting patient weights</p> <code>None</code> <code>hospitalization_ids</code> <code>List[str]</code> <p>List of specific hospitalization IDs to filter and process. When provided, only medication and vitals data for these hospitalizations will be loaded and processed, improving performance for targeted analyses.</p> <code>None</code> <code>show_intermediate</code> <code>bool</code> <p>If True, includes intermediate calculation columns in output</p> <code>False</code> <code>override</code> <code>bool</code> <p>If True, continues processing with warnings for unacceptable units</p> <code>False</code> <code>save_to_table</code> <code>bool</code> <p>If True, saves the converted DataFrame to the table's df_converted property and stores conversion_counts as a table property. If False, returns the converted data without updating the table.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame] or None</code> <p>(converted_df, counts_df) when save_to_table=False, None otherwise</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_dose_units_for_intermittent_meds(\n    self,\n    preferred_units: Dict[str, str],\n    vitals_df: pd.DataFrame = None,\n    hospitalization_ids: Optional[List[str]] = None,\n    show_intermediate: bool = False,\n    override: bool = False,\n    save_to_table: bool = True\n) -&gt; Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"\n    Convert dose units for intermittent medication data.\n\n    Parameters\n    ----------\n    preferred_units : Dict[str, str]\n        Dict of preferred units for each medication category\n    vitals_df : pd.DataFrame, optional\n        Vitals DataFrame for extracting patient weights\n    hospitalization_ids : List[str], optional\n        List of specific hospitalization IDs to filter and process. When provided,\n        only medication and vitals data for these hospitalizations will be loaded\n        and processed, improving performance for targeted analyses.\n    show_intermediate : bool, default=False\n        If True, includes intermediate calculation columns in output\n    override : bool, default=False\n        If True, continues processing with warnings for unacceptable units\n    save_to_table : bool, default=True\n        If True, saves the converted DataFrame to the table's df_converted\n        property and stores conversion_counts as a table property. If False,\n        returns the converted data without updating the table.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame] or None\n        (converted_df, counts_df) when save_to_table=False, None otherwise\n    \"\"\"\n    from .utils.unit_converter import convert_dose_units_by_med_category\n\n    # Log function entry with parameters\n    self.logger.info(f\"Starting dose unit conversion for intermittent medications with parameters: \"\n                    f\"preferred_units={preferred_units}, hospitalization_ids={'provided' if hospitalization_ids else 'None'}, \"\n                    f\"show_intermediate={show_intermediate}, override={override}, save_to_table={save_to_table}\")\n\n    # Load medication table with optional hospitalization_ids filter\n    if self.medication_admin_intermittent is None:\n        self.logger.info(\"Loading medication_admin_intermittent table...\")\n        if hospitalization_ids is not None:\n            self.logger.info(f\"Filtering for {len(hospitalization_ids)} hospitalization(s)\")\n            self.load_table('medication_admin_intermittent', filters={'hospitalization_id': hospitalization_ids})\n        else:\n            self.load_table('medication_admin_intermittent')\n        self.logger.debug(\"medication_admin_intermittent table loaded successfully\")\n\n    # Determine hospitalization_ids for vitals loading if not provided\n    if hospitalization_ids is None:\n        hospitalization_ids = self.medication_admin_intermittent.df['hospitalization_id'].unique().tolist()\n        self.logger.debug(f\"Extracted {len(hospitalization_ids)} unique hospitalization_id(s) from medication data\")\n\n    # Load vitals df with filters for weight_kg only\n    if vitals_df is None:\n        self.logger.debug(\"No vitals_df provided, loading filtered vitals table\")\n        if (self.vitals is None) or (self.vitals.df is None):\n            self.logger.info(f\"Loading vitals table for {len(hospitalization_ids)} hospitalization(s), vital_category='weight_kg'\")\n            self.load_table('vitals', filters={'hospitalization_id': hospitalization_ids, 'vital_category': ['weight_kg']})\n        vitals_df = self.vitals.df\n        self.logger.debug(f\"Using vitals data with shape: {vitals_df.shape}\")\n    else:\n        self.logger.debug(f\"Using provided vitals_df with shape: {vitals_df.shape}\")\n\n    # Call the conversion function with all parameters\n    self.logger.info(\"Starting dose unit conversion\")\n    self.logger.debug(f\"Input DataFrame shape: {self.medication_admin_intermittent.df.shape}\")\n\n    converted_df, counts_df = convert_dose_units_by_med_category(\n        self.medication_admin_intermittent.df,\n        vitals_df=vitals_df,\n        preferred_units=preferred_units,\n        show_intermediate=show_intermediate,\n        override=override\n    )\n\n    self.logger.info(\"Dose unit conversion completed\")\n    self.logger.debug(f\"Output DataFrame shape: {converted_df.shape}\")\n    self.logger.debug(f\"Conversion counts summary: {len(counts_df)} conversions tracked\")\n\n    # If save_to_table is True, update the table's df_converted and store conversion_counts\n    if save_to_table:\n        self.logger.info(\"Updating medication_admin_intermittent table with converted data\")\n        self.medication_admin_intermittent.df_converted = converted_df\n        self.medication_admin_intermittent.conversion_counts = counts_df\n        self.logger.debug(\"Conversion counts stored as table property\")\n    else:\n        self.logger.info(\"Returning converted data without updating table\")\n        return converted_df, counts_df\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_wide_to_hourly","title":"convert_wide_to_hourly","text":"<pre><code>convert_wide_to_hourly(\n    aggregation_config,\n    wide_df=None,\n    id_name=\"hospitalization_id\",\n    hourly_window=1,\n    fill_gaps=False,\n    memory_limit=\"4GB\",\n    temp_directory=None,\n    batch_size=None,\n)\n</code></pre> <p>Convert wide dataset to temporal aggregation using DuckDB with event-based windowing.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_config</code> <code>Dict[str, List[str]]</code> <p>Dict mapping aggregation methods to columns Example: {     'mean': ['heart_rate', 'sbp'],     'max': ['spo2'],     'min': ['map'],     'median': ['glucose'],     'first': ['gcs_total'],     'last': ['assessment_value'],     'boolean': ['norepinephrine'],     'one_hot_encode': ['device_category'] }</p> required <code>wide_df</code> <code>DataFrame</code> <p>Wide dataset DataFrame. If None, uses the stored wide_df from create_wide_dataset()</p> <code>None</code> <code>id_name</code> <code>str</code> <p>Column name to use for grouping aggregation. Options: - 'hospitalization_id': Group by individual hospitalizations (default) - 'encounter_block': Group by encounter blocks (after encounter stitching) - Any other ID column present in the wide dataset</p> <code>'hospitalization_id'</code> <code>hourly_window</code> <code>int</code> <p>Aggregation window size in hours (1-72). Windows start from each group's first event, not calendar boundaries.</p> <code>1</code> <code>fill_gaps</code> <code>bool</code> <p>Create rows for windows with no data (filled with NaN). False = sparse output (current behavior), True = dense output.</p> <code>False</code> <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '4GB', '8GB')</p> <code>'4GB'</code> <code>temp_directory</code> <code>str</code> <p>Directory for DuckDB temp files</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Process in batches if specified</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Aggregated DataFrame with columns: - window_number: Sequential window index (0-indexed) - window_start_dttm: Window start timestamp - window_end_dttm: Window end timestamp - All aggregated columns per config</p> <p>Examples:</p> <p>Standard hourly with sparse output (only windows with data)::</p> <pre><code>co.create_wide_dataset(...)\nhourly_df = co.convert_wide_to_hourly(\n    aggregation_config=config,\n    hourly_window=1,\n    fill_gaps=False\n)\n</code></pre> <p>6-hour windows with gap filling (all windows 0 to max)::</p> <pre><code>hourly_df = co.convert_wide_to_hourly(\n    aggregation_config=config,\n    hourly_window=6,\n    fill_gaps=True\n)\n# If data exists at windows 0, 1, 5:\n# - fill_gaps=False creates 3 rows (0, 1, 5)\n# - fill_gaps=True creates 6 rows (0, 1, 2, 3, 4, 5) with NaN in 2-4\n</code></pre> <p>Using encounter blocks after stitching::</p> <pre><code>co.run_stitch_encounters()\nco.create_wide_dataset(...)\nhourly_df = co.convert_wide_to_hourly(\n    aggregation_config=config,\n    id_name='encounter_block',\n    hourly_window=12\n)\n</code></pre> <p>Using explicit wide_df parameter::</p> <pre><code>hourly_df = co.convert_wide_to_hourly(\n    wide_df=my_df,\n    aggregation_config=config,\n    hourly_window=24\n)\n</code></pre> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_wide_to_hourly(\n    self,\n    aggregation_config: Dict[str, List[str]],\n    wide_df: Optional[pd.DataFrame] = None,\n    id_name: str = 'hospitalization_id',\n    hourly_window: int = 1,\n    fill_gaps: bool = False,\n    memory_limit: str = '4GB',\n    temp_directory: Optional[str] = None,\n    batch_size: Optional[int] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert wide dataset to temporal aggregation using DuckDB with event-based windowing.\n\n    Parameters\n    ----------\n    aggregation_config : Dict[str, List[str]]\n        Dict mapping aggregation methods to columns\n        Example: {\n            'mean': ['heart_rate', 'sbp'],\n            'max': ['spo2'],\n            'min': ['map'],\n            'median': ['glucose'],\n            'first': ['gcs_total'],\n            'last': ['assessment_value'],\n            'boolean': ['norepinephrine'],\n            'one_hot_encode': ['device_category']\n        }\n    wide_df : pd.DataFrame, optional\n        Wide dataset DataFrame. If None, uses the stored wide_df from create_wide_dataset()\n    id_name : str, default='hospitalization_id'\n        Column name to use for grouping aggregation. Options:\n        - 'hospitalization_id': Group by individual hospitalizations (default)\n        - 'encounter_block': Group by encounter blocks (after encounter stitching)\n        - Any other ID column present in the wide dataset\n    hourly_window : int, default=1\n        Aggregation window size in hours (1-72). Windows start from each\n        group's first event, not calendar boundaries.\n    fill_gaps : bool, default=False\n        Create rows for windows with no data (filled with NaN).\n        False = sparse output (current behavior), True = dense output.\n    memory_limit : str, default='4GB'\n        DuckDB memory limit (e.g., '4GB', '8GB')\n    temp_directory : str, optional\n        Directory for DuckDB temp files\n    batch_size : int, optional\n        Process in batches if specified\n\n    Returns\n    -------\n    pd.DataFrame\n        Aggregated DataFrame with columns:\n        - window_number: Sequential window index (0-indexed)\n        - window_start_dttm: Window start timestamp\n        - window_end_dttm: Window end timestamp\n        - All aggregated columns per config\n\n    Examples\n    --------\n    Standard hourly with sparse output (only windows with data)::\n\n        co.create_wide_dataset(...)\n        hourly_df = co.convert_wide_to_hourly(\n            aggregation_config=config,\n            hourly_window=1,\n            fill_gaps=False\n        )\n\n    6-hour windows with gap filling (all windows 0 to max)::\n\n        hourly_df = co.convert_wide_to_hourly(\n            aggregation_config=config,\n            hourly_window=6,\n            fill_gaps=True\n        )\n        # If data exists at windows 0, 1, 5:\n        # - fill_gaps=False creates 3 rows (0, 1, 5)\n        # - fill_gaps=True creates 6 rows (0, 1, 2, 3, 4, 5) with NaN in 2-4\n\n    Using encounter blocks after stitching::\n\n        co.run_stitch_encounters()\n        co.create_wide_dataset(...)\n        hourly_df = co.convert_wide_to_hourly(\n            aggregation_config=config,\n            id_name='encounter_block',\n            hourly_window=12\n        )\n\n    Using explicit wide_df parameter::\n\n        hourly_df = co.convert_wide_to_hourly(\n            wide_df=my_df,\n            aggregation_config=config,\n            hourly_window=24\n        )\n    \"\"\"\n    from clifpy.utils.wide_dataset import convert_wide_to_hourly\n\n    # Use provided wide_df or fall back to stored one\n    if wide_df is None:\n        if self.wide_df is None:\n            raise ValueError(\n                \"No wide dataset found. Please either:\\n\"\n                \"1. Run create_wide_dataset() first, OR\\n\"\n                \"2. Provide a wide_df parameter\"\n            )\n        wide_df = self.wide_df\n\n    return convert_wide_to_hourly(\n        wide_df=wide_df,\n        aggregation_config=aggregation_config,\n        id_name=id_name,\n        hourly_window=hourly_window,\n        fill_gaps=fill_gaps,\n        memory_limit=memory_limit,\n        temp_directory=temp_directory,\n        batch_size=batch_size\n    )\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.create_wide_dataset","title":"create_wide_dataset","text":"<pre><code>create_wide_dataset(\n    tables_to_load=None,\n    category_filters=None,\n    sample=False,\n    hospitalization_ids=None,\n    encounter_blocks=None,\n    cohort_df=None,\n    output_format=\"dataframe\",\n    save_to_data_location=False,\n    output_filename=None,\n    return_dataframe=True,\n    batch_size=1000,\n    memory_limit=None,\n    threads=None,\n    show_progress=True,\n)\n</code></pre> <p>Create wide time-series dataset using DuckDB for high performance.</p> <p>Parameters:</p> Name Type Description Default <code>tables_to_load</code> <code>List[str]</code> <p>List of table names to include in the wide dataset (e.g., ['vitals', 'labs', 'respiratory_support']). If None, only base tables (patient, hospitalization, adt) are loaded.</p> <code>None</code> <code>category_filters</code> <code>Dict[str, List[str]]</code> <p>Dictionary mapping table names to lists for filtering/selection. Behavior differs by table type:</p> <p>PIVOT TABLES (narrow to wide - category values from schema): - Values are category values to filter and pivot into columns - Examples:   * vitals: temp_c, heart_rate, sbp, dbp, spo2, respiratory_rate, map   * labs: hemoglobin, wbc, sodium, potassium, creatinine, glucose_serum, lactate   * medication_admin_continuous: norepinephrine, epinephrine, propofol, fentanyl   * patient_assessments: RASS, gcs_total, cam_total, braden_total</p> <p>WIDE TABLES (already wide - column names from schema): - Values are column names to keep from the table - Examples:   * respiratory_support: device_category, mode_category, fio2_set, peep_set</p> <p>Usage Example: <pre><code>category_filters = {\n    'vitals': ['heart_rate', 'sbp', 'spo2'],\n    'labs': ['hemoglobin', 'sodium', 'creatinine'],\n    'respiratory_support': ['device_category', 'fio2_set', 'peep_set']\n}\n</code></pre></p> <p>Supported Tables: For complete list of supported tables and their types, see: clifpy/schemas/wide_tables_config.yaml</p> <p>Category Values: For complete lists of acceptable category values, see: - Table schemas: clifpy/schemas/*_schema.yaml - Use <code>co.vitals.df['vital_category'].unique()</code> to see available values in your data</p> <code>None</code> <code>sample</code> <code>bool</code> <p>If True, randomly sample 20 hospitalizations for testing purposes.</p> <code>False</code> <code>hospitalization_ids</code> <code>List[str]</code> <p>List of specific hospitalization IDs to include. When provided, only data for these hospitalizations will be loaded, improving performance for large datasets.</p> <code>None</code> <code>encounter_blocks</code> <code>List[int]</code> <p>List of encounter block IDs to include when encounter stitching has been performed. Automatically converts encounter blocks to their corresponding hospitalization IDs. Only used when encounter stitching is enabled and encounter mapping exists.</p> <code>None</code> <code>cohort_df</code> <code>DataFrame</code> <p>DataFrame containing cohort definitions with columns:</p> <ul> <li>'patient_id': Patient identifier</li> <li>'start_time': Start of time window (datetime)</li> <li>'end_time': End of time window (datetime)</li> </ul> <p>When encounter stitching is enabled, can also include 'encounter_block' column. Used to filter data to specific time windows per patient.</p> <code>None</code> <code>output_format</code> <code>str</code> <p>Format for output data. Options: 'dataframe', 'csv', 'parquet'.</p> <code>'dataframe'</code> <code>save_to_data_location</code> <code>bool</code> <p>If True, save output file to the data directory specified in orchestrator config.</p> <code>False</code> <code>output_filename</code> <code>str</code> <p>Custom filename for saved output. If None, auto-generates filename with timestamp.</p> <code>None</code> <code>return_dataframe</code> <code>bool</code> <p>If True, return DataFrame even when saving to file. If False and saving, returns None to save memory.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Number of hospitalizations to process per batch. Lower values use less memory.</p> <code>1000</code> <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '8GB', '16GB'). If None, uses DuckDB default.</p> <code>None</code> <code>threads</code> <code>int</code> <p>Number of threads for DuckDB to use. If None, uses all available cores.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>If True, display progress bars during processing.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>The wide dataset is stored in the <code>wide_df</code> property of the orchestrator instance. Access the result via <code>orchestrator.wide_df</code> after calling this method.</p> Notes <ul> <li>When hospitalization_ids is provided, the function efficiently loads only the   specified hospitalizations from all tables, significantly reducing memory usage   and processing time for targeted analyses.</li> <li>The wide dataset will have one row per hospitalization per time point, with   columns for each category value specified in category_filters.</li> </ul> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def create_wide_dataset(\n    self,\n    tables_to_load: Optional[List[str]] = None,\n    category_filters: Optional[Dict[str, List[str]]] = None,\n    sample: bool = False,\n    hospitalization_ids: Optional[List[str]] = None,\n    encounter_blocks: Optional[List[int]] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    output_format: str = 'dataframe',\n    save_to_data_location: bool = False,\n    output_filename: Optional[str] = None,\n    return_dataframe: bool = True,\n    batch_size: int = 1000,\n    memory_limit: Optional[str] = None,\n    threads: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; None:\n    \"\"\"\n    Create wide time-series dataset using DuckDB for high performance.\n\n    Parameters\n    ----------\n    tables_to_load : List[str], optional\n        List of table names to include in the wide dataset (e.g., ['vitals', 'labs', 'respiratory_support']).\n        If None, only base tables (patient, hospitalization, adt) are loaded.\n    category_filters : Dict[str, List[str]], optional\n        Dictionary mapping table names to lists for filtering/selection. Behavior differs\n        by table type:\n\n        **PIVOT TABLES** (narrow to wide - category values from schema):\n        - Values are **category values** to filter and pivot into columns\n        - Examples:\n          * vitals: temp_c, heart_rate, sbp, dbp, spo2, respiratory_rate, map\n          * labs: hemoglobin, wbc, sodium, potassium, creatinine, glucose_serum, lactate\n          * medication_admin_continuous: norepinephrine, epinephrine, propofol, fentanyl\n          * patient_assessments: RASS, gcs_total, cam_total, braden_total\n\n        **WIDE TABLES** (already wide - column names from schema):\n        - Values are **column names** to keep from the table\n        - Examples:\n          * respiratory_support: device_category, mode_category, fio2_set, peep_set\n\n        Usage Example:\n        ```python\n        category_filters = {\n            'vitals': ['heart_rate', 'sbp', 'spo2'],\n            'labs': ['hemoglobin', 'sodium', 'creatinine'],\n            'respiratory_support': ['device_category', 'fio2_set', 'peep_set']\n        }\n        ```\n\n        **Supported Tables:**\n        For complete list of supported tables and their types, see:\n        clifpy/schemas/wide_tables_config.yaml\n\n        **Category Values:**\n        For complete lists of acceptable category values, see:\n        - Table schemas: clifpy/schemas/*_schema.yaml\n        - Use `co.vitals.df['vital_category'].unique()` to see available values in your data\n    sample : bool, default=False\n        If True, randomly sample 20 hospitalizations for testing purposes.\n    hospitalization_ids : List[str], optional\n        List of specific hospitalization IDs to include. When provided, only data for these\n        hospitalizations will be loaded, improving performance for large datasets.\n    encounter_blocks : List[int], optional\n        List of encounter block IDs to include when encounter stitching has been performed.\n        Automatically converts encounter blocks to their corresponding hospitalization IDs.\n        Only used when encounter stitching is enabled and encounter mapping exists.\n    cohort_df : pd.DataFrame, optional\n        DataFrame containing cohort definitions with columns:\n\n        - 'patient_id': Patient identifier\n        - 'start_time': Start of time window (datetime)\n        - 'end_time': End of time window (datetime)\n\n        When encounter stitching is enabled, can also include 'encounter_block' column.\n        Used to filter data to specific time windows per patient.\n    output_format : str, default='dataframe'\n        Format for output data. Options: 'dataframe', 'csv', 'parquet'.\n    save_to_data_location : bool, default=False\n        If True, save output file to the data directory specified in orchestrator config.\n    output_filename : str, optional\n        Custom filename for saved output. If None, auto-generates filename with timestamp.\n    return_dataframe : bool, default=True\n        If True, return DataFrame even when saving to file. If False and saving,\n        returns None to save memory.\n    batch_size : int, default=1000\n        Number of hospitalizations to process per batch. Lower values use less memory.\n    memory_limit : str, optional\n        DuckDB memory limit (e.g., '8GB', '16GB'). If None, uses DuckDB default.\n    threads : int, optional\n        Number of threads for DuckDB to use. If None, uses all available cores.\n    show_progress : bool, default=True\n        If True, display progress bars during processing.\n\n    Returns\n    -------\n    None\n        The wide dataset is stored in the `wide_df` property of the orchestrator instance.\n        Access the result via `orchestrator.wide_df` after calling this method.\n\n    Notes\n    -----\n    - When hospitalization_ids is provided, the function efficiently loads only the\n      specified hospitalizations from all tables, significantly reducing memory usage\n      and processing time for targeted analyses.\n    - The wide dataset will have one row per hospitalization per time point, with\n      columns for each category value specified in category_filters.\n    \"\"\"\n    self.logger.info(\"=\" * 50)\n    self.logger.info(\"\ud83d\ude80 WIDE DATASET CREATION STARTED\")\n    self.logger.info(\"=\" * 50)\n\n    self.logger.info(\"Phase 1: Initialization\")\n    self.logger.debug(\"  1.1: Validating parameters\")\n\n    # Import the utility function\n    from clifpy.utils.wide_dataset import create_wide_dataset as _create_wide\n\n    # Handle encounter stitching scenarios\n    if self.encounter_mapping is not None:\n        self.logger.info(\"  1.2: Configuring encounter stitching (enabled)\")\n    else:\n        self.logger.debug(\"  1.2: Encounter stitching (disabled)\")\n\n    self.logger.info(\"Phase 2: Encounter Processing\")\n\n    if self.encounter_mapping is not None:\n        self.logger.info(\"  2.1: === SPECIAL: ENCOUNTER STITCHING ===\")\n        # Handle cohort_df with encounter_block column\n        if cohort_df is not None:\n            if 'encounter_block' in cohort_df.columns:\n                self.logger.info(\"       - Detected encounter_block column in cohort_df\")\n                self.logger.debug(\"       - Mapping encounter blocks to hospitalization IDs\")\n                # Merge cohort_df with encounter_mapping to get hospitalization_ids\n                cohort_df = pd.merge(\n                    cohort_df,\n                    self.encounter_mapping[['hospitalization_id', 'encounter_block']],\n                    on='encounter_block',\n                    how='inner',\n                    suffixes=('_orig', '')\n                )\n                # If hospitalization_id_orig exists (cohort had both), use the mapping version\n                if 'hospitalization_id_orig' in cohort_df.columns:\n                    cohort_df = cohort_df.drop(columns=['hospitalization_id_orig'])\n                self.logger.info(f\"       - Processing {cohort_df['encounter_block'].nunique()} encounter blocks from cohort_df\")\n            elif 'hospitalization_id' in cohort_df.columns:\n                self.logger.info(\"Encounter stitching has been performed. Your cohort_df uses hospitalization_id. \" +\n                      \"Consider using 'encounter_block' column instead for cleaner encounter-level filtering\")\n            else:\n                self.logger.warning(\"cohort_df must contain either 'hospitalization_id' or 'encounter_block' column\")\n\n        # Handle encounter_blocks parameter\n        if encounter_blocks is not None:\n            self.logger.debug(\"       - Processing encounter_blocks parameter\")\n            if len(encounter_blocks) == 0:\n                self.logger.warning(\"       - Empty encounter_blocks list provided. Processing all encounter blocks\")\n                encounter_blocks = None\n            else:\n                # Validate that provided encounter_blocks exist in mapping\n                invalid_blocks = [b for b in encounter_blocks if b not in self.encounter_mapping['encounter_block'].values]\n                if invalid_blocks:\n                    self.logger.warning(f\"       - Invalid encounter blocks found: {invalid_blocks}\")\n                    encounter_blocks = [b for b in encounter_blocks if b in self.encounter_mapping['encounter_block'].values]\n\n                if encounter_blocks:  # Only if valid blocks remain\n                    hospitalization_ids = self.encounter_mapping[\n                        self.encounter_mapping['encounter_block'].isin(encounter_blocks)\n                    ]['hospitalization_id'].tolist()\n                    self.logger.info(f\"       - Converting {len(encounter_blocks)} encounter blocks to {len(hospitalization_ids)} hospitalizations\")\n                else:\n                    self.logger.warning(\"       - No valid encounter blocks found. Processing all data\")\n                    encounter_blocks = None\n\n        # If no filters provided after stitching\n        elif hospitalization_ids is None and cohort_df is None:\n            self.logger.debug(\"       - No encounter_blocks provided - processing all encounter blocks\")\n    else:\n        self.logger.debug(\"  2.1: No encounter stitching performed\")\n\n    filters = None\n    if hospitalization_ids:\n        filters = {'hospitalization_id': hospitalization_ids}\n\n    self.logger.info(\"Phase 3: Table Loading\")\n\n    self.logger.info(\"  3.1: Auto-loading base tables\")\n    # Auto-load base tables if not loaded\n    if self.patient is None:\n        self.logger.info(\"       - Loading patient table\")\n        self.load_table('patient')  # Patient doesn't need filters\n    if self.hospitalization is None:\n        self.logger.info(\"       - Loading hospitalization table\")\n        self.load_table('hospitalization', filters=filters)\n    if self.adt is None:\n        self.logger.info(\"       - Loading adt table\")\n        self.load_table('adt', filters=filters)\n\n    # Load optional tables only if not already loaded\n    self.logger.info(f\"  3.2: Loading optional tables: {tables_to_load or 'None'}\")\n    if tables_to_load:\n        for table_name in tables_to_load:\n            if getattr(self, table_name, None) is None:\n                self.logger.info(f\"       - Loading {table_name} table\")\n                try:\n                    self.load_table(table_name)\n                except Exception as e:\n                    self.logger.warning(f\"       - Could not load {table_name}: {e}\")\n            else:\n                self.logger.debug(f\"       - {table_name} table already loaded\")\n\n    # Check if patient_assessments needs assessment_value column\n    if (tables_to_load and 'patient_assessments' in tables_to_load) or \\\n       (category_filters and 'patient_assessments' in category_filters):\n        if self.patient_assessments is not None and hasattr(self.patient_assessments, 'df'):\n            df = self.patient_assessments.df\n            if 'numerical_value' in df.columns and 'categorical_value' in df.columns:\n                if 'assessment_value' not in df.columns:\n\n                    self.logger.info(\"  === SPECIAL: PATIENT ASSESSMENTS PROCESSING ===\")\n                    self.logger.info(\"       - Merging numerical_value and categorical_value columns\")\n                    try:\n                        import polars as pl\n                        self.logger.debug(\"       - Using Polars for performance optimization\")\n\n                        # Convert to Polars for efficient processing\n                        df_pl = pl.from_pandas(df)\n\n                        # Check data integrity using Polars\n                        both_filled = df_pl.filter(\n                            (pl.col('numerical_value').is_not_null()) &amp;\n                            (pl.col('categorical_value').is_not_null())\n                        )\n                        both_filled_count = len(both_filled)\n\n                        if both_filled_count &gt; 0:\n                            self.logger.warning(f\"       - Found {both_filled_count} rows with both numerical and categorical values - numerical values will take precedence\")\n\n                        # Create assessment_value using Polars coalesce (much faster than pandas fillna)\n                        df_pl = df_pl.with_columns(\n                            pl.coalesce([\n                                pl.col('numerical_value'),\n                                pl.col('categorical_value')\n                            ]).cast(pl.Utf8).alias('assessment_value')\n                        )\n\n                        # Calculate statistics efficiently with Polars\n                        num_count = df_pl.select(pl.col('numerical_value').is_not_null().sum()).item()\n                        cat_count = df_pl.select(pl.col('categorical_value').is_not_null().sum()).item()\n                        total_count = df_pl.select(pl.col('assessment_value').is_not_null().sum()).item()\n\n                        # Convert back to pandas for compatibility\n                        self.patient_assessments.df = df_pl.to_pandas()\n\n                        self.logger.info(f\"       - Created assessment_value column: {num_count} numerical, {cat_count} categorical, {total_count} total non-null\")\n                        self.logger.debug(f\"       -   Stored as string type for processing compatibility\")\n\n                    except ImportError:\n                        self.logger.warning(\"       - Polars not installed. Using pandas (slower)\")\n                        # Fallback to pandas\n                        both_filled = df[(df['numerical_value'].notna()) &amp;\n                                        (df['categorical_value'].notna())]\n                        if len(both_filled) &gt; 0:\n                            self.logger.warning(f\"       - Found {len(both_filled)} rows with both numerical and categorical values\")\n\n                        df['assessment_value'] = df['numerical_value'].fillna(df['categorical_value'])\n                        df['assessment_value'] = df['assessment_value'].astype(str)\n\n                        num_count = df['numerical_value'].notna().sum()\n                        cat_count = df['categorical_value'].notna().sum()\n                        total_count = df['assessment_value'].notna().sum()\n\n                        self.logger.info(f\"       - Created assessment_value column: {num_count} numerical, {cat_count} categorical, {total_count} total non-null\")\n\n    self.logger.info(\"Phase 4: Calling Wide Dataset Utility\")\n\n    self.logger.debug(f\"  4.1: Passing to wide_dataset.create_wide_dataset()\")\n    self.logger.debug(f\"       - Tables: {tables_to_load or 'None'}\")\n    self.logger.debug(f\"       - Category filters: {list(category_filters.keys()) if category_filters else 'None'}\")\n    self.logger.debug(f\"       - Batch size: {batch_size}\")\n    self.logger.debug(f\"       - Memory limit: {memory_limit}\")\n    self.logger.debug(f\"       - Show progress: {show_progress}\")\n\n    # Call utility function with self as clif_instance and store result in wide_df property\n    self.wide_df = _create_wide(\n        clif_instance=self,\n        optional_tables=tables_to_load,\n        category_filters=category_filters,\n        sample=sample,\n        hospitalization_ids=hospitalization_ids,\n        cohort_df=cohort_df,\n        output_format=output_format,\n        save_to_data_location=save_to_data_location,\n        output_filename=output_filename,\n        return_dataframe=return_dataframe,\n        batch_size=batch_size,\n        memory_limit=memory_limit,\n        threads=threads,\n        show_progress=show_progress\n    )\n\n    self.logger.info(\"Phase 5: Post-Processing\")\n\n    # Add encounter_block column if encounter mapping exists and not already present\n    if self.encounter_mapping is not None and self.wide_df is not None:\n\n        self.logger.info(\"  5.1: === SPECIAL: ADDING ENCOUNTER BLOCKS ===\")\n        if 'encounter_block' not in self.wide_df.columns:\n            self.logger.info(\"       - Adding encounter_block column from encounter mapping\")\n            self.wide_df = pd.merge(\n                self.wide_df,\n                self.encounter_mapping[['hospitalization_id', 'encounter_block']],\n                on='hospitalization_id',\n                how='left'\n            )\n            self.logger.info(f\"       - Added encounter_block column - {self.wide_df['encounter_block'].nunique()} unique encounter blocks\")\n        else:\n            self.logger.debug(f\"       - Encounter_block column already present - {self.wide_df['encounter_block'].nunique()} unique encounter blocks\")\n    else:\n        self.logger.debug(\"  5.1: No encounter block mapping to add\")\n\n    # Optimize data types for assessment columns using Polars for performance\n    if self.wide_df is not None and ((tables_to_load and 'patient_assessments' in tables_to_load) or \\\n       (category_filters and 'patient_assessments' in category_filters)):\n\n        self.logger.info(\"  5.2: === SPECIAL: ASSESSMENT TYPE OPTIMIZATION ===\")\n        try:\n            import polars as pl\n            self.logger.debug(\"       - Using Polars for performance optimization\")\n\n            # Determine which assessment columns to check\n            assessment_columns = []\n\n            if category_filters and 'patient_assessments' in category_filters:\n                # If specific categories were requested, use those\n                assessment_columns = [col for col in category_filters['patient_assessments']\n                                     if col in self.wide_df.columns]\n            else:\n                # Get all possible assessment categories from the schema\n                if self.patient_assessments and hasattr(self.patient_assessments, 'schema'):\n                    schema = self.patient_assessments.schema\n                    if 'columns' in schema:\n                        for col_def in schema['columns']:\n                            if col_def.get('name') == 'assessment_category':\n                                assessment_columns = col_def.get('permissible_values', [])\n                                break\n\n                # Filter to only columns that exist in wide_df\n                assessment_columns = [col for col in assessment_columns if col in self.wide_df.columns]\n\n            if assessment_columns:\n                self.logger.info(f\"       - Analyzing {len(assessment_columns)} assessment columns\")\n\n                # Convert to Polars for efficient processing\n                df_pl = pl.from_pandas(self.wide_df)\n\n                numeric_conversions = []\n                string_kept = []\n\n                # Process all columns in one go for better performance\n                for col in assessment_columns:\n                    try:\n                        # Create a temporary column with numeric conversion attempt\n                        temp_col = f\"{col}_numeric_test\"\n                        df_pl = df_pl.with_columns(\n                            pl.col(col).cast(pl.Float64, strict=False).alias(temp_col)\n                        )\n\n                        # Check conversion success rate\n                        stats = df_pl.select([\n                            pl.col(col).is_not_null().sum().alias('original_count'),\n                            pl.col(temp_col).is_not_null().sum().alias('converted_count')\n                        ]).row(0)\n\n                        if stats[0] &gt; 0:  # If there are non-null values\n                            conversion_rate = stats[1] / stats[0]\n\n                            if conversion_rate &gt;= 0.95:  # 95% or more are numeric\n                                # Replace original with converted\n                                df_pl = df_pl.drop(col).rename({temp_col: col})\n                                numeric_conversions.append(col)\n                            else:\n                                # Keep original, drop temp\n                                df_pl = df_pl.drop(temp_col)\n                                string_kept.append(col)\n                        else:\n                            # No data, just drop temp\n                            df_pl = df_pl.drop(temp_col)\n\n                    except Exception:\n                        # Keep as string if any error, clean up temp column if it exists\n                        if f\"{col}_numeric_test\" in df_pl.columns:\n                            df_pl = df_pl.drop(f\"{col}_numeric_test\")\n                        string_kept.append(col)\n\n                # Convert back to pandas\n                self.wide_df = df_pl.to_pandas()\n\n                # Report conversions\n                if numeric_conversions:\n                    self.logger.info(f\"       - Converted to numeric: {len(numeric_conversions)} columns\")\n                    self.logger.debug(f\"       -   Examples: {', '.join(numeric_conversions[:5])}\")\n                    if len(numeric_conversions) &gt; 5:\n                        self.logger.debug(f\"       -   ... and {len(numeric_conversions) - 5} more\")\n\n                if string_kept:\n                    self.logger.info(f\"       - Kept as string: {len(string_kept)} columns with mixed/text values\")\n                    self.logger.debug(f\"       -   Examples: {', '.join(string_kept[:5])}\")\n                    if len(string_kept) &gt; 5:\n                        self.logger.debug(f\"       -   ... and {len(string_kept) - 5} more\")\n            else:\n                self.logger.debug(\"       - No assessment columns found to optimize\")\n\n        except ImportError:\n            self.logger.warning(\"       - Polars not installed. Skipping type optimization\")\n            self.logger.info(\"       - Install polars for better performance: pip install polars\")\n    else:\n        self.logger.debug(\"  5.2: No assessment type optimization needed\")\n\n    self.logger.info(\"Phase 6: Completion\")\n\n    if self.wide_df is not None:\n        # Convert encounter_block to int32 if it exists (memory optimization)\n        if 'encounter_block' in self.wide_df.columns:\n            self.wide_df['encounter_block'] = self.wide_df['encounter_block'].astype('Int32')\n\n        self.logger.info(f\"  6.1: Wide dataset stored in self.wide_df\")\n        self.logger.info(f\"  6.2: Dataset shape: {self.wide_df.shape[0]} rows x {self.wide_df.shape[1]} columns\")\n    else:\n        self.logger.warning(\"  6.1: No wide dataset was created\")\n\n    self.logger.info(\"=\" * 50)\n    self.logger.info(\"\u2705 WIDE DATASET CREATION COMPLETED\")\n    self.logger.info(\"=\" * 50)\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config_path='./config.json')\n</code></pre> <p>Create a ClifOrchestrator instance from a configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration JSON file</p> <code>'./config.json'</code> <p>Returns:</p> Type Description <code>ClifOrchestrator</code> <p>Configured instance</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>@classmethod\ndef from_config(cls, config_path: str = \"./config.json\") -&gt; 'ClifOrchestrator':\n    \"\"\"\n    Create a ClifOrchestrator instance from a configuration file.\n\n    Parameters\n    ----------\n    config_path : str\n        Path to the configuration JSON file\n\n    Returns\n    -------\n    ClifOrchestrator\n        Configured instance\n    \"\"\"\n    return cls(config_path=config_path)\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_encounter_mapping","title":"get_encounter_mapping","text":"<pre><code>get_encounter_mapping()\n</code></pre> <p>Return the encounter mapping DataFrame if encounter stitching was performed.</p> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>Mapping of hospitalization_id to encounter_block if stitching was performed, None if stitching was not performed or failed.</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_encounter_mapping(self) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Return the encounter mapping DataFrame if encounter stitching was performed.\n\n    Returns\n    -------\n    pd.DataFrame or None\n        Mapping of hospitalization_id to encounter_block if stitching was performed,\n        None if stitching was not performed or failed.\n    \"\"\"\n    if self.encounter_mapping is None:\n        self.run_stitch_encounters()\n    return self.encounter_mapping\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_loaded_tables","title":"get_loaded_tables","text":"<pre><code>get_loaded_tables()\n</code></pre> <p>Return list of currently loaded table names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of loaded table names</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_loaded_tables(self) -&gt; List[str]:\n    \"\"\"\n    Return list of currently loaded table names.\n\n    Returns\n    -------\n    List[str]\n        List of loaded table names\n    \"\"\"\n    loaded = []\n    for table_name in TABLE_CLASSES.keys():\n        if getattr(self, table_name) is not None:\n            loaded.append(table_name)\n    return loaded\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_sys_resource_info","title":"get_sys_resource_info","text":"<pre><code>get_sys_resource_info(print_summary=True)\n</code></pre> <p>Get system resource information including CPU, memory, and practical thread limits.</p> <p>Parameters:</p> Name Type Description Default <code>print_summary</code> <code>bool</code> <p>Whether to print a formatted summary</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing system resource information:</p> <ul> <li>cpu_count_physical: Number of physical CPU cores</li> <li>cpu_count_logical: Number of logical CPU cores</li> <li>cpu_usage_percent: Current CPU usage percentage</li> <li>memory_total_gb: Total RAM in GB</li> <li>memory_available_gb: Available RAM in GB</li> <li>memory_used_gb: Used RAM in GB</li> <li>memory_usage_percent: Memory usage percentage</li> <li>process_threads: Number of threads used by current process</li> <li>max_recommended_threads: Recommended max threads for optimal performance</li> </ul> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_sys_resource_info(self, print_summary: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get system resource information including CPU, memory, and practical thread limits.\n\n    Parameters\n    ----------\n    print_summary : bool, default=True\n        Whether to print a formatted summary\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary containing system resource information:\n\n        - cpu_count_physical: Number of physical CPU cores\n        - cpu_count_logical: Number of logical CPU cores\n        - cpu_usage_percent: Current CPU usage percentage\n        - memory_total_gb: Total RAM in GB\n        - memory_available_gb: Available RAM in GB\n        - memory_used_gb: Used RAM in GB\n        - memory_usage_percent: Memory usage percentage\n        - process_threads: Number of threads used by current process\n        - max_recommended_threads: Recommended max threads for optimal performance\n    \"\"\"\n    # Get current process\n    current_process = psutil.Process()\n\n    # CPU information\n    cpu_count_physical = psutil.cpu_count(logical=False)\n    cpu_count_logical = psutil.cpu_count(logical=True)\n    cpu_usage_percent = psutil.cpu_percent(interval=1)\n\n    # Memory information\n    memory = psutil.virtual_memory()\n    memory_total_gb = memory.total / (1024**3)\n    memory_available_gb = memory.available / (1024**3)\n    memory_used_gb = memory.used / (1024**3)\n    memory_usage_percent = memory.percent\n\n    # Thread information\n    process_threads = current_process.num_threads()\n    max_recommended_threads = cpu_count_physical  # Conservative recommendation\n\n    resource_info = {\n        'cpu_count_physical': cpu_count_physical,\n        'cpu_count_logical': cpu_count_logical,\n        'cpu_usage_percent': cpu_usage_percent,\n        'memory_total_gb': memory_total_gb,\n        'memory_available_gb': memory_available_gb,\n        'memory_used_gb': memory_used_gb,\n        'memory_usage_percent': memory_usage_percent,\n        'process_threads': process_threads,\n        'max_recommended_threads': max_recommended_threads\n    }\n\n    if print_summary:\n        self.logger.info(\"=\" * 50)\n        self.logger.info(\"SYSTEM RESOURCES\")\n        self.logger.info(\"=\" * 50)\n        self.logger.info(f\"CPU Cores (Physical): {cpu_count_physical}\")\n        self.logger.info(f\"CPU Cores (Logical):  {cpu_count_logical}\")\n        self.logger.info(f\"CPU Usage:            {cpu_usage_percent:.1f}%\")\n        self.logger.info(\"-\" * 50)\n        self.logger.info(f\"Total RAM:            {memory_total_gb:.1f} GB\")\n        self.logger.info(f\"Available RAM:        {memory_available_gb:.1f} GB\")\n        self.logger.info(f\"Used RAM:             {memory_used_gb:.1f} GB\")\n        self.logger.info(f\"Memory Usage:         {memory_usage_percent:.1f}%\")\n        self.logger.info(\"-\" * 50)\n        self.logger.info(f\"Process Threads:      {process_threads}\")\n        self.logger.info(f\"Max Recommended:      {max_recommended_threads} threads\")\n        self.logger.info(\"-\" * 50)\n        self.logger.info(f\"RECOMMENDATION: Use {max(1, cpu_count_physical-2)}-{cpu_count_physical} threads for optimal performance\")\n        self.logger.info(f\"(Based on {cpu_count_physical} physical CPU cores)\")\n        self.logger.info(\"=\" * 50)\n\n    return resource_info\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_tables_obj_list","title":"get_tables_obj_list","text":"<pre><code>get_tables_obj_list()\n</code></pre> <p>Return list of loaded table objects.</p> <p>Returns:</p> Type Description <code>List</code> <p>List of loaded table objects</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_tables_obj_list(self) -&gt; List:\n    \"\"\"\n    Return list of loaded table objects.\n\n    Returns\n    -------\n    List\n        List of loaded table objects\n    \"\"\"\n    table_objects = []\n    for table_name in TABLE_CLASSES.keys():\n        table_obj = getattr(self, table_name)\n        if table_obj is not None:\n            table_objects.append(table_obj)\n    return table_objects\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.initialize","title":"initialize","text":"<pre><code>initialize(\n    tables=None,\n    sample_size=None,\n    columns=None,\n    filters=None,\n)\n</code></pre> <p>Initialize specified tables with optional filtering and column selection.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>List[str]</code> <p>List of table names to load. Defaults to ['patient'].</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of rows to load for each table.</p> <code>None</code> <code>columns</code> <code>Dict[str, List[str]]</code> <p>Dictionary mapping table names to lists of columns to load.</p> <code>None</code> <code>filters</code> <code>Dict[str, Dict]</code> <p>Dictionary mapping table names to filter dictionaries.</p> <code>None</code> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def initialize(\n    self,\n    tables: Optional[List[str]] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[Dict[str, List[str]]] = None,\n    filters: Optional[Dict[str, Dict[str, Any]]] = None\n):\n    \"\"\"\n    Initialize specified tables with optional filtering and column selection.\n\n    Parameters\n    ----------\n    tables : List[str], optional\n        List of table names to load. Defaults to ['patient'].\n    sample_size : int, optional\n        Number of rows to load for each table.\n    columns : Dict[str, List[str]], optional\n        Dictionary mapping table names to lists of columns to load.\n    filters : Dict[str, Dict], optional\n        Dictionary mapping table names to filter dictionaries.\n    \"\"\"\n    if tables is None:\n        tables = ['patient']\n\n    for table in tables:\n        # Get table-specific columns and filters if provided\n        table_columns = columns.get(table) if columns else None\n        table_filters = filters.get(table) if filters else None\n\n        try:\n            self.load_table(table, sample_size, table_columns, table_filters)\n        except ValueError as e:\n            self.logger.warning(f\"{e}\")\n\n    # Perform encounter stitching if enabled\n    if self.stitch_encounter:\n        self.run_stitch_encounters()\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.load_table","title":"load_table","text":"<pre><code>load_table(\n    table_name, sample_size=None, columns=None, filters=None\n)\n</code></pre> <p>Load table data and create table object.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to load</p> required <code>sample_size</code> <code>int</code> <p>Number of rows to load</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>Specific columns to load</p> <code>None</code> <code>filters</code> <code>Dict</code> <p>Filters to apply when loading</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Patient, Hospitalization, Adt, Labs, Vitals, MedicationAdminContinuous, PatientAssessments, RespiratorySupport, Position]</code> <p>The loaded table object</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def load_table(\n    self,\n    table_name: str,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n) -&gt; Union[Patient, Hospitalization, Adt, Labs, Vitals, MedicationAdminContinuous, MedicationAdminIntermittent, PatientAssessments, RespiratorySupport, Position, HospitalDiagnosis, MicrobiologyCulture, CrrtTherapy, PatientProcedures, MicrobiologySusceptibility, EcmoMcs, MicrobiologyNonculture, CodeStatus]:\n    \"\"\"\n    Load table data and create table object.\n\n    Parameters\n    ----------\n    table_name : str\n        Name of the table to load\n    sample_size : int, optional\n        Number of rows to load\n    columns : List[str], optional\n        Specific columns to load\n    filters : Dict, optional\n        Filters to apply when loading\n\n    Returns\n    -------\n    Union[Patient, Hospitalization, Adt, Labs, Vitals, MedicationAdminContinuous, PatientAssessments, RespiratorySupport, Position]\n        The loaded table object\n    \"\"\"\n    if table_name not in TABLE_CLASSES:\n        raise ValueError(f\"Unknown table: {table_name}. Available tables: {list(TABLE_CLASSES.keys())}\")\n\n    table_class = TABLE_CLASSES[table_name]\n    table_object = table_class.from_file(\n        data_directory=self.data_directory,\n        filetype=self.filetype,\n        timezone=self.timezone,\n        output_directory=self.output_directory,\n        sample_size=sample_size,\n        columns=columns,\n        filters=filters\n    )\n    setattr(self, table_name, table_object)\n    return table_object\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.validate_all","title":"validate_all","text":"<pre><code>validate_all()\n</code></pre> <p>Run validation on all loaded tables.</p> <p>This method runs the validate() method on each loaded table and reports the results.</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def validate_all(self):\n    \"\"\"\n    Run validation on all loaded tables.\n\n    This method runs the validate() method on each loaded table\n    and reports the results.\n    \"\"\"\n    loaded_tables = self.get_loaded_tables()\n\n    if not loaded_tables:\n        self.logger.info(\"No tables loaded to validate\")\n        return\n\n    self.logger.info(f\"Validating {len(loaded_tables)} table(s)\")\n\n    for table_name in loaded_tables:\n        table_obj = getattr(self, table_name)\n        self.logger.info(f\"Validating {table_name}\")\n        table_obj.validate()\n</code></pre>"},{"location":"api/tables/","title":"Table Classes","text":""},{"location":"api/tables/#patient","title":"Patient","text":""},{"location":"api/tables/#clifpy.tables.patient.Patient","title":"clifpy.tables.patient.Patient","text":"<pre><code>Patient(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient table wrapper inheriting from BaseTable.</p> <p>This class handles patient-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#adt-admission-discharge-transfer","title":"ADT (Admission, Discharge, Transfer)","text":""},{"location":"api/tables/#clifpy.tables.adt.Adt","title":"clifpy.tables.adt.Adt","text":"<pre><code>Adt(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>ADT (Admission/Discharge/Transfer) table wrapper inheriting from BaseTable.</p> <p>This class handles ADT-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the ADT table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the ADT table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: adt(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.check_overlapping_admissions","title":"check_overlapping_admissions","text":"<pre><code>check_overlapping_admissions(\n    save_overlaps=False, overlaps_output_directory=None\n)\n</code></pre> <p>Check for overlapping admissions within the same hospitalization.</p> <p>Identifies cases where a patient has overlapping stays in different locations within the same hospitalization (i.e., the out_dttm of one location is after the in_dttm of the next location).</p> <p>Parameters:     save_overlaps (bool): If True, save detailed overlap information to CSV. Default is False.     overlaps_output_directory (str, optional): Directory for saving the overlaps CSV file.          If None, uses the output_directory provided at initialization.</p> <p>Returns:     int: Count of unique hospitalizations that have overlapping admissions</p> <p>Raises:     RuntimeError: If an error occurs during processing</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def check_overlapping_admissions(self, save_overlaps: bool = False, overlaps_output_directory: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Check for overlapping admissions within the same hospitalization.\n\n    Identifies cases where a patient has overlapping stays in different locations\n    within the same hospitalization (i.e., the out_dttm of one location is after\n    the in_dttm of the next location).\n\n    Parameters:\n        save_overlaps (bool): If True, save detailed overlap information to CSV. Default is False.\n        overlaps_output_directory (str, optional): Directory for saving the overlaps CSV file. \n            If None, uses the output_directory provided at initialization.\n\n    Returns:\n        int: Count of unique hospitalizations that have overlapping admissions\n\n    Raises:\n        RuntimeError: If an error occurs during processing\n    \"\"\"\n    try:\n        if self.df is None:\n            return 0\n\n        if 'hospitalization_id' not in self.df.columns:\n            error = \"hospitalization_id is missing.\"\n            raise ValueError(error)\n\n        # Sort by hospitalization_id and in_dttm to make comparisons easier\n        data = self.df.sort_values(by=['hospitalization_id', 'in_dttm'])\n\n        overlaps = []\n        overlapping_hospitalizations = set()\n\n        # Group by hospitalization_id to compare bookings for each hospitalization\n        for hospitalization_id, group in data.groupby('hospitalization_id'):\n            for i in range(len(group) - 1):\n                # Current and next bookings\n                current = group.iloc[i]\n                next = group.iloc[i + 1]\n\n                # Check if the locations are different and times overlap\n                if (\n                    current['location_name'] != next['location_name'] and\n                    current['out_dttm'] &gt; next['in_dttm']\n                ):\n                    overlapping_hospitalizations.add(hospitalization_id)\n\n                    if save_overlaps:\n                        overlaps.append({\n                            'hospitalization_id': hospitalization_id,\n                            'Initial Location': current['location_name'],\n                            'Initial Location Category': current['location_category'],\n                            'Overlapping Location': next['location_name'],\n                            'Overlapping Location Category': next['location_category'],\n                            'Admission Start': current['in_dttm'],\n                            'Admission End': current['out_dttm'],\n                            'Next Admission Start': next['in_dttm']\n                        })\n\n        # Save overlaps to CSV if requested\n        if save_overlaps and overlaps:\n            overlaps_df = pd.DataFrame(overlaps)\n            # Determine the directory to save the overlaps file\n            save_dir = overlaps_output_directory if overlaps_output_directory is not None else self.output_directory\n            if save_dir is not None:\n                os.makedirs(save_dir, exist_ok=True)\n                file_path = os.path.join(save_dir, 'overlapping_admissions.csv')\n                overlaps_df.to_csv(file_path, index=False)\n            else:\n                # Fallback to original method if no directory is specified\n                self.save_dataframe(overlaps_df, 'overlapping_admissions')\n\n        return len(overlapping_hospitalizations)\n\n    except Exception as e:\n        # Handle errors gracefully\n        raise RuntimeError(f\"Error checking time overlap: {str(e)}\")\n</code></pre>"},{"location":"api/tables/#hospitalization","title":"Hospitalization","text":""},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization","title":"clifpy.tables.hospitalization.Hospitalization","text":"<pre><code>Hospitalization(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Hospitalization table wrapper inheriting from BaseTable.</p> <p>This class handles hospitalization-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the hospitalization table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the hospitalization table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: hospitalization(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.calculate_length_of_stay","title":"calculate_length_of_stay","text":"<pre><code>calculate_length_of_stay()\n</code></pre> <p>Calculate length of stay for each hospitalization and return DataFrame with LOS column.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def calculate_length_of_stay(self) -&gt; pd.DataFrame:\n    \"\"\"Calculate length of stay for each hospitalization and return DataFrame with LOS column.\"\"\"\n    if self.df is None:\n        return pd.DataFrame()\n\n    required_cols = ['admission_dttm', 'discharge_dttm']\n    if not all(col in self.df.columns for col in required_cols):\n        print(f\"Missing required columns: {[col for col in required_cols if col not in self.df.columns]}\")\n        return pd.DataFrame()\n\n    df_copy = self.df.copy()\n    df_copy['admission_dttm'] = pd.to_datetime(df_copy['admission_dttm'])\n    df_copy['discharge_dttm'] = pd.to_datetime(df_copy['discharge_dttm'])\n\n    # Calculate LOS in days\n    df_copy['length_of_stay_days'] = (df_copy['discharge_dttm'] - df_copy['admission_dttm']).dt.total_seconds() / (24 * 3600)\n\n    return df_copy\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_mortality_rate","title":"get_mortality_rate","text":"<pre><code>get_mortality_rate()\n</code></pre> <p>Calculate in-hospital mortality rate.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_mortality_rate(self) -&gt; float:\n    \"\"\"Calculate in-hospital mortality rate.\"\"\"\n    if self.df is None or 'discharge_category' not in self.df.columns:\n        return 0.0\n\n    total_hospitalizations = len(self.df)\n    if total_hospitalizations == 0:\n        return 0.0\n\n    expired_count = len(self.df[self.df['discharge_category'] == 'Expired'])\n    return (expired_count / total_hospitalizations) * 100\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_patient_hospitalization_counts","title":"get_patient_hospitalization_counts","text":"<pre><code>get_patient_hospitalization_counts()\n</code></pre> <p>Return DataFrame with hospitalization counts per patient.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_patient_hospitalization_counts(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with hospitalization counts per patient.\"\"\"\n    if self.df is None or 'patient_id' not in self.df.columns:\n        return pd.DataFrame()\n\n    patient_counts = (self.df.groupby('patient_id')\n                     .agg({\n                         'hospitalization_id': 'count',\n                         'admission_dttm': ['min', 'max']\n                     })\n                     .reset_index())\n\n    # Flatten column names\n    patient_counts.columns = ['patient_id', 'hospitalization_count', 'first_admission', 'last_admission']\n\n    # Calculate span of care\n    patient_counts['first_admission'] = pd.to_datetime(patient_counts['first_admission'])\n    patient_counts['last_admission'] = pd.to_datetime(patient_counts['last_admission'])\n    patient_counts['care_span_days'] = (patient_counts['last_admission'] - patient_counts['first_admission']).dt.total_seconds() / (24 * 3600)\n\n    return patient_counts.sort_values('hospitalization_count', ascending=False)\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_summary_stats","title":"get_summary_stats","text":"<pre><code>get_summary_stats()\n</code></pre> <p>Return comprehensive summary statistics for hospitalization data.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_summary_stats(self) -&gt; Dict:\n    \"\"\"Return comprehensive summary statistics for hospitalization data.\"\"\"\n    if self.df is None:\n        return {}\n\n    stats = {\n        'total_hospitalizations': len(self.df),\n        'unique_patients': self.df['patient_id'].nunique() if 'patient_id' in self.df.columns else 0,\n        'discharge_category_counts': self.df['discharge_category'].value_counts().to_dict() if 'discharge_category' in self.df.columns else {},\n        'admission_type_counts': self.df['admission_type_category'].value_counts().to_dict() if 'admission_type_category' in self.df.columns else {},\n        'date_range': {\n            'earliest_admission': self.df['admission_dttm'].min() if 'admission_dttm' in self.df.columns else None,\n            'latest_admission': self.df['admission_dttm'].max() if 'admission_dttm' in self.df.columns else None,\n            'earliest_discharge': self.df['discharge_dttm'].min() if 'discharge_dttm' in self.df.columns else None,\n            'latest_discharge': self.df['discharge_dttm'].max() if 'discharge_dttm' in self.df.columns else None\n        }\n    }\n\n    # Age statistics\n    if 'age_at_admission' in self.df.columns:\n        age_data = self.df['age_at_admission'].dropna()\n        if not age_data.empty:\n            stats['age_stats'] = {\n                'mean': round(age_data.mean(), 1),\n                'median': age_data.median(),\n                'min': age_data.min(),\n                'max': age_data.max(),\n                'std': round(age_data.std(), 1)\n            }\n\n    # Length of stay statistics\n    if all(col in self.df.columns for col in ['admission_dttm', 'discharge_dttm']):\n        los_df = self.calculate_length_of_stay()\n        if 'length_of_stay_days' in los_df.columns:\n            los_data = los_df['length_of_stay_days'].dropna()\n            if not los_data.empty:\n                stats['length_of_stay_stats'] = {\n                    'mean_days': round(los_data.mean(), 1),\n                    'median_days': round(los_data.median(), 1),\n                    'min_days': round(los_data.min(), 1),\n                    'max_days': round(los_data.max(), 1),\n                    'std_days': round(los_data.std(), 1)\n                }\n\n    # Mortality rate\n    stats['mortality_rate_percent'] = round(self.get_mortality_rate(), 2)\n\n    return stats\n</code></pre>"},{"location":"api/tables/#labs","title":"Labs","text":""},{"location":"api/tables/#clifpy.tables.labs.Labs","title":"clifpy.tables.labs.Labs","text":"<pre><code>Labs(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Labs table wrapper inheriting from BaseTable.</p> <p>This class handles laboratory data and validations including reference unit validation while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the labs table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the labs table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: labs(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Initialize lab reference units\n    self._lab_reference_units = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load lab-specific schema data\n    self._load_labs_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_category_stats","title":"get_lab_category_stats","text":"<pre><code>get_lab_category_stats()\n</code></pre> <p>Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_category_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.\"\"\"\n    if (\n        self.df is None\n        or 'lab_value_numeric' not in self.df.columns\n        or 'hospitalization_id' not in self.df.columns        # remove this line if hosp-id is optional\n    ):\n        return {\"status\": \"Missing columns\"}\n\n    stats = (\n        self.df\n        .groupby('lab_category')\n        .agg(\n            count=('lab_value_numeric', 'count'),\n            unique=('hospitalization_id', 'nunique'),\n            missing_pct=('lab_value_numeric', lambda x: 100 * x.isna().mean()),\n            mean=('lab_value_numeric', 'mean'),\n            std=('lab_value_numeric', 'std'),\n            min=('lab_value_numeric', 'min'),\n            q1=('lab_value_numeric', lambda x: x.quantile(0.25)),\n            median=('lab_value_numeric', 'median'),\n            q3=('lab_value_numeric', lambda x: x.quantile(0.75)),\n            max=('lab_value_numeric', 'max'),\n        )\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_reference_units","title":"get_lab_reference_units","text":"<pre><code>get_lab_reference_units(save=False, output_directory=None)\n</code></pre> <p>Get all unique reference units observed in the data, grouped by lab_category along with their counts.</p> <p>Uses Polars for efficient processing of large datasets, with automatic fallback to pandas if Polars is unavailable or fails.</p> <p>Parameters:</p> Name Type Description Default <code>save</code> <code>bool</code> <p>If True, save the results to the output directory as a CSV file.</p> <code>False</code> <code>output_directory</code> <code>str</code> <p>Directory to save results. If None, uses self.output_directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: ['lab_category', 'reference_unit', 'count']</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_reference_units(\n    self,\n    save: bool = False,\n    output_directory: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get all unique reference units observed in the data,\n    grouped by lab_category along with their counts.\n\n    Uses Polars for efficient processing of large datasets, with automatic\n    fallback to pandas if Polars is unavailable or fails.\n\n    Parameters\n    ----------\n    save : bool, default False\n        If True, save the results to the output directory as a CSV file.\n    output_directory : str, optional\n        Directory to save results. If None, uses self.output_directory.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with columns: ['lab_category', 'reference_unit', 'count']\n    \"\"\"\n    if self.df is None:\n        raise ValueError(\"No data\")\n\n    # Try Polars first (more efficient for large data), fall back to pandas\n    try:\n        result_pl = self._get_lab_reference_units_polars()\n        result_df = result_pl.to_pandas()\n        self.logger.debug(\"Used Polars for get_lab_reference_units\")\n    except Exception as e:\n        self.logger.debug(f\"Polars failed ({e}), falling back to pandas\")\n        # Ensure we have a pandas DataFrame for the fallback\n        if not isinstance(self.df, pd.DataFrame):\n            try:\n                self.df = self.df.to_pandas() if hasattr(self.df, 'to_pandas') else pd.DataFrame(self.df)\n            except Exception:\n                return pd.DataFrame(columns=['lab_category', 'reference_unit', 'count'])\n        result_df = self._get_lab_reference_units_pandas()\n\n    if save:\n        save_dir = output_directory if output_directory is not None else self.output_directory\n        os.makedirs(save_dir, exist_ok=True)\n        csv_path = os.path.join(save_dir, 'lab_reference_units.csv')\n        result_df.to_csv(csv_path, index=False)\n        self.logger.info(f\"Saved lab reference units to {csv_path}\")\n\n    return result_df\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_specimen_stats","title":"get_lab_specimen_stats","text":"<pre><code>get_lab_specimen_stats()\n</code></pre> <p>Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_specimen_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.\"\"\"\n    if (\n        self.df is None\n        or 'lab_value_numeric' not in self.df.columns\n        or 'hospitalization_id' not in self.df.columns \n        or 'lab_speciment_category' not in self.df.columns       # remove this line if hosp-id is optional\n    ):\n        return {\"status\": \"Missing columns\"}\n\n    stats = (\n        self.df\n        .groupby('lab_specimen_category')\n        .agg(\n            count=('lab_value_numeric', 'count'),\n            unique=('hospitalization_id', 'nunique'),\n            missing_pct=('lab_value_numeric', lambda x: 100 * x.isna().mean()),\n            mean=('lab_value_numeric', 'mean'),\n            std=('lab_value_numeric', 'std'),\n            min=('lab_value_numeric', 'min'),\n            q1=('lab_value_numeric', lambda x: x.quantile(0.25)),\n            median=('lab_value_numeric', 'median'),\n            q3=('lab_value_numeric', lambda x: x.quantile(0.75)),\n            max=('lab_value_numeric', 'max'),\n        )\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.standardize_reference_units","title":"standardize_reference_units","text":"<pre><code>standardize_reference_units(\n    inplace=True,\n    save=False,\n    lowercase=False,\n    output_directory=None,\n)\n</code></pre> <p>Standardize reference unit strings to match the schema's target units.</p> <p>Uses Polars for efficient processing of large datasets, with automatic fallback to pandas if Polars is unavailable or fails.</p> <p>Uses fuzzy matching to detect similar unit strings (e.g., 'mmhg' -&gt; 'mmHg', '10*3/ul' -&gt; '10^3/\u03bcL', 'hr' -&gt; 'hour') and converts them to the preferred target unit defined in the schema.</p> <p>This does NOT perform value conversions between different unit types (e.g., mg/dL to mmol/L). Units that don't match any target will be logged as warnings.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modify self.df in place. If False, return a copy.</p> <code>True</code> <code>save</code> <code>bool</code> <p>If True, save a CSV of the unit mappings applied to the output directory.</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>If True, convert all reference units to lowercase instead of using the schema's original casing (e.g., 'mg/dl' instead of 'mg/dL').</p> <code>False</code> <code>output_directory</code> <code>str</code> <p>Directory to save results. If None, uses self.output_directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>If inplace=False, returns the modified DataFrame. Otherwise None.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def standardize_reference_units(\n    self,\n    inplace: bool = True,\n    save: bool = False,\n    lowercase: bool = False,\n    output_directory: Optional[str] = None\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Standardize reference unit strings to match the schema's target units.\n\n    Uses Polars for efficient processing of large datasets, with automatic\n    fallback to pandas if Polars is unavailable or fails.\n\n    Uses fuzzy matching to detect similar unit strings (e.g., 'mmhg' -&gt; 'mmHg',\n    '10*3/ul' -&gt; '10^3/\u03bcL', 'hr' -&gt; 'hour') and converts them to the preferred\n    target unit defined in the schema.\n\n    This does NOT perform value conversions between different unit types\n    (e.g., mg/dL to mmol/L). Units that don't match any target will be logged\n    as warnings.\n\n    Parameters\n    ----------\n    inplace : bool, default True\n        If True, modify self.df in place. If False, return a copy.\n    save : bool, default False\n        If True, save a CSV of the unit mappings applied to the output directory.\n    lowercase : bool, default False\n        If True, convert all reference units to lowercase instead of using\n        the schema's original casing (e.g., 'mg/dl' instead of 'mg/dL').\n    output_directory : str, optional\n        Directory to save results. If None, uses self.output_directory.\n\n    Returns\n    -------\n    Optional[pd.DataFrame]\n        If inplace=False, returns the modified DataFrame. Otherwise None.\n    \"\"\"\n    if self.df is None:\n        raise ValueError(\n            \"No data loaded. Please provide data using one of these methods:\\n\"\n            \"  1. Labs.from_file(data_directory=..., filetype=..., timezone=...)\\n\"\n            \"  2. Labs(data=your_dataframe)\"\n        )\n\n    # Check for required columns\n    missing = self._validate_required_columns({'lab_category', 'reference_unit'})\n    if missing:\n        raise ValueError(f\"Required columns not found: {missing}\")\n\n    if not self._lab_reference_units:\n        self.logger.warning(\"No lab reference units defined in schema\")\n        return None\n\n    # Get unique combinations (works for both pandas and polars)\n    try:\n        import polars as pl\n        if isinstance(self.df, (pl.DataFrame, pl.LazyFrame)):\n            if isinstance(self.df, pl.LazyFrame):\n                unique_combos_df = (\n                    self.df\n                    .select(['lab_category', 'reference_unit'])\n                    .unique()\n                    .collect()\n                    .to_pandas()\n                )\n            else:\n                unique_combos_df = (\n                    self.df\n                    .select(['lab_category', 'reference_unit'])\n                    .unique()\n                    .to_pandas()\n                )\n        else:\n            unique_combos_df = self.df[['lab_category', 'reference_unit']].drop_duplicates()\n    except ImportError:\n        unique_combos_df = self.df[['lab_category', 'reference_unit']].drop_duplicates()\n\n    # Build mapping dictionary (shared logic)\n    unit_mapping, mappings_applied, unmatched_units = self._build_unit_mapping(\n        unique_combos_df, lowercase\n    )\n\n    # Try Polars first, fall back to pandas\n    try:\n        result_df = self._standardize_reference_units_polars(unit_mapping, lowercase)\n        self.logger.debug(\"Used Polars for standardize_reference_units\")\n    except Exception as e:\n        self.logger.debug(f\"Polars failed ({e}), falling back to pandas\")\n        # Ensure we have a pandas DataFrame for the fallback\n        if not isinstance(self.df, pd.DataFrame):\n            try:\n                import polars as pl\n                if isinstance(self.df, (pl.DataFrame, pl.LazyFrame)):\n                    if isinstance(self.df, pl.LazyFrame):\n                        self.df = self.df.collect().to_pandas()\n                    else:\n                        self.df = self.df.to_pandas()\n                else:\n                    self.df = pd.DataFrame(self.df)\n            except Exception:\n                raise ValueError(\"Could not convert data to pandas DataFrame\")\n        result_df = self._standardize_reference_units_pandas(unit_mapping, lowercase)\n\n    # Handle inplace at API level\n    if inplace:\n        self.df = result_df\n\n    # Log results\n    if unit_mapping:\n        actual_mappings = [m for m in mappings_applied if not m.get('silent')]\n        if actual_mappings:\n            self.logger.info(f\"Applied {len(actual_mappings)} unit standardizations\")\n    elif not lowercase:\n        self.logger.info(\"No unit standardizations needed\")\n\n    # Warn about unmatched units\n    for item in unmatched_units:\n        self.logger.warning(\n            f\"Unmatched unit '{item['source_unit']}' for {item['lab_category']}. \"\n            f\"Expected one of: {item['expected_units']}\"\n        )\n\n    # Save mapping if requested\n    if save and mappings_applied:\n        save_dir = output_directory if output_directory is not None else self.output_directory\n        os.makedirs(save_dir, exist_ok=True)\n        mapping_df = pd.DataFrame(mappings_applied)\n        csv_path = os.path.join(save_dir, 'lab_reference_unit_standardized.csv')\n        mapping_df.to_csv(csv_path, index=False)\n        self.logger.info(f\"Saved unit mappings to {csv_path}\")\n\n    if not inplace:\n        # Convert to pandas if returning\n        try:\n            import polars as pl\n            if isinstance(result_df, pl.DataFrame):\n                return result_df.to_pandas()\n        except ImportError:\n            pass\n        return result_df\n\n    return None\n</code></pre>"},{"location":"api/tables/#microbiology-culture","title":"Microbiology Culture","text":""},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture","title":"clifpy.tables.microbiology_culture.MicrobiologyCulture","text":"<pre><code>MicrobiologyCulture(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Microbiology Culture table wrapper inheriting from BaseTable.</p> <p>This class handles microbiology culture-specific data and validations including organism identification validation and culture method validation.</p> <p>Initialize the microbiology culture table.</p> <p>Parameters:     data_directory (str): Path to the directory containing data files     filetype (str): Type of data file (csv, parquet, etc.)     timezone (str): Timezone for datetime columns     output_directory (str, optional): Directory for saving output files and logs     data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the microbiology culture table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Initialize time order validation errors list\n    self.time_order_validation_errors: List[dict] = []\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.cat_vs_name_map","title":"cat_vs_name_map  <code>staticmethod</code>","text":"<pre><code>cat_vs_name_map(\n    df,\n    category_col,\n    name_col,\n    *,\n    group_col=None,\n    dropna=True,\n    sort=\"freq_then_alpha\",\n    max_names_per_cat=None,\n    include_counts=False\n)\n</code></pre> <p>Build mappings from category\u2192names (2-level) or group\u2192category\u2192names (3-level).</p> <p>Returns: - if group_col is None:         { category: [names...] }  or  { category: [{\"name\":..., \"n\":...}, ...] } - if group_col is provided:         { group: { category: [names...] } }  or         { group: { category: [{\"name\":..., \"n\":...}, ...] } }</p> <p>Notes - Names are unique per (category[, group]) and sorted by:     freq desc, then alpha  (default), or alpha only if sort=\"alpha\" - Set include_counts=True to return [{\"name\":..., \"n\":...}] instead of plain strings. - Set max_names_per_cat to truncate long lists per category.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>@staticmethod\ndef cat_vs_name_map(\n    df: pd.DataFrame,\n    category_col: str,\n    name_col: str,\n    *,\n    group_col: Optional[str] = None,                 # \u2190 if provided, returns {group: {cat: [names...]}}\n    dropna: bool = True,\n    sort: Literal[\"freq_then_alpha\", \"alpha\"] = \"freq_then_alpha\",\n    max_names_per_cat: Optional[int] = None,\n    include_counts: bool = False,                    # if True \u2192 lists of {\"name\":..., \"n\":...}\n) -&gt; Union[Dict[str, List[str]], Dict[str, Dict[str, List[str]]],\n        Dict[str, Dict[str, List[Dict[str, int]]]], Dict[str, List[Dict[str, int]]]]:\n    \"\"\"\n    Build mappings from category\u2192names (2-level) or group\u2192category\u2192names (3-level).\n\n    Returns:\n    - if group_col is None:\n            { category: [names...] }  or  { category: [{\"name\":..., \"n\":...}, ...] }\n    - if group_col is provided:\n            { group: { category: [names...] } }  or\n            { group: { category: [{\"name\":..., \"n\":...}, ...] } }\n\n    Notes\n    - Names are unique per (category[, group]) and sorted by:\n        freq desc, then alpha  (default), or alpha only if sort=\"alpha\"\n    - Set include_counts=True to return [{\"name\":..., \"n\":...}] instead of plain strings.\n    - Set max_names_per_cat to truncate long lists per category.\n    \"\"\"\n    if df is None:\n        return {}\n\n    required = [category_col, name_col] + ([group_col] if group_col else [])\n    if any(col not in df.columns for col in required):\n        return {}\n\n    sub = df[required].copy()\n    if dropna:\n        sub = sub.dropna(subset=required)\n\n    # frequency at the most granular level available\n    group_by_cols = ([group_col] if group_col else []) + [category_col, name_col]\n    counts = (\n        sub.groupby(group_by_cols)\n        .size()\n        .reset_index(name=\"n\")\n    )\n\n    def _sort_block(block: pd.DataFrame) -&gt; pd.DataFrame:\n        if sort == \"alpha\":\n            return block.sort_values([name_col], ascending=[True], kind=\"mergesort\")\n        # default: freq desc then alpha\n        return block.sort_values([\"n\", name_col], ascending=[False, True], kind=\"mergesort\")\n\n    def _emit_names(block: pd.DataFrame):\n        if include_counts:\n            out = [{\"name\": str(r[name_col]), \"n\": int(r[\"n\"])} for _, r in block.iterrows()]\n        else:\n            out = block[name_col].astype(str).tolist()\n        if max_names_per_cat is not None:\n            out = out[:max_names_per_cat]\n        return out\n\n    if group_col:\n        # 3-level: group \u2192 category \u2192 [names or {\"name\",\"n\"}]\n        result: Dict[str, Dict[str, List[Union[str, Dict[str, int]]]]] = {}\n        for grp_val, grp_block in counts.groupby(group_col, sort=False):\n            cat_map: Dict[str, List[Union[str, Dict[str, int]]]] = {}\n            for cat_val, cat_block in grp_block.groupby(category_col, sort=False):\n                sorted_block = _sort_block(cat_block)\n                cat_map[str(cat_val)] = _emit_names(sorted_block)\n            result[str(grp_val)] = cat_map\n        return result\n    else:\n        # 2-level: category \u2192 [names or {\"name\",\"n\"}]\n        result2: Dict[str, List[Union[str, Dict[str, int]]]] = {}\n        for cat_val, cat_block in counts.groupby(category_col, sort=False):\n            sorted_block = _sort_block(cat_block)\n            result2[str(cat_val)] = _emit_names(sorted_block)\n        return result2\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Return <code>True</code> if the last validation finished without errors.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"Return ``True`` if the last validation finished without errors.\"\"\"\n    return not self.errors and not self.time_order_validation_errors\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.top_fluid_org_outliers","title":"top_fluid_org_outliers","text":"<pre><code>top_fluid_org_outliers(\n    level=\"organism_group\", min_count=0, top_k=10\n)\n</code></pre> <p>Identify top positive and negative outliers in fluid_category vs organism_group or organism_category.</p> <p>Parameters:     level (str): \"organism_group\" or \"organism_category\" (non-standard)     min_count (int): Minimum observed count to consider     top_k (int): Number of top positive and negative outliers to return</p> <p>Returns:     Dict with keys \"top_positive\" and \"top_negative\", each containing a DataFrame of outliers.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def top_fluid_org_outliers(\n    self,\n    level: Literal[\"organism_group\", \"organism_category\"] = \"organism_group\",\n    min_count: int = 0,\n    top_k: int = 10,\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Identify top positive and negative outliers in fluid_category vs organism_group or organism_category.\n\n    Parameters:\n        level (str): \"organism_group\" or \"organism_category\" (non-standard)\n        min_count (int): Minimum observed count to consider\n        top_k (int): Number of top positive and negative outliers to return\n\n    Returns:\n        Dict with keys \"top_positive\" and \"top_negative\", each containing a DataFrame of outliers.\n    \"\"\"\n    tbl = pd.crosstab(self.df[\"fluid_category\"], self.df[level])\n    if tbl.empty:\n        return {\"top_positive\": pd.DataFrame(), \"top_negative\": pd.DataFrame()}\n\n    total = tbl.values.sum()\n    exp = (tbl.sum(1).values.reshape(-1,1) @ tbl.sum(0).values.reshape(1,-1)) / total\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        z = (tbl.values - exp) / np.sqrt(exp)\n\n    long = pd.DataFrame({\n        \"fluid_category\": np.repeat(tbl.index.values, tbl.shape[1]),\n        level: np.tile(tbl.columns.values, tbl.shape[0]),\n        \"observed\": tbl.values.ravel().astype(float),\n        \"expected\": exp.ravel().astype(float),\n        \"std_resid\": z.ravel().astype(float),\n    }).dropna()\n\n    long = long[long[\"observed\"] &gt;= min_count]\n    top_pos = long.sort_values(\"std_resid\", ascending=False).head(top_k).reset_index(drop=True)\n    top_neg = long.sort_values(\"std_resid\", ascending=True).head(top_k).reset_index(drop=True)\n    return {\"top_positive\": top_pos, \"top_negative\": top_neg}\n</code></pre>"},{"location":"api/tables/#clifpy.tables.microbiology_culture.MicrobiologyCulture.validate_timestamp_order","title":"validate_timestamp_order","text":"<pre><code>validate_timestamp_order()\n</code></pre> <p>Check that order_dttm \u2264 collect_dttm \u2264 result_dttm. - Resets self.time_order_validation_errors - Adds one entry per violated rule - Extends self.errors and logs: 'Found {len(self.time_order_validation_errors)} time order validation errors' Returns a dataframe of all violating rows (union of both rules) or None if OK.</p> Source code in <code>clifpy/tables/microbiology_culture.py</code> <pre><code>def validate_timestamp_order(self):\n    \"\"\"\n    Check that order_dttm \u2264 collect_dttm \u2264 result_dttm.\n    - Resets self.time_order_validation_errors\n    - Adds one entry per violated rule\n    - Extends self.errors and logs: 'Found {len(self.time_order_validation_errors)} time order validation errors'\n    Returns a dataframe of all violating rows (union of both rules) or None if OK.\n    \"\"\"\n    # Reset time order validation bucket\n    self.time_order_validation_errors = []\n\n    df = self.df\n    key_cols = [\"patient_id\", \"hospitalization_id\", \"organism_id\"]\n    time_cols = [\"order_dttm\", \"collect_dttm\", \"result_dttm\"]\n\n    # Check for missing columns\n    missing = [col for col in time_cols if col not in df.columns]\n    if missing:\n        msg = (\n            f\"Missing required timestamp columns for time order validation: {', '.join(missing)}\"\n        )\n        self.time_order_validation_errors.append({\n            \"type\": \"missing_time_order_columns\",\n            \"columns\": missing,\n            \"message\": msg,\n            \"table\": getattr(self, \"table_name\", \"unknown\"),\n        })\n        if hasattr(self, \"errors\"):\n            self.errors.extend(self.time_order_validation_errors)\n        self.logger.warning(msg)\n        return None\n\n    grace = pd.Timedelta(minutes=1)\n\n    # Flag if order is \u2265 1 minute after collect (allow small jitter where collect \u2265 order within 1 min)\n    m_order_ge_collect = (df[\"order_dttm\"] - df[\"collect_dttm\"]) &gt;= grace\n\n    # Flag if collect is \u2265 1 minute after result (allow small jitter where result \u2265 collect within 1 min)\n    m_collect_ge_result = (df[\"collect_dttm\"] - df[\"result_dttm\"]) &gt;= grace\n\n    n1 = int(m_order_ge_collect.sum())\n    n2 = int(m_collect_ge_result.sum())\n\n    if n1 &gt; 0:\n        self.time_order_validation_errors.append({\n            \"type\": \"time_order_validation\",\n            \"rule\": \"order_dttm &lt;= collect_dttm, grace 1 min\",\n            \"message\": f\"{n1} rows have order_dttm &gt; collect_dttm\",\n            \"rows\": n1,\n            \"table\": getattr(self, \"table_name\", \"unknown\"),\n        })\n    if n2 &gt; 0:\n        self.time_order_validation_errors.append({\n            \"type\": \"time_order_validation\",\n            \"rule\": \"collect_dttm &lt;= result_dttm, grace 1 min\",\n            \"message\": f\"{n2} rows have collect_dttm &gt; result_dttm\",\n            \"rows\": n2,\n            \"table\": getattr(self, \"table_name\", \"unknown\"),\n        })\n\n    # Add range validation errors to main errors list (exact logging style)\n    if self.time_order_validation_errors:\n        if hasattr(self, \"errors\"):\n            self.errors.extend(self.time_order_validation_errors)\n        self.logger.warning(f\"Found {len(self.time_order_validation_errors)} range validation errors\")\n\n    # Return violating rows (union), showing keys + timestamps\n    any_bad = m_order_ge_collect | m_collect_ge_result\n    if any_bad.any():\n        show_cols = [*key_cols, \"order_dttm\", \"collect_dttm\", \"result_dttm\"]\n        return df.loc[any_bad, [c for c in show_cols if c in df.columns]].copy()\n\n    # Nothing to report\n    self.logger.info(\"validate_timestamp_order: passed (no violations)\")\n    return None\n</code></pre>"},{"location":"api/tables/#vitals","title":"Vitals","text":""},{"location":"api/tables/#clifpy.tables.vitals.Vitals","title":"clifpy.tables.vitals.Vitals","text":"<pre><code>Vitals(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Vitals table wrapper inheriting from BaseTable.</p> <p>This class handles vitals-specific data and validations including range validation for vital signs.</p> <p>Initialize the vitals table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the vitals table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Initialize range validation errors list\n    self.range_validation_errors: List[dict] = []\n\n    # Load vital ranges and units from schema\n    self._vital_units = None\n    self._vital_ranges = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load vital-specific schema data\n    self._load_vitals_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.vital_ranges","title":"vital_ranges  <code>property</code>","text":"<pre><code>vital_ranges\n</code></pre> <p>Get the vital ranges from the schema.</p>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.vital_units","title":"vital_units  <code>property</code>","text":"<pre><code>vital_units\n</code></pre> <p>Get the vital units mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.filter_by_vital_category","title":"filter_by_vital_category","text":"<pre><code>filter_by_vital_category(vital_category)\n</code></pre> <p>Return all records for a specific vital category (e.g., 'heart_rate', 'temp_c').</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def filter_by_vital_category(self, vital_category: str) -&gt; pd.DataFrame:\n    \"\"\"Return all records for a specific vital category (e.g., 'heart_rate', 'temp_c').\"\"\"\n    if self.df is None or 'vital_category' not in self.df.columns:\n        return pd.DataFrame()\n\n    return self.df[self.df['vital_category'] == vital_category].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.get_vital_summary_stats","title":"get_vital_summary_stats","text":"<pre><code>get_vital_summary_stats()\n</code></pre> <p>Return summary statistics for each vital category.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def get_vital_summary_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each vital category.\"\"\"\n    if self.df is None or 'vital_value' not in self.df.columns:\n        return pd.DataFrame()\n\n    # Convert vital_value to numeric\n    df_copy = self.df.copy()\n    df_copy['vital_value'] = pd.to_numeric(df_copy['vital_value'], errors='coerce')\n\n    # Group by vital category and calculate stats\n    stats = df_copy.groupby('vital_category')['vital_value'].agg([\n        'count', 'mean', 'std', 'min', 'max',\n        ('q1', lambda x: x.quantile(0.25)),\n        ('median', lambda x: x.quantile(0.5)),\n        ('q3', lambda x: x.quantile(0.75))\n    ]).round(2)\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Return <code>True</code> if the last validation finished without errors.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"Return ``True`` if the last validation finished without errors.\"\"\"\n    return not self.errors and not self.range_validation_errors\n</code></pre>"},{"location":"api/tables/#respiratory-support","title":"Respiratory Support","text":""},{"location":"api/tables/#clifpy.tables.respiratory_support.RespiratorySupport","title":"clifpy.tables.respiratory_support.RespiratorySupport","text":"<pre><code>RespiratorySupport(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Respiratory support table wrapper inheriting from BaseTable.</p> <p>This class handles respiratory support data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the respiratory_support table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/respiratory_support.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the respiratory_support table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.respiratory_support.RespiratorySupport.waterfall","title":"waterfall","text":"<pre><code>waterfall(\n    *,\n    id_col=\"hospitalization_id\",\n    bfill=False,\n    verbose=True,\n    return_dataframe=False\n)\n</code></pre> <p>Clean + waterfall-fill the respiratory_support table.</p> <p>Parameters:</p> Name Type Description Default <code>id_col</code> <code>str</code> <p>Encounter-level identifier column (default: hospitalization_id)</p> <code>'hospitalization_id'</code> <code>bfill</code> <code>bool</code> <p>If True, numeric setters are back-filled after forward-fill</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print progress messages</p> <code>True</code> <code>return_dataframe</code> <code>bool</code> <p>If True, returns DataFrame instead of RespiratorySupport instance</p> <code>False</code> <p>Returns:</p> Type Description <code>RespiratorySupport</code> <p>New instance with processed data (or DataFrame if return_dataframe=True)</p> Notes <p>The waterfall function expects data in UTC timezone. If your data is in a different timezone, it will be converted to UTC for processing, then converted back to the original timezone on return. The original object is not modified.</p> Source code in <code>clifpy/tables/respiratory_support.py</code> <pre><code>    def waterfall(\n    self,\n    *,\n    id_col: str = \"hospitalization_id\",\n    bfill: bool = False,\n    verbose: bool = True,\n    return_dataframe: bool = False\n) -&gt; Union['RespiratorySupport', pd.DataFrame]:\n        \"\"\"\n        Clean + waterfall-fill the respiratory_support table.\n\n        Parameters\n        ----------\n        id_col : str\n            Encounter-level identifier column (default: hospitalization_id)\n        bfill : bool\n            If True, numeric setters are back-filled after forward-fill\n        verbose : bool\n            Print progress messages\n        return_dataframe : bool\n            If True, returns DataFrame instead of RespiratorySupport instance\n\n        Returns\n        -------\n        RespiratorySupport\n            New instance with processed data (or DataFrame if return_dataframe=True)\n\n        Notes\n        -----\n        The waterfall function expects data in UTC timezone. If your data is in a\n        different timezone, it will be converted to UTC for processing, then converted\n        back to the original timezone on return. The original object is not modified.\n        \"\"\"\n        if self.df is None or self.df.empty:\n            raise ValueError(\"No data available to process. Load data first.\")\n\n        # Work on a copy\n        df_copy = self.df.copy()\n\n        # --- Capture original tz (if any), convert to UTC for processing\n        original_tz = None\n        if 'recorded_dttm' in df_copy.columns:\n            if pd.api.types.is_datetime64tz_dtype(df_copy['recorded_dttm']):\n                original_tz = df_copy['recorded_dttm'].dt.tz\n                if verbose and str(original_tz) != 'UTC':\n                    print(f\"Converting timezone from {original_tz} to UTC for waterfall processing\")\n                # Convert to UTC (no-op if already UTC)\n                df_copy['recorded_dttm'] = df_copy['recorded_dttm'].dt.tz_convert('UTC')\n            else:\n                # tz-naive; leave as-is (function expects UTC semantics already)\n                original_tz = None\n\n        # --- Run the waterfall (expects UTC)\n        processed_df = process_resp_support_waterfall(\n            df_copy,\n            id_col=id_col,\n            bfill=bfill,\n            verbose=verbose\n        )\n\n        # --- Convert back to original tz if we had one\n        if original_tz is not None:\n            # Guard: ensure tz-aware before tz_convert\n            if pd.api.types.is_datetime64tz_dtype(processed_df['recorded_dttm']):\n                if verbose and str(original_tz) != 'UTC':\n                    print(f\"Converting timezone from UTC back to {original_tz} after processing\")\n                processed_df = processed_df.copy()\n                processed_df['recorded_dttm'] = processed_df['recorded_dttm'].dt.tz_convert(original_tz)\n            else:\n                # If something made it tz-naive, localize then convert (shouldn't happen, but safe)\n                processed_df = processed_df.copy()\n                processed_df['recorded_dttm'] = (\n                    processed_df['recorded_dttm']\n                    .dt.tz_localize('UTC')\n                    .dt.tz_convert(original_tz)\n                )\n\n        # Return DataFrame if requested\n        if return_dataframe:\n            return processed_df\n\n        # Otherwise, return a new wrapped instance (timezone metadata stays the same as the current object)\n        return RespiratorySupport(\n            data_directory=self.data_directory,\n            filetype=self.filetype,\n            timezone=self.timezone,  # this is your package-level field; we keep it unchanged\n            output_directory=self.output_directory,\n            data=processed_df\n        )\n</code></pre>"},{"location":"api/tables/#medication-administration-continuous","title":"Medication Administration (Continuous)","text":""},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous","title":"clifpy.tables.medication_admin_continuous.MedicationAdminContinuous","text":"<pre><code>MedicationAdminContinuous(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Medication administration continuous table wrapper inheriting from BaseTable.</p> <p>This class handles medication administration continuous data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the MedicationAdminContinuous table.</p> <p>This class handles continuous medication administration data, including validation, dose unit standardization, and unit conversion capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files. If None and data is provided, defaults to current directory.</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.). If None and data is provided, defaults to 'parquet'.</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns. Used for proper timestamp handling.</p> <code>\"UTC\"</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not specified, outputs are saved to the current working directory.</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded DataFrame to use instead of loading from file. Supports backward compatibility with direct DataFrame initialization.</p> <code>None</code> Notes <p>The class supports two initialization patterns: 1. Loading from file: provide data_directory and filetype 2. Direct DataFrame: provide data parameter (legacy support)</p> <p>Upon initialization, the class loads medication schema data including category-to-group mappings from the YAML schema.</p> Source code in <code>clifpy/tables/medication_admin_continuous.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the MedicationAdminContinuous table.\n\n    This class handles continuous medication administration data, including validation,\n    dose unit standardization, and unit conversion capabilities.\n\n    Parameters\n    ----------\n    data_directory : str, optional\n        Path to the directory containing data files. If None and data is provided,\n        defaults to current directory.\n    filetype : str, optional\n        Type of data file (csv, parquet, etc.). If None and data is provided,\n        defaults to 'parquet'.\n    timezone : str, default=\"UTC\"\n        Timezone for datetime columns. Used for proper timestamp handling.\n    output_directory : str, optional\n        Directory for saving output files and logs. If not specified, outputs\n        are saved to the current working directory.\n    data : pd.DataFrame, optional\n        Pre-loaded DataFrame to use instead of loading from file. Supports\n        backward compatibility with direct DataFrame initialization.\n\n    Notes\n    -----\n    The class supports two initialization patterns:\n    1. Loading from file: provide data_directory and filetype\n    2. Direct DataFrame: provide data parameter (legacy support)\n\n    Upon initialization, the class loads medication schema data including\n    category-to-group mappings from the YAML schema.\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: medication_admin_continuous(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Load medication mappings\n    self._med_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load medication-specific schema data\n    self._load_medication_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping","title":"med_category_to_group_mapping  <code>property</code>","text":"<pre><code>med_category_to_group_mapping\n</code></pre> <p>Get the medication category to group mapping from the schema.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>A dictionary mapping medication categories to their therapeutic groups. Returns a copy to prevent external modification of the internal mapping. Returns an empty dict if no mappings are loaded.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mac = MedicationAdminContinuous(data)\n&gt;&gt;&gt; mappings = mac.med_category_to_group_mapping\n&gt;&gt;&gt; mappings['Antibiotics']\n'Antimicrobials'\n</code></pre>"},{"location":"api/tables/#patient-assessments","title":"Patient Assessments","text":""},{"location":"api/tables/#clifpy.tables.patient_assessments.PatientAssessments","title":"clifpy.tables.patient_assessments.PatientAssessments","text":"<pre><code>PatientAssessments(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient assessments table wrapper inheriting from BaseTable.</p> <p>This class handles patient assessment data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient_assessments table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient_assessments.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient_assessments table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: patient_assessments(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Initialize assessment mappings\n    self._assessment_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load assessment-specific schema data\n    self._load_assessment_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.patient_assessments.PatientAssessments.assessment_category_to_group_mapping","title":"assessment_category_to_group_mapping  <code>property</code>","text":"<pre><code>assessment_category_to_group_mapping\n</code></pre> <p>Get the assessment category to group mapping from the schema.</p>"},{"location":"api/tables/#position","title":"Position","text":""},{"location":"api/tables/#clifpy.tables.position.Position","title":"clifpy.tables.position.Position","text":"<pre><code>Position(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Position table wrapper inheriting from BaseTable.</p> <p>This class handles patient position data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the position table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/position.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the position table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: position(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.position.Position.get_position_category_stats","title":"get_position_category_stats","text":"<pre><code>get_position_category_stats()\n</code></pre> <p>Return summary statistics for each position category, including missingness and unique patient counts. Expects columns: 'position_category', 'position_name', and optionally 'hospitalization_id'.</p> Source code in <code>clifpy/tables/position.py</code> <pre><code>def get_position_category_stats(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Return summary statistics for each position category, including missingness and unique patient counts.\n    Expects columns: 'position_category', 'position_name', and optionally 'hospitalization_id'.\n    \"\"\"\n    if self.df is None or 'position_category' not in self.df.columns or 'hospitalization_id' not in self.df.columns:\n        return {\"status\": \"Missing columns\"}\n\n    agg_dict = {\n        'count': ('position_category', 'count'),\n        'unique': ('hospitalization_id', 'nunique'),\n    }\n\n    stats = (\n        self.df\n        .groupby('position_category')\n        .agg(**agg_dict)\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#medication-administration-intermittent","title":"Medication Administration (Intermittent)","text":""},{"location":"api/tables/#clifpy.tables.medication_admin_intermittent.MedicationAdminIntermittent","title":"clifpy.tables.medication_admin_intermittent.MedicationAdminIntermittent","text":"<pre><code>MedicationAdminIntermittent(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Medication administration intermittent table wrapper inheriting from BaseTable.</p> <p>This class handles medication administration intermittent data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the MedicationAdminIntermittent table.</p> <p>This class handles intermittent medication administration data, including validation, dose unit standardization, and unit conversion capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files. If None and data is provided, defaults to current directory.</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.). If None and data is provided, defaults to 'parquet'.</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns. Used for proper timestamp handling.</p> <code>\"UTC\"</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not specified, outputs are saved to the current working directory.</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded DataFrame to use instead of loading from file. Supports backward compatibility with direct DataFrame initialization.</p> <code>None</code> Notes <p>The class supports two initialization patterns: 1. Loading from file: provide data_directory and filetype 2. Direct DataFrame: provide data parameter (legacy support)</p> <p>Upon initialization, the class loads medication schema data including category-to-group mappings from the YAML schema.</p> Source code in <code>clifpy/tables/medication_admin_intermittent.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the MedicationAdminIntermittent table.\n\n    This class handles intermittent medication administration data, including validation,\n    dose unit standardization, and unit conversion capabilities.\n\n    Parameters\n    ----------\n    data_directory : str, optional\n        Path to the directory containing data files. If None and data is provided,\n        defaults to current directory.\n    filetype : str, optional\n        Type of data file (csv, parquet, etc.). If None and data is provided,\n        defaults to 'parquet'.\n    timezone : str, default=\"UTC\"\n        Timezone for datetime columns. Used for proper timestamp handling.\n    output_directory : str, optional\n        Directory for saving output files and logs. If not specified, outputs\n        are saved to the current working directory.\n    data : pd.DataFrame, optional\n        Pre-loaded DataFrame to use instead of loading from file. Supports\n        backward compatibility with direct DataFrame initialization.\n\n    Notes\n    -----\n    The class supports two initialization patterns:\n    1. Loading from file: provide data_directory and filetype\n    2. Direct DataFrame: provide data parameter (legacy support)\n\n    Upon initialization, the class loads medication schema data including\n    category-to-group mappings from the YAML schema.\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: medication_admin_intermittent(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Load medication mappings\n    self._med_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load medication-specific schema data\n    self._load_medication_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.medication_admin_intermittent.MedicationAdminIntermittent.med_category_to_group_mapping","title":"med_category_to_group_mapping  <code>property</code>","text":"<pre><code>med_category_to_group_mapping\n</code></pre> <p>Get the medication category to group mapping from the schema.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>A dictionary mapping medication categories to their therapeutic groups. Returns a copy to prevent external modification of the internal mapping. Returns an empty dict if no mappings are loaded.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mai = MedicationAdminIntermittent(data)\n&gt;&gt;&gt; mappings = mai.med_category_to_group_mapping\n&gt;&gt;&gt; mappings['Antibiotics']\n'Antimicrobials'\n</code></pre>"},{"location":"api/tables/#hospital-diagnosis","title":"Hospital Diagnosis","text":""},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis","title":"clifpy.tables.hospital_diagnosis.HospitalDiagnosis","text":"<pre><code>HospitalDiagnosis(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Hospital diagnosis table wrapper inheriting from BaseTable.</p> <p>This class handles hospital diagnosis-specific data and validations while leveraging the common functionality provided by BaseTable. Hospital diagnosis codes are finalized billing diagnosis codes for hospital reimbursement, appropriate for calculation of comorbidity scores but should not be used as input features into a prediction model for an inpatient event.</p> <p>Initialize the hospital diagnosis table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the hospital diagnosis table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Auto-load data if not provided\n    if data is None and data_directory is not None and filetype is not None:\n        self.load_table()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis.get_diagnosis_by_format","title":"get_diagnosis_by_format","text":"<pre><code>get_diagnosis_by_format()\n</code></pre> <p>Group diagnoses by format (ICD9/ICD10) and return summary statistics.</p> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def get_diagnosis_by_format(self) -&gt; Dict:\n    \"\"\"Group diagnoses by format (ICD9/ICD10) and return summary statistics.\"\"\"\n    if self.df is None or 'diagnosis_code_format' not in self.df.columns:\n        return {}\n\n    format_stats = {}\n\n    for format_type in self.df['diagnosis_code_format'].unique():\n        subset = self.df[self.df['diagnosis_code_format'] == format_type]\n\n        format_stats[format_type] = {\n            'total_diagnoses': len(subset),\n            'unique_diagnosis_codes': subset['diagnosis_code'].nunique() if 'diagnosis_code' in subset.columns else 0,\n            'unique_hospitalizations': subset['hospitalization_id'].nunique() if 'hospitalization_id' in subset.columns else 0\n        }\n\n        # Primary vs secondary for this format\n        if 'diagnosis_primary' in subset.columns:\n            primary_counts = subset['diagnosis_primary'].value_counts().to_dict()\n            format_stats[format_type]['primary_count'] = primary_counts.get(1, 0)\n            format_stats[format_type]['secondary_count'] = primary_counts.get(0, 0)\n\n        # POA statistics for this format\n        if 'poa_present' in subset.columns:\n            poa_counts = subset['poa_present'].value_counts().to_dict()\n            format_stats[format_type]['poa_present_count'] = poa_counts.get(1, 0)\n            format_stats[format_type]['poa_not_present_count'] = poa_counts.get(0, 0)\n\n    return format_stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis.get_diagnosis_summary","title":"get_diagnosis_summary","text":"<pre><code>get_diagnosis_summary()\n</code></pre> <p>Return comprehensive summary statistics for hospital diagnosis data.</p> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def get_diagnosis_summary(self) -&gt; Dict:\n    \"\"\"Return comprehensive summary statistics for hospital diagnosis data.\"\"\"\n    if self.df is None:\n        return {}\n\n    stats = {\n        'total_diagnoses': len(self.df),\n        'unique_hospitalizations': self.df['hospitalization_id'].nunique() if 'hospitalization_id' in self.df.columns else 0,\n        'unique_diagnosis_codes': self.df['diagnosis_code'].nunique() if 'diagnosis_code' in self.df.columns else 0\n    }\n\n    # Diagnosis code format distribution\n    if 'diagnosis_code_format' in self.df.columns:\n        stats['diagnosis_format_counts'] = self.df['diagnosis_code_format'].value_counts().to_dict()\n\n    # Primary vs secondary diagnosis distribution\n    if 'diagnosis_primary' in self.df.columns:\n        primary_counts = self.df['diagnosis_primary'].value_counts().to_dict()\n        stats['primary_diagnosis_counts'] = {\n            'primary': primary_counts.get(1, 0),\n            'secondary': primary_counts.get(0, 0)\n        }\n\n    # Present on admission distribution\n    if 'poa_present' in self.df.columns:\n        poa_counts = self.df['poa_present'].value_counts().to_dict()\n        stats['poa_counts'] = {\n            'present_on_admission': poa_counts.get(1, 0),\n            'not_present_on_admission': poa_counts.get(0, 0)\n        }\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis.get_hospitalization_diagnosis_counts","title":"get_hospitalization_diagnosis_counts","text":"<pre><code>get_hospitalization_diagnosis_counts()\n</code></pre> <p>Return DataFrame with diagnosis counts per hospitalization.</p> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def get_hospitalization_diagnosis_counts(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with diagnosis counts per hospitalization.\"\"\"\n    if self.df is None or 'hospitalization_id' not in self.df.columns:\n        return pd.DataFrame()\n\n    hosp_counts = (self.df.groupby('hospitalization_id')\n                  .agg({\n                      'diagnosis_code': 'count',\n                      'diagnosis_primary': lambda x: (x == 1).sum(),\n                      'poa_present': lambda x: (x == 1).sum() if 'poa_present' in self.df.columns else 0\n                  })\n                  .reset_index())\n\n    hosp_counts.columns = ['hospitalization_id', 'total_diagnoses', 'primary_diagnoses', 'poa_present_diagnoses']\n    hosp_counts['secondary_diagnoses'] = hosp_counts['total_diagnoses'] - hosp_counts['primary_diagnoses']\n\n    return hosp_counts.sort_values('total_diagnoses', ascending=False)\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis.get_poa_statistics","title":"get_poa_statistics","text":"<pre><code>get_poa_statistics()\n</code></pre> <p>Calculate present on admission statistics by diagnosis type.</p> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def get_poa_statistics(self) -&gt; Dict:\n    \"\"\"Calculate present on admission statistics by diagnosis type.\"\"\"\n    if self.df is None or 'poa_present' not in self.df.columns or 'diagnosis_primary' not in self.df.columns:\n        return {}\n\n    stats = {}\n\n    # Overall POA statistics\n    total_diagnoses = len(self.df)\n    poa_present = len(self.df[self.df['poa_present'] == 1])\n    poa_not_present = len(self.df[self.df['poa_present'] == 0])\n\n    stats['overall'] = {\n        'total_diagnoses': total_diagnoses,\n        'poa_present_count': poa_present,\n        'poa_not_present_count': poa_not_present,\n        'poa_present_rate': (poa_present / total_diagnoses * 100) if total_diagnoses &gt; 0 else 0\n    }\n\n    # POA statistics by primary/secondary diagnosis\n    for diagnosis_type, diagnosis_value in [('primary', 1), ('secondary', 0)]:\n        subset = self.df[self.df['diagnosis_primary'] == diagnosis_value]\n        if not subset.empty:\n            subset_total = len(subset)\n            subset_poa_present = len(subset[subset['poa_present'] == 1])\n            subset_poa_not_present = len(subset[subset['poa_present'] == 0])\n\n            stats[diagnosis_type] = {\n                'total_diagnoses': subset_total,\n                'poa_present_count': subset_poa_present,\n                'poa_not_present_count': subset_poa_not_present,\n                'poa_present_rate': (subset_poa_present / subset_total * 100) if subset_total &gt; 0 else 0\n            }\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis.get_primary_diagnosis_counts","title":"get_primary_diagnosis_counts","text":"<pre><code>get_primary_diagnosis_counts()\n</code></pre> <p>Return DataFrame with counts of primary diagnoses by diagnosis code.</p> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def get_primary_diagnosis_counts(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with counts of primary diagnoses by diagnosis code.\"\"\"\n    if self.df is None or 'diagnosis_primary' not in self.df.columns:\n        return pd.DataFrame()\n\n    primary_diagnoses = self.df[self.df['diagnosis_primary'] == 1]\n\n    if primary_diagnoses.empty:\n        return pd.DataFrame()\n\n    diagnosis_counts = (primary_diagnoses.groupby(['diagnosis_code', 'diagnosis_code_format'])\n                       .size()\n                       .reset_index(name='count'))\n\n    return diagnosis_counts.sort_values('count', ascending=False)\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospital_diagnosis.HospitalDiagnosis.load_table","title":"load_table","text":"<pre><code>load_table()\n</code></pre> <p>Load hospital diagnosis table data from the configured data directory.</p> Source code in <code>clifpy/tables/hospital_diagnosis.py</code> <pre><code>def load_table(self):\n    \"\"\"Load hospital diagnosis table data from the configured data directory.\"\"\"\n    from ..utils.io import load_data\n\n    if self.data_directory is None or self.filetype is None:\n        raise ValueError(\"data_directory and filetype must be set to load data\")\n\n    self.df = load_data(\n        self.table_name,\n        self.data_directory,\n        self.filetype,\n        site_tz=self.timezone\n    )\n\n    if self.logger:\n        self.logger.info(f\"Loaded {len(self.df)} rows from {self.table_name} table\")\n</code></pre>"},{"location":"api/tables/#crrt-therapy","title":"CRRT Therapy","text":""},{"location":"api/tables/#clifpy.tables.crrt_therapy.CrrtTherapy","title":"clifpy.tables.crrt_therapy.CrrtTherapy","text":"<pre><code>CrrtTherapy(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>CRRT (Continuous Renal Replacement Therapy) table wrapper inheriting from BaseTable.</p> <p>This class handles CRRT therapy data including dialysis modes, flow rates, and ultrafiltration parameters while leveraging the common functionality  provided by BaseTable.</p> <p>Initialize the CRRT therapy table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/crrt_therapy.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the CRRT therapy table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#patient-procedures","title":"Patient Procedures","text":""},{"location":"api/tables/#clifpy.tables.patient_procedures.PatientProcedures","title":"clifpy.tables.patient_procedures.PatientProcedures","text":"<pre><code>PatientProcedures(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient procedures table wrapper inheriting from BaseTable.</p> <p>This class handles patient procedure data including CPT, ICD10PCS, and HCPCS codes while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient procedures table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient_procedures.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient procedures table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#microbiology-susceptibility","title":"Microbiology Susceptibility","text":""},{"location":"api/tables/#clifpy.tables.microbiology_susceptibility.MicrobiologySusceptibility","title":"clifpy.tables.microbiology_susceptibility.MicrobiologySusceptibility","text":"<pre><code>MicrobiologySusceptibility(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Microbiology susceptibility table wrapper inheriting from BaseTable.</p> <p>This class handles antimicrobial susceptibility testing data including antimicrobial categories and susceptibility results while leveraging  the common functionality provided by BaseTable.</p> <p>Initialize the microbiology susceptibility table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/microbiology_susceptibility.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the microbiology susceptibility table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#ecmo-mcs","title":"ECMO MCS","text":""},{"location":"api/tables/#clifpy.tables.ecmo_mcs.EcmoMcs","title":"clifpy.tables.ecmo_mcs.EcmoMcs","text":"<pre><code>EcmoMcs(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>ECMO (Extracorporeal Membrane Oxygenation) and MCS (Mechanical Circulatory Support)  table wrapper inheriting from BaseTable.</p> <p>This class handles ECMO/MCS device data including device types, flow rates, and support parameters while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the ECMO/MCS table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/ecmo_mcs.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the ECMO/MCS table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#microbiology-non-culture","title":"Microbiology Non-Culture","text":""},{"location":"api/tables/#clifpy.tables.microbiology_nonculture.MicrobiologyNonculture","title":"clifpy.tables.microbiology_nonculture.MicrobiologyNonculture","text":"<pre><code>MicrobiologyNonculture(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Microbiology non-culture table wrapper inheriting from BaseTable.</p> <p>This class handles microbiology non-culture test data including PCR and other molecular diagnostic results while leveraging the common  functionality provided by BaseTable.</p> <p>Initialize the microbiology non-culture table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/microbiology_nonculture.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the microbiology non-culture table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#code-status","title":"Code Status","text":""},{"location":"api/tables/#clifpy.tables.code_status.CodeStatus","title":"clifpy.tables.code_status.CodeStatus","text":"<pre><code>CodeStatus(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Code status table wrapper inheriting from BaseTable.</p> <p>This class handles patient code status data including DNR, DNAR, DNR/DNI, Full Code, and other resuscitation preferences while leveraging the common  functionality provided by BaseTable.</p> <p>Initialize the code status table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/code_status.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the code status table.\n\n    Parameters\n    ----------\n    data_directory : str\n        Path to the directory containing data files\n    filetype : str\n        Type of data file (csv, parquet, etc.)\n    timezone : str\n        Timezone for datetime columns\n    output_directory : str, optional\n        Directory for saving output files and logs\n    data : pd.DataFrame, optional\n        Pre-loaded data to use instead of loading from file\n    \"\"\"\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/utilities/","title":"Utilities API Reference","text":"<p>CLIFpy provides several utility modules to support data processing and analysis tasks.</p>"},{"location":"api/utilities/#med-unit-converter","title":"Med Unit Converter","text":"<p>The unit converter module provides comprehensive medication dose unit conversion functionality.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.convert_dose_units_by_med_category","title":"clifpy.utils.unit_converter.convert_dose_units_by_med_category","text":"<pre><code>convert_dose_units_by_med_category(\n    med_df,\n    vitals_df=None,\n    preferred_units=None,\n    show_intermediate=False,\n    override=False,\n)\n</code></pre> <p>Convert medication dose units to user-defined preferred units for each med_category.</p> <p>This function performs a two-step conversion process:</p> <ol> <li>Standardizes all dose units to a base set of standard units (mcg/min, ml/min, u/min for rates)</li> <li>Converts from base units to medication-specific preferred units if provided</li> </ol> <p>The conversion maintains unit class consistency (rates stay rates, amounts stay amounts) and handles weight-based dosing appropriately using patient weights.</p> <p>Parameters:</p> Name Type Description Default <code>med_df</code> <code>DataFrame</code> <p>Medication DataFrame with required columns:</p> <ul> <li>med_dose: Original dose values (numeric)</li> <li>med_dose_unit: Original dose unit strings (e.g., 'MCG/KG/HR', 'mL/hr')</li> <li>med_category: Medication category identifier (e.g., 'propofol', 'fentanyl')</li> <li>weight_kg: Patient weight in kg (optional, will be extracted from vitals_df if missing)</li> </ul> required <code>vitals_df</code> <code>DataFrame</code> <p>Vitals DataFrame for extracting patient weights if not in med_df. Required columns if weight_kg missing from med_df:</p> <ul> <li>hospitalization_id: Patient identifier</li> <li>recorded_dttm: Timestamp of vital recording</li> <li>vital_category: Must include 'weight_kg' values</li> <li>vital_value: Weight values</li> </ul> <code>None</code> <code>preferred_units</code> <code>dict</code> <p>Dictionary mapping medication categories to their preferred units. Keys are medication category names, values are target unit strings. Example: {'propofol': 'mcg/kg/min', 'fentanyl': 'mcg/hr', 'insulin': 'u/hr'} If None, uses base units (mcg/min, ml/min, u/min) as defaults.</p> <code>None</code> <code>show_intermediate</code> <code>bool</code> <p>If False, excludes intermediate calculation columns (multipliers) from output. If True, retains all columns including conversion multipliers for debugging.</p> <code>False</code> <code>override</code> <code>bool</code> <p>If True, prints warning messages for unacceptable preferred units but continues processing. If False, raises ValueError when encountering unacceptable preferred units.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>A tuple containing:</p> <ul> <li> <p>[0] Converted medication DataFrame with additional columns:</p> <ul> <li>_clean_unit: Cleaned unit format</li> <li>_base_unit: Base unit after first conversion</li> <li>_base_dose: Dose value in base units</li> <li>_preferred_unit: Target unit for medication category</li> <li>med_dose_converted: Final dose value in preferred units</li> <li>med_dose_unit_converted: Final unit string after conversion</li> <li>_unit_class: Classification ('rate', 'amount', or 'unrecognized')</li> <li>_convert_status: Status message indicating success or reason for failure</li> </ul> <p>If show_intermediate=True, also includes conversion multipliers.</p> </li> <li> <p>[1] Summary counts DataFrame with conversion statistics grouped by medication category</p> </li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns (med_dose_unit, med_dose) are missing from med_df, if standardization to base units fails, or if conversion to preferred units fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; med_df = pd.DataFrame({\n...     'med_category': ['propofol', 'fentanyl', 'insulin'],\n...     'med_dose': [200, 2, 5],\n...     'med_dose_unit': ['MCG/KG/MIN', 'mcg/kg/hr', 'units/hr'],\n...     'weight_kg': [70, 80, 75]\n... })\n&gt;&gt;&gt; preferred = {\n...     'propofol': 'mcg/kg/min',\n...     'fentanyl': 'mcg/hr',\n...     'insulin': 'u/hr'\n... }\n&gt;&gt;&gt; result_df, counts_df = convert_dose_units_by_med_category(med_df, preferred_units=preferred)\n</code></pre> Notes <p>The function handles various unit formats including:</p> <ul> <li>Weight-based dosing: /kg, /lb (uses patient weight for conversion)</li> <li>Time conversions: /hr to /min</li> <li>Volume conversions: L to mL</li> <li>Mass conversions: mg, ng, g to mcg</li> <li>Unit conversions: milli-units (mu) to units (u)</li> </ul> <p>Unrecognized units are preserved but flagged in the _unit_class column.</p> Todo <p>Implement config file parsing for default preferred_units.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def convert_dose_units_by_med_category(\n    med_df: pd.DataFrame,\n    vitals_df: pd.DataFrame = None,\n    preferred_units: dict = None,\n    show_intermediate: bool = False,\n    override: bool = False\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Convert medication dose units to user-defined preferred units for each med_category.\n\n    This function performs a two-step conversion process:\n\n    1. Standardizes all dose units to a base set of standard units (mcg/min, ml/min, u/min for rates)\n    2. Converts from base units to medication-specific preferred units if provided\n\n    The conversion maintains unit class consistency (rates stay rates, amounts stay amounts)\n    and handles weight-based dosing appropriately using patient weights.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        Medication DataFrame with required columns:\n\n        - med_dose: Original dose values (numeric)\n        - med_dose_unit: Original dose unit strings (e.g., 'MCG/KG/HR', 'mL/hr')\n        - med_category: Medication category identifier (e.g., 'propofol', 'fentanyl')\n        - weight_kg: Patient weight in kg (optional, will be extracted from vitals_df if missing)\n    vitals_df : pd.DataFrame, optional\n        Vitals DataFrame for extracting patient weights if not in med_df.\n        Required columns if weight_kg missing from med_df:\n\n        - hospitalization_id: Patient identifier\n        - recorded_dttm: Timestamp of vital recording\n        - vital_category: Must include 'weight_kg' values\n        - vital_value: Weight values\n    preferred_units : dict, optional\n        Dictionary mapping medication categories to their preferred units.\n        Keys are medication category names, values are target unit strings.\n        Example: {'propofol': 'mcg/kg/min', 'fentanyl': 'mcg/hr', 'insulin': 'u/hr'}\n        If None, uses base units (mcg/min, ml/min, u/min) as defaults.\n    show_intermediate : bool, default False\n        If False, excludes intermediate calculation columns (multipliers) from output.\n        If True, retains all columns including conversion multipliers for debugging.\n    override : bool, default False\n        If True, prints warning messages for unacceptable preferred units but continues processing.\n        If False, raises ValueError when encountering unacceptable preferred units.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        A tuple containing:\n\n        - [0] Converted medication DataFrame with additional columns:\n\n            * _clean_unit: Cleaned unit format\n            * _base_unit: Base unit after first conversion\n            * _base_dose: Dose value in base units\n            * _preferred_unit: Target unit for medication category\n            * med_dose_converted: Final dose value in preferred units\n            * med_dose_unit_converted: Final unit string after conversion\n            * _unit_class: Classification ('rate', 'amount', or 'unrecognized')\n            * _convert_status: Status message indicating success or reason for failure\n\n            If show_intermediate=True, also includes conversion multipliers.\n\n        - [1] Summary counts DataFrame with conversion statistics grouped by medication category\n\n    Raises\n    ------\n    ValueError\n        If required columns (med_dose_unit, med_dose) are missing from med_df,\n        if standardization to base units fails, or if conversion to preferred units fails.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; med_df = pd.DataFrame({\n    ...     'med_category': ['propofol', 'fentanyl', 'insulin'],\n    ...     'med_dose': [200, 2, 5],\n    ...     'med_dose_unit': ['MCG/KG/MIN', 'mcg/kg/hr', 'units/hr'],\n    ...     'weight_kg': [70, 80, 75]\n    ... })\n    &gt;&gt;&gt; preferred = {\n    ...     'propofol': 'mcg/kg/min',\n    ...     'fentanyl': 'mcg/hr',\n    ...     'insulin': 'u/hr'\n    ... }\n    &gt;&gt;&gt; result_df, counts_df = convert_dose_units_by_med_category(med_df, preferred_units=preferred)\n\n    Notes\n    -----\n    The function handles various unit formats including:\n\n    - Weight-based dosing: /kg, /lb (uses patient weight for conversion)\n    - Time conversions: /hr to /min\n    - Volume conversions: L to mL\n    - Mass conversions: mg, ng, g to mcg\n    - Unit conversions: milli-units (mu) to units (u)\n\n    Unrecognized units are preserved but flagged in the _unit_class column.\n\n    Todo\n    ----\n    Implement config file parsing for default preferred_units.\n    \"\"\"\n    # check if the requested med_categories are in the input med_df\n    requested_med_categories = set(preferred_units.keys())\n    extra_med_categories = requested_med_categories - set(med_df['med_category'])\n    if extra_med_categories:\n        error_msg = f\"The following med_categories are given a preferred unit but not found in the input med_df: {extra_med_categories}\"\n        if override:\n            logger.warning(error_msg)\n        else:\n            raise ValueError(error_msg)\n\n    try:\n        med_df_base, _ = standardize_dose_to_base_units(med_df, vitals_df)\n    except ValueError as e:\n        raise ValueError(f\"Error standardizing dose units to base units: {e}\")\n\n    try:\n        # join the preferred units to the df\n        preferred_units_df = pd.DataFrame(preferred_units.items(), columns=['med_category', '_preferred_unit'])\n        q = \"\"\"\n        SELECT l.*\n            -- for unspecified preferred units, use the base units by default\n            , _preferred_unit: COALESCE(r._preferred_unit, l._base_unit)\n        FROM med_df_base l\n        LEFT JOIN preferred_units_df r USING (med_category)\n        \"\"\"\n        med_df_preferred = duckdb.sql(q)\n\n        med_df_converted = _convert_base_units_to_preferred_units(med_df_preferred, override=override).to_df()\n    except ValueError as e:\n        raise ValueError(f\"Error converting dose units to preferred units: {e}\")\n\n    try:\n        convert_counts_df = _create_unit_conversion_counts_table(\n            med_df_converted, \n            group_by=[\n                'med_category',\n                'med_dose_unit', '_clean_unit', '_base_unit', '_unit_class',\n                '_preferred_unit', 'med_dose_unit_converted', '_convert_status'\n                ]\n            )\n    except ValueError as e:\n        raise ValueError(f\"Error creating unit conversion counts table: {e}\")\n\n    if show_intermediate:\n        return med_df_converted, convert_counts_df\n    else:\n        # the default (detailed_output=False) is to drop multiplier columns which likely are not useful for the user\n        multiplier_cols = [col for col in med_df_converted.columns if 'multiplier' in col]\n        qa_cols = [\n            '_weight_recorded_dttm',\n            '_weighted', '_weighted_preferred',\n            '_base_dose', '_base_unit',\n            '_preferred_unit',\n            '_unit_class_preferred',\n            '_unit_subclass', '_unit_subclass_preferred'\n            ]\n\n        cols_to_drop = [c for c in multiplier_cols + qa_cols if c in med_df_converted.columns]\n\n        return med_df_converted.drop(columns=cols_to_drop), convert_counts_df.to_df()\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.standardize_dose_to_base_units","title":"clifpy.utils.unit_converter.standardize_dose_to_base_units","text":"<pre><code>standardize_dose_to_base_units(med_df, vitals_df=None)\n</code></pre> <p>Standardize medication dose units to a base set of standard units.</p> <p>Main public API function that performs complete dose unit standardization pipeline: format cleaning, name cleaning, and unit conversion. Returns both base data and a summary table of conversions.</p> <p>Parameters:</p> Name Type Description Default <code>med_df</code> <code>DataFrame</code> <p>Medication DataFrame with required columns:</p> <ul> <li>med_dose_unit: Original dose unit strings</li> <li>med_dose: Dose values</li> <li>weight_kg: Patient weights (optional, can be added from vitals_df)</li> </ul> <p>Additional columns are preserved in output.</p> required <code>vitals_df</code> <code>DataFrame</code> <p>Vitals DataFrame for extracting patient weights if not in med_df. Required columns if weight_kg missing from med_df:</p> <ul> <li>hospitalization_id: Patient identifier</li> <li>recorded_dttm: Timestamp of vital recording</li> <li>vital_category: Must include 'weight_kg' values</li> <li>vital_value: Weight values</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>A tuple containing:</p> <ul> <li> <p>[0] base medication DataFrame with additional columns:</p> <ul> <li>_clean_unit: Cleaned unit string</li> <li>_unit_class: 'rate', 'amount', or 'unrecognized'</li> <li>_base_dose: base dose value</li> <li>_base_unit: base unit</li> <li>amount_multiplier, time_multiplier, weight_multiplier: Conversion factors</li> </ul> </li> <li> <p>[1] Summary counts DataFrame showing conversion patterns and frequencies</p> </li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from med_df.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; med_df = pd.DataFrame({\n...     'med_dose': [6, 100, 500],\n...     'med_dose_unit': ['MCG/KG/HR', 'mL / hr', 'mg'],\n...     'weight_kg': [70, 80, 75]\n... })\n&gt;&gt;&gt; base_df, counts_df = standardize_dose_to_base_units(med_df)\n&gt;&gt;&gt; '_base_unit' in base_df.columns\nTrue\n&gt;&gt;&gt; 'count' in counts_df.columns\nTrue\n</code></pre> Notes <p>Standard units for conversion:</p> <ul> <li>Rate units: mcg/min, ml/min, u/min (all per minute)</li> <li>Amount units: mcg, ml, u (base units)</li> </ul> <p>The function automatically handles:</p> <ul> <li>Weight-based dosing (/kg, /lb) using patient weights</li> <li>Time conversions (per hour to per minute)</li> <li>Volume conversions (L to mL)</li> <li>Mass conversions (mg, ng, g to mcg)</li> <li>Unit conversions (milli-units to units)</li> </ul> <p>Unrecognized units are flagged but preserved in the output.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def standardize_dose_to_base_units(\n    med_df: pd.DataFrame,\n    vitals_df: pd.DataFrame = None\n    ) -&gt; Tuple[duckdb.DuckDBPyRelation, duckdb.DuckDBPyRelation]:\n    \"\"\"Standardize medication dose units to a base set of standard units.\n\n    Main public API function that performs complete dose unit standardization\n    pipeline: format cleaning, name cleaning, and unit conversion.\n    Returns both base data and a summary table of conversions.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        Medication DataFrame with required columns:\n\n        - med_dose_unit: Original dose unit strings\n        - med_dose: Dose values\n        - weight_kg: Patient weights (optional, can be added from vitals_df)\n\n        Additional columns are preserved in output.\n    vitals_df : pd.DataFrame, optional\n        Vitals DataFrame for extracting patient weights if not in med_df.\n        Required columns if weight_kg missing from med_df:\n\n        - hospitalization_id: Patient identifier\n        - recorded_dttm: Timestamp of vital recording\n        - vital_category: Must include 'weight_kg' values\n        - vital_value: Weight values\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        A tuple containing:\n\n        - [0] base medication DataFrame with additional columns:\n\n            * _clean_unit: Cleaned unit string\n            * _unit_class: 'rate', 'amount', or 'unrecognized'\n            * _base_dose: base dose value\n            * _base_unit: base unit\n            * amount_multiplier, time_multiplier, weight_multiplier: Conversion factors\n\n        - [1] Summary counts DataFrame showing conversion patterns and frequencies\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from med_df.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; med_df = pd.DataFrame({\n    ...     'med_dose': [6, 100, 500],\n    ...     'med_dose_unit': ['MCG/KG/HR', 'mL / hr', 'mg'],\n    ...     'weight_kg': [70, 80, 75]\n    ... })\n    &gt;&gt;&gt; base_df, counts_df = standardize_dose_to_base_units(med_df)\n    &gt;&gt;&gt; '_base_unit' in base_df.columns\n    True\n    &gt;&gt;&gt; 'count' in counts_df.columns\n    True\n\n    Notes\n    -----\n    Standard units for conversion:\n\n    - Rate units: mcg/min, ml/min, u/min (all per minute)\n    - Amount units: mcg, ml, u (base units)\n\n    The function automatically handles:\n\n    - Weight-based dosing (/kg, /lb) using patient weights\n    - Time conversions (per hour to per minute)\n    - Volume conversions (L to mL)\n    - Mass conversions (mg, ng, g to mcg)\n    - Unit conversions (milli-units to units)\n\n    Unrecognized units are flagged but preserved in the output.\n    \"\"\"\n    if 'weight_kg' not in med_df.columns:\n        logger.info(\"pulling the most recent weight from the vitals table since no `weight_kg` column exists in the medication table\")\n        med_df = find_most_recent_weight(med_df, vitals_df)#.to_df()\n\n    # check if the required columns are present\n    required_columns = {'med_dose_unit', 'med_dose', 'weight_kg'}\n    missing_columns = required_columns - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    # Clean dose units using DuckDB to avoid pandas materialization\n    med_df_cleaned = _clean_dose_unit_formats_duckdb(med_df)\n    med_df_cleaned = _clean_dose_unit_names_duckdb(med_df_cleaned)\n    med_df_base = _convert_clean_units_to_base_units(med_df_cleaned)\n    convert_counts_df = _create_unit_conversion_counts_table(\n        med_df_base,\n        group_by=['med_dose_unit', '_clean_unit', '_base_unit', '_unit_class']\n        )\n\n    return med_df_base, convert_counts_df\n</code></pre>"},{"location":"api/utilities/#constants-and-data-structures","title":"Constants and Data Structures","text":""},{"location":"api/utilities/#acceptable-units","title":"Acceptable Units","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter.ACCEPTABLE_AMOUNT_UNITS","title":"clifpy.utils.unit_converter.ACCEPTABLE_AMOUNT_UNITS  <code>module-attribute</code>","text":"<pre><code>ACCEPTABLE_AMOUNT_UNITS = {\n    \"ml\",\n    \"l\",\n    \"mu\",\n    \"u\",\n    \"mcg\",\n    \"mg\",\n    \"ng\",\n    \"g\",\n}\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.ACCEPTABLE_RATE_UNITS","title":"clifpy.utils.unit_converter.ACCEPTABLE_RATE_UNITS  <code>module-attribute</code>","text":"<pre><code>ACCEPTABLE_RATE_UNITS = _acceptable_rate_units()\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.ALL_ACCEPTABLE_UNITS","title":"clifpy.utils.unit_converter.ALL_ACCEPTABLE_UNITS  <code>module-attribute</code>","text":"<pre><code>ALL_ACCEPTABLE_UNITS = (\n    ACCEPTABLE_RATE_UNITS | ACCEPTABLE_AMOUNT_UNITS\n)\n</code></pre>"},{"location":"api/utilities/#unit-patterns","title":"Unit Patterns","text":"<p>The following constants define regex patterns for unit classification:</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter.MASS_REGEX","title":"clifpy.utils.unit_converter.MASS_REGEX  <code>module-attribute</code>","text":"<pre><code>MASS_REGEX = f'^(mcg|mg|ng|g){AMOUNT_ENDER}'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.VOLUME_REGEX","title":"clifpy.utils.unit_converter.VOLUME_REGEX  <code>module-attribute</code>","text":"<pre><code>VOLUME_REGEX = f'^(l|ml){AMOUNT_ENDER}'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.UNIT_REGEX","title":"clifpy.utils.unit_converter.UNIT_REGEX  <code>module-attribute</code>","text":"<pre><code>UNIT_REGEX = f'^(u|mu){AMOUNT_ENDER}'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.HR_REGEX","title":"clifpy.utils.unit_converter.HR_REGEX  <code>module-attribute</code>","text":"<pre><code>HR_REGEX = f'/hr$'\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.WEIGHT_REGEX","title":"clifpy.utils.unit_converter.WEIGHT_REGEX  <code>module-attribute</code>","text":"<pre><code>WEIGHT_REGEX = f'/(lb|kg)/'\n</code></pre>"},{"location":"api/utilities/#conversion-mappings","title":"Conversion Mappings","text":""},{"location":"api/utilities/#clifpy.utils.unit_converter.UNIT_NAMING_VARIANTS","title":"clifpy.utils.unit_converter.UNIT_NAMING_VARIANTS  <code>module-attribute</code>","text":"<pre><code>UNIT_NAMING_VARIANTS = {\n    \"/hr\": \"/h(r|our)?$\",\n    \"/min\": \"/m(in|inute)?$\",\n    \"u\": \"u(nits|nit)?\",\n    \"m\": \"milli-?\",\n    \"l\": \"l(iters|itres|itre|iter)?\",\n    \"mcg\": \"^(u|\u00b5|\u03bc)g\",\n    \"g\": \"^g(rams|ram)?\",\n}\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter.REGEX_TO_FACTOR_MAPPER","title":"clifpy.utils.unit_converter.REGEX_TO_FACTOR_MAPPER  <code>module-attribute</code>","text":"<pre><code>REGEX_TO_FACTOR_MAPPER = {\n    HR_REGEX: \"1/60\",\n    L_REGEX: \"1000\",\n    MU_REGEX: \"1/1000\",\n    MG_REGEX: \"1000\",\n    NG_REGEX: \"1/1000\",\n    G_REGEX: \"1000000\",\n    KG_REGEX: \"weight_kg\",\n    LB_REGEX: \"weight_kg * 2.20462\",\n}\n</code></pre>"},{"location":"api/utilities/#internal-functions","title":"Internal Functions","text":"<p>The following functions are used internally by the main conversion functions. They are documented here for completeness and advanced usage.</p> <p>This section documents the utility functions available in CLIFpy for data processing, validation, and specialized operations.</p>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_formats","title":"clifpy.utils.unit_converter._clean_dose_unit_formats","text":"<pre><code>_clean_dose_unit_formats(s)\n</code></pre> <p>Clean dose unit formatting by removing spaces and converting to lowercase.</p> <p>This is the first step in the cleaning pipeline. It standardizes the basic formatting of dose units before applying name cleaning.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Series containing dose unit strings to clean.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series with cleaned formatting (no spaces, lowercase).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; s = pd.Series(['mL / hr', 'MCG/KG/MIN', ' Mg/Hr '])\n&gt;&gt;&gt; result = _clean_dose_unit_formats(s)\n&gt;&gt;&gt; list(result)\n['ml/hr', 'mcg/kg/min', 'mg/hr']\n</code></pre> Notes <p>This function is typically used as the first step in the cleaning pipeline, followed by _clean_dose_unit_names().</p> <p>.. deprecated::     Use _clean_dose_unit_formats_duckdb for better performance.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _clean_dose_unit_formats(s: pd.Series) -&gt; pd.Series:\n    \"\"\"Clean dose unit formatting by removing spaces and converting to lowercase.\n\n    This is the first step in the cleaning pipeline. It standardizes\n    the basic formatting of dose units before applying name cleaning.\n\n    Parameters\n    ----------\n    s : pd.Series\n        Series containing dose unit strings to clean.\n\n    Returns\n    -------\n    pd.Series\n        Series with cleaned formatting (no spaces, lowercase).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; s = pd.Series(['mL / hr', 'MCG/KG/MIN', ' Mg/Hr '])\n    &gt;&gt;&gt; result = _clean_dose_unit_formats(s)\n    &gt;&gt;&gt; list(result)\n    ['ml/hr', 'mcg/kg/min', 'mg/hr']\n\n    Notes\n    -----\n    This function is typically used as the first step in the cleaning\n    pipeline, followed by _clean_dose_unit_names().\n\n    .. deprecated::\n        Use _clean_dose_unit_formats_duckdb for better performance.\n    \"\"\"\n    return s.str.replace(r'\\s+', '', regex=True).str.lower().replace('', None, regex=False)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._clean_dose_unit_names","title":"clifpy.utils.unit_converter._clean_dose_unit_names","text":"<pre><code>_clean_dose_unit_names(s)\n</code></pre> <p>Clean dose unit name variants to standard abbreviations.</p> <p>Applies regex patterns to convert various unit name variants to their standard abbreviated forms (e.g., 'milliliter' -&gt; 'ml', 'hour' -&gt; 'hr').</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Series containing dose unit strings with name variants. Should already be format-cleaned (lowercase, no spaces).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series with clean unit names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; s = pd.Series(['milliliter/hour', 'units/minute', '\u00b5g/kg/h'])\n&gt;&gt;&gt; result = _clean_dose_unit_names(s)\n&gt;&gt;&gt; list(result)\n['ml/hr', 'u/min', 'mcg/kg/hr']\n</code></pre> Notes <p>Handles conversions including:</p> <ul> <li>Time: hour/h -&gt; hr, minute/m -&gt; min</li> <li>Volume: liter/liters/litre/litres -&gt; l</li> <li>Units: units/unit -&gt; u, milli-units -&gt; mu</li> <li>Mass: \u00b5g/ug -&gt; mcg, gram -&gt; g</li> </ul> <p>This function should be applied after _clean_dose_unit_formats().</p> <p>.. deprecated::     Use _clean_dose_unit_names_duckdb for better performance.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _clean_dose_unit_names(s: pd.Series) -&gt; pd.Series:\n    \"\"\"Clean dose unit name variants to standard abbreviations.\n\n    Applies regex patterns to convert various unit name variants to their\n    standard abbreviated forms (e.g., 'milliliter' -&gt; 'ml', 'hour' -&gt; 'hr').\n\n    Parameters\n    ----------\n    s : pd.Series\n        Series containing dose unit strings with name variants.\n        Should already be format-cleaned (lowercase, no spaces).\n\n    Returns\n    -------\n    pd.Series\n        Series with clean unit names.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; s = pd.Series(['milliliter/hour', 'units/minute', '\u00b5g/kg/h'])\n    &gt;&gt;&gt; result = _clean_dose_unit_names(s)\n    &gt;&gt;&gt; list(result)\n    ['ml/hr', 'u/min', 'mcg/kg/hr']\n\n    Notes\n    -----\n    Handles conversions including:\n\n    - Time: hour/h -&gt; hr, minute/m -&gt; min\n    - Volume: liter/liters/litre/litres -&gt; l\n    - Units: units/unit -&gt; u, milli-units -&gt; mu\n    - Mass: \u00b5g/ug -&gt; mcg, gram -&gt; g\n\n    This function should be applied after _clean_dose_unit_formats().\n\n    .. deprecated::\n        Use _clean_dose_unit_names_duckdb for better performance.\n    \"\"\"\n    for repl, pattern in UNIT_NAMING_VARIANTS.items():\n        s = s.str.replace(pattern, repl, regex=True)\n    return s\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_clean_units_to_base_units","title":"clifpy.utils.unit_converter._convert_clean_units_to_base_units","text":"<pre><code>_convert_clean_units_to_base_units(med_df)\n</code></pre> <p>Convert clean dose units to base units.</p> <p>Core conversion function that transforms various dose units into a base set of standard units (mcg/min, ml/min, u/min for rates; mcg, ml, u for amounts). Uses DuckDB for efficient SQL-based transformations.</p> <p>Parameters:</p> Name Type Description Default <code>med_df</code> <code>DataFrame</code> <p>DataFrame containing medication data with required columns:</p> <ul> <li>_clean_unit: Cleaned unit strings</li> <li>med_dose: Original dose values</li> <li>weight_kg: Patient weight (used for /kg and /lb conversions)</li> </ul> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Original DataFrame with additional columns:</p> <ul> <li>_unit_class: 'rate', 'amount', or 'unrecognized'</li> <li>_amount_multiplier: Factor for amount conversion</li> <li>_time_multiplier: Factor for time conversion (hr to min)</li> <li>_weight_multiplier: Factor for weight-based conversion</li> <li>_base_dose: base dose value</li> <li>_base_unit: base unit string</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'med_dose': [6, 100],\n...     '_clean_unit': ['mcg/kg/hr', 'ml/hr'],\n...     'weight_kg': [70, 80]\n... })\n&gt;&gt;&gt; result = _convert_clean_dose_units_to_base_units(df)\n&gt;&gt;&gt; 'mcg/min' in result['_base_unit'].values\nTrue\n&gt;&gt;&gt; 'ml/min' in result['_base_unit'].values\nTrue\n</code></pre> Notes <p>Conversion targets:</p> <ul> <li>Rate units: mcg/min, ml/min, u/min</li> <li>Amount units: mcg, ml, u</li> <li>Unrecognized units: original dose and (cleaned) unit will be preserved</li> </ul> <p>Weight-based conversions use patient weight from weight_kg column. Time conversions: /hr -&gt; /min (divide by 60).</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _convert_clean_units_to_base_units(med_df: pd.DataFrame | duckdb.DuckDBPyRelation) -&gt; duckdb.DuckDBPyRelation:\n    \"\"\"Convert clean dose units to base units.\n\n    Core conversion function that transforms various dose units into a base\n    set of standard units (mcg/min, ml/min, u/min for rates; mcg, ml, u for amounts).\n    Uses DuckDB for efficient SQL-based transformations.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        DataFrame containing medication data with required columns:\n\n        - _clean_unit: Cleaned unit strings\n        - med_dose: Original dose values\n        - weight_kg: Patient weight (used for /kg and /lb conversions)\n\n    Returns\n    -------\n    pd.DataFrame\n        Original DataFrame with additional columns:\n\n        - _unit_class: 'rate', 'amount', or 'unrecognized'\n        - _amount_multiplier: Factor for amount conversion\n        - _time_multiplier: Factor for time conversion (hr to min)\n        - _weight_multiplier: Factor for weight-based conversion\n        - _base_dose: base dose value\n        - _base_unit: base unit string\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\n    ...     'med_dose': [6, 100],\n    ...     '_clean_unit': ['mcg/kg/hr', 'ml/hr'],\n    ...     'weight_kg': [70, 80]\n    ... })\n    &gt;&gt;&gt; result = _convert_clean_dose_units_to_base_units(df)\n    &gt;&gt;&gt; 'mcg/min' in result['_base_unit'].values\n    True\n    &gt;&gt;&gt; 'ml/min' in result['_base_unit'].values\n    True\n\n    Notes\n    -----\n    Conversion targets:\n\n    - Rate units: mcg/min, ml/min, u/min\n    - Amount units: mcg, ml, u\n    - Unrecognized units: original dose and (cleaned) unit will be preserved\n\n    Weight-based conversions use patient weight from weight_kg column.\n    Time conversions: /hr -&gt; /min (divide by 60).\n    \"\"\"\n\n    amount_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_base,\n        patterns=[L_REGEX, MU_REGEX, MG_REGEX, NG_REGEX, G_REGEX],\n        else_case='1'\n        )\n\n    time_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_base,\n        patterns=[HR_REGEX],\n        else_case='1'\n        )\n\n    weight_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_base,\n        patterns=[KG_REGEX, LB_REGEX],\n        else_case='1'\n        )\n\n    q = f\"\"\"\n    SELECT *\n        -- classify and check acceptability first\n        , _unit_class: CASE\n            WHEN _clean_unit IN ('{RATE_UNITS_STR}') THEN 'rate' \n            WHEN _clean_unit IN ('{AMOUNT_UNITS_STR}') THEN 'amount'\n            ELSE 'unrecognized' END\n        -- mark if the input unit is adjusted by weight (e.g. 'mcg/kg/hr')\n        , _weighted: CASE\n            WHEN regexp_matches(_clean_unit, '{WEIGHT_REGEX}') THEN 1 ELSE 0 END\n        -- parse and generate multipliers\n        , _amount_multiplier: CASE\n            WHEN _unit_class = 'unrecognized' THEN 1 ELSE ({amount_clause}) END \n        , _time_multiplier: CASE\n            WHEN _unit_class = 'unrecognized' THEN 1 ELSE ({time_clause}) END \n        , _weight_multiplier: CASE\n            WHEN _unit_class = 'unrecognized' THEN 1 ELSE ({weight_clause}) END\n        -- calculate the base dose\n        , _base_dose: CASE\n            -- when the input unit is weighted but weight_kg is missing, keep the original dose\n            WHEN _weighted = 1 AND weight_kg IS NULL THEN med_dose\n            ELSE med_dose * _amount_multiplier * _time_multiplier * _weight_multiplier \n            END\n        -- id the base unit\n        , _base_unit: CASE \n            -- when the input unit is weighted but weight_kg is missing, keep the original dose\n            WHEN _weighted = 1 AND weight_kg IS NULL THEN _clean_unit\n            WHEN _unit_class = 'unrecognized' THEN _clean_unit\n            WHEN _unit_class = 'rate' AND regexp_matches(_clean_unit, '{MASS_REGEX}') THEN 'mcg/min'\n            WHEN _unit_class = 'rate' AND regexp_matches(_clean_unit, '{VOLUME_REGEX}') THEN 'ml/min'\n            WHEN _unit_class = 'rate' AND regexp_matches(_clean_unit, '{UNIT_REGEX}') THEN 'u/min'\n            WHEN _unit_class = 'amount' AND regexp_matches(_clean_unit, '{MASS_REGEX}') THEN 'mcg'\n            WHEN _unit_class = 'amount' AND regexp_matches(_clean_unit, '{VOLUME_REGEX}') THEN 'ml'\n            WHEN _unit_class = 'amount' AND regexp_matches(_clean_unit, '{UNIT_REGEX}') THEN 'u'\n            END\n    FROM med_df \n    \"\"\"\n    return duckdb.sql(q)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_base_units_to_preferred_units","title":"clifpy.utils.unit_converter._convert_base_units_to_preferred_units","text":"<pre><code>_convert_base_units_to_preferred_units(\n    med_df, override=False\n)\n</code></pre> <p>Convert base standardized units to user-preferred units.</p> <p>Performs the second stage of unit conversion, transforming from standardized base units (mcg/min, ml/min, u/min) to medication-specific preferred units while maintaining unit class consistency.</p> <p>Parameters:</p> Name Type Description Default <code>med_df</code> <code>DataFrame</code> <p>DataFrame with required columns from first-stage conversion:</p> <ul> <li>_base_dose: Dose values in standardized units</li> <li>_base_unit: Standardized unit strings (may be NULL)</li> <li>_preferred_unit: Target unit strings for each medication</li> <li>weight_kg: Patient weights (optional, used for weight-based conversions)</li> </ul> required <code>override</code> <code>bool</code> <p>If True, prints warnings but continues when encountering:</p> <ul> <li>Unacceptable preferred units not in ALL_ACCEPTABLE_UNITS</li> <li>Cross-class conversions (e.g., rate to amount)</li> <li>Cross-subclass conversions (e.g., mass to volume)</li> </ul> <p>If False, raises ValueError for these conditions.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Original DataFrame with additional columns:</p> <ul> <li>_unit_class: Classification of base unit ('rate', 'amount', 'unrecognized')</li> <li>_unit_subclass: Subclassification ('mass', 'volume', 'unit', 'unrecognized')</li> <li>_unit_class_preferred: Classification of preferred unit</li> <li>_unit_subclass_preferred: Subclassification of preferred unit</li> <li>_convert_status: Success or failure reason message</li> <li>_amount_multiplier_preferred: Conversion factor for amount units</li> <li>_time_multiplier_preferred: Conversion factor for time units</li> <li>_weight_multiplier_preferred: Conversion factor for weight-based units</li> <li>med_dose_converted: Final converted dose value</li> <li>med_dose_unit_converted: Final unit string after conversion</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from med_df or if preferred units are not in ALL_ACCEPTABLE_UNITS (when override=False).</p> Notes <p>Conversion rules enforced:</p> <ul> <li>Conversions only allowed within same unit class (rate\u2192rate, amount\u2192amount)</li> <li>Cannot convert between incompatible subclasses (e.g., mass\u2192volume)</li> <li>When conversion fails, falls back to base units and dose values</li> <li>Missing units (NULL) are handled with 'original unit is missing' status</li> </ul> <p>The function uses DuckDB SQL for efficient processing and applies regex pattern matching to classify units and calculate conversion factors.</p> See Also <p>_convert_clean_dose_units_to_base_units : First-stage conversion convert_dose_units_by_med_category : Public API for complete conversion pipeline</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _convert_base_units_to_preferred_units(\n    med_df: pd.DataFrame | duckdb.DuckDBPyRelation,\n    override: bool = False\n    ) -&gt; duckdb.DuckDBPyRelation:\n    \"\"\"Convert base standardized units to user-preferred units.\n\n    Performs the second stage of unit conversion, transforming from standardized\n    base units (mcg/min, ml/min, u/min) to medication-specific preferred units\n    while maintaining unit class consistency.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        DataFrame with required columns from first-stage conversion:\n\n        - _base_dose: Dose values in standardized units\n        - _base_unit: Standardized unit strings (may be NULL)\n        - _preferred_unit: Target unit strings for each medication\n        - weight_kg: Patient weights (optional, used for weight-based conversions)\n    override : bool, default False\n        If True, prints warnings but continues when encountering:\n\n        - Unacceptable preferred units not in ALL_ACCEPTABLE_UNITS\n        - Cross-class conversions (e.g., rate to amount)\n        - Cross-subclass conversions (e.g., mass to volume)\n\n        If False, raises ValueError for these conditions.\n\n    Returns\n    -------\n    pd.DataFrame\n        Original DataFrame with additional columns:\n\n        - _unit_class: Classification of base unit ('rate', 'amount', 'unrecognized')\n        - _unit_subclass: Subclassification ('mass', 'volume', 'unit', 'unrecognized')\n        - _unit_class_preferred: Classification of preferred unit\n        - _unit_subclass_preferred: Subclassification of preferred unit\n        - _convert_status: Success or failure reason message\n        - _amount_multiplier_preferred: Conversion factor for amount units\n        - _time_multiplier_preferred: Conversion factor for time units\n        - _weight_multiplier_preferred: Conversion factor for weight-based units\n        - med_dose_converted: Final converted dose value\n        - med_dose_unit_converted: Final unit string after conversion\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from med_df or if preferred units are not\n        in ALL_ACCEPTABLE_UNITS (when override=False).\n\n    Notes\n    -----\n    Conversion rules enforced:\n\n    - Conversions only allowed within same unit class (rate\u2192rate, amount\u2192amount)\n    - Cannot convert between incompatible subclasses (e.g., mass\u2192volume)\n    - When conversion fails, falls back to base units and dose values\n    - Missing units (NULL) are handled with 'original unit is missing' status\n\n    The function uses DuckDB SQL for efficient processing and applies regex\n    pattern matching to classify units and calculate conversion factors.\n\n    See Also\n    --------\n    _convert_clean_dose_units_to_base_units : First-stage conversion\n    convert_dose_units_by_med_category : Public API for complete conversion pipeline\n    \"\"\"\n    # check presense of all required columns\n    required_columns = {'_base_dose', '_preferred_unit'}\n    missing_columns = required_columns - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    # check user-defined _preferred_unit are in the set of acceptable units\n    q = f\"\"\"\n    SELECT DISTINCT _preferred_unit\n    FROM med_df\n    \"\"\"\n    all_preferred_units = set(duckdb.sql(q).to_df()['_preferred_unit'])\n    unacceptable_preferred_units = all_preferred_units - ALL_ACCEPTABLE_UNITS - {None}\n    if unacceptable_preferred_units:\n        error_msg = f\"Cannot accommodate the conversion to the following preferred units: {unacceptable_preferred_units}. Consult the function documentation for a list of acceptable units.\"\n        if override:\n            logger.warning(error_msg)\n        else:\n            raise ValueError(error_msg)\n\n    amount_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_preferred,\n        patterns=[L_REGEX, MU_REGEX, MG_REGEX, NG_REGEX, G_REGEX],\n        else_case='1'\n        )\n\n    time_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_preferred,\n        patterns=[HR_REGEX],\n        else_case='1'\n        )\n\n    weight_clause = _concat_builders_by_patterns(\n        builder=_pattern_to_factor_builder_for_preferred,\n        patterns=[KG_REGEX, LB_REGEX],\n        else_case='1'\n        )\n\n    unit_class_clause = f\"\"\"\n    , _unit_class: CASE\n        WHEN _base_unit IN ('{RATE_UNITS_STR}') THEN 'rate' \n        WHEN _base_unit IN ('{AMOUNT_UNITS_STR}') THEN 'amount'\n        ELSE 'unrecognized' END\n    \"\"\" if '_unit_class' not in med_df.columns else ''\n\n    weighted_clause = f\"\"\"\n    , _weighted: CASE\n        WHEN regexp_matches(_clean_unit, '{WEIGHT_REGEX}') THEN 1 ELSE 0 END\n    \"\"\" if '_weighted' not in med_df.columns else ''\n\n    dose_converted_name = \"med_dose\" if \"med_dose\" in med_df.columns else \"_base_dose\"\n    unit_converted_name = \"_clean_unit\" if \"_clean_unit\" in med_df.columns else \"_base_unit\"\n\n    q = f\"\"\"\n    SELECT l.*\n        {unit_class_clause}\n        , _unit_subclass: CASE \n            WHEN regexp_matches(_base_unit, '{MASS_REGEX}') THEN 'mass'\n            WHEN regexp_matches(_base_unit, '{VOLUME_REGEX}') THEN 'volume'\n            WHEN regexp_matches(_base_unit, '{UNIT_REGEX}') THEN 'unit'\n            ELSE 'unrecognized' END\n        , _unit_class_preferred: CASE \n            WHEN _preferred_unit IN ('{RATE_UNITS_STR}') THEN 'rate' \n            WHEN _preferred_unit IN ('{AMOUNT_UNITS_STR}') THEN 'amount'\n            ELSE 'unrecognized' END\n        , _unit_subclass_preferred: CASE \n            WHEN regexp_matches(_preferred_unit, '{MASS_REGEX}') THEN 'mass'\n            WHEN regexp_matches(_preferred_unit, '{VOLUME_REGEX}') THEN 'volume'\n            WHEN regexp_matches(_preferred_unit, '{UNIT_REGEX}') THEN 'unit'\n            ELSE 'unrecognized' END\n        , _weighted_preferred: CASE\n            WHEN regexp_matches(_preferred_unit, '{WEIGHT_REGEX}') THEN 1 ELSE 0 END\n        , _convert_status: CASE \n            WHEN _weighted_preferred = 1 AND weight_kg IS NULL \n                THEN 'cannot convert to a weighted unit if weight_kg is missing'\n            WHEN _base_unit IS NULL THEN 'original unit is missing'\n            WHEN _unit_class == 'unrecognized' OR _unit_subclass == 'unrecognized'\n                THEN 'original unit ' || _base_unit || ' is not recognized'\n            WHEN _unit_class_preferred == 'unrecognized' OR _unit_subclass_preferred == 'unrecognized'\n                THEN 'user-preferred unit ' || _preferred_unit || ' is not recognized'\n            WHEN _unit_class != _unit_class_preferred \n                THEN 'cannot convert ' || _unit_class || ' to ' || _unit_class_preferred\n            WHEN _unit_subclass != _unit_subclass_preferred\n                THEN 'cannot convert ' || _unit_subclass || ' to ' || _unit_subclass_preferred\n            WHEN _unit_class == _unit_class_preferred AND _unit_subclass == _unit_subclass_preferred\n                -- AND _unit_class != 'unrecognized' AND _unit_subclass != 'unrecognized'\n                THEN 'success'\n            ELSE 'other error - please report'\n            END\n        , _amount_multiplier_preferred: {amount_clause}\n        , _time_multiplier_preferred: {time_clause}\n        , _weight_multiplier_preferred: {weight_clause}\n        -- fall back to the base units and dose (i.e. the input) if conversion cannot be accommondated\n        , med_dose_converted: CASE\n            WHEN _convert_status == 'success' THEN _base_dose * _amount_multiplier_preferred * _time_multiplier_preferred * _weight_multiplier_preferred\n            ELSE {dose_converted_name}\n            END\n        , med_dose_unit_converted: CASE\n            WHEN _convert_status == 'success' THEN _preferred_unit\n            ELSE {unit_converted_name}\n            END\n    FROM med_df l\n    \"\"\"\n    return duckdb.sql(q)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._create_unit_conversion_counts_table","title":"clifpy.utils.unit_converter._create_unit_conversion_counts_table","text":"<pre><code>_create_unit_conversion_counts_table(med_df, group_by)\n</code></pre> <p>Create summary table of unit conversion counts.</p> <p>Generates a grouped summary showing the frequency of each unit conversion pattern, useful for data quality assessment and identifying common or problematic unit patterns.</p> <p>Parameters:</p> Name Type Description Default <code>med_df</code> <code>DataFrame</code> <p>DataFrame with required columns from conversion process:</p> <ul> <li>med_dose_unit: Original unit string</li> <li>_clean_unit: Cleaned unit string</li> <li>_base_unit: base standard unit</li> <li>_unit_class: Classification (rate/amount/unrecognized)</li> </ul> required <code>group_by</code> <code>List[str]</code> <p>List of columns to group by.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary DataFrame with columns:</p> <ul> <li>med_dose_unit: Original unit</li> <li>_clean_unit: After cleaning</li> <li>_base_unit: After conversion</li> <li>_unit_class: Classification</li> <li>count: Number of occurrences</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from input DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # df_base = standardize_dose_to_base_units(med_df)[0]\n&gt;&gt;&gt; # counts = _create_unit_conversion_counts_table(df_base, ['med_dose_unit'])\n&gt;&gt;&gt; # 'count' in counts.columns\nTrue\n</code></pre> Notes <p>This table is particularly useful for:</p> <ul> <li>Identifying unrecognized units that need handling</li> <li>Understanding the distribution of unit types in your data</li> <li>Quality control and validation of conversions</li> </ul> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _create_unit_conversion_counts_table(\n    med_df: pd.DataFrame | duckdb.DuckDBPyRelation,\n    group_by: List[str]\n    ) -&gt; duckdb.DuckDBPyRelation:\n    \"\"\"Create summary table of unit conversion counts.\n\n    Generates a grouped summary showing the frequency of each unit conversion\n    pattern, useful for data quality assessment and identifying common or\n    problematic unit patterns.\n\n    Parameters\n    ----------\n    med_df : pd.DataFrame\n        DataFrame with required columns from conversion process:\n\n        - med_dose_unit: Original unit string\n        - _clean_unit: Cleaned unit string\n        - _base_unit: base standard unit\n        - _unit_class: Classification (rate/amount/unrecognized)\n    group_by : List[str]\n        List of columns to group by.\n\n    Returns\n    -------\n    pd.DataFrame\n        Summary DataFrame with columns:\n\n        - med_dose_unit: Original unit\n        - _clean_unit: After cleaning\n        - _base_unit: After conversion\n        - _unit_class: Classification\n        - count: Number of occurrences\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from input DataFrame.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; # df_base = standardize_dose_to_base_units(med_df)[0]\n    &gt;&gt;&gt; # counts = _create_unit_conversion_counts_table(df_base, ['med_dose_unit'])\n    &gt;&gt;&gt; # 'count' in counts.columns\n    True\n\n    Notes\n    -----\n    This table is particularly useful for:\n\n    - Identifying unrecognized units that need handling\n    - Understanding the distribution of unit types in your data\n    - Quality control and validation of conversions\n    \"\"\"\n    # check presense of all the group by columns\n    # required_columns = {'med_dose_unit', 'med_dose_unit_normalized', 'med_dose_unit_limited', 'unit_class'}\n    missing_columns = set(group_by) - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    # build the string that enumerates the group by columns \n    # e.g. 'med_dose_unit, med_dose_unit_normalized, unit_class'\n    cols_enum_str = f\"{', '.join(group_by)}\"\n    order_by_clause = f\"med_category, count DESC\" if 'med_category' in group_by else \"count DESC\"\n\n    q = f\"\"\"\n    SELECT {cols_enum_str}   \n        , COUNT(*) as count\n    FROM med_df\n    GROUP BY {cols_enum_str}\n    ORDER BY {order_by_clause}\n    \"\"\"\n    return duckdb.sql(q)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._convert_set_to_str_for_sql","title":"clifpy.utils.unit_converter._convert_set_to_str_for_sql","text":"<pre><code>_convert_set_to_str_for_sql(s)\n</code></pre> <p>Convert a set of strings to SQL IN clause format.</p> <p>Transforms a Python set into a comma-separated string suitable for use in SQL IN clauses within DuckDB queries.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Set[str]</code> <p>Set of strings to be formatted for SQL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Comma-separated string with items separated by \"','\". Does not include outer quotes - those are added in SQL query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; units = {'ml/hr', 'mcg/min', 'u/hr'}\n&gt;&gt;&gt; _convert_set_to_str_for_sql(units)\n\"ml/hr','mcg/min','u/hr\"\n</code></pre> <p>Usage in SQL queries:</p> <pre><code>&gt;&gt;&gt; # f\"WHERE unit IN ('{_convert_set_to_str_for_sql(units)}')\"\n</code></pre> Notes <p>This is a helper function for building DuckDB SQL queries that need to check if values are in a set of acceptable units.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _convert_set_to_str_for_sql(s: Set[str]) -&gt; str:\n    \"\"\"Convert a set of strings to SQL IN clause format.\n\n    Transforms a Python set into a comma-separated string suitable for use\n    in SQL IN clauses within DuckDB queries.\n\n    Parameters\n    ----------\n    s : Set[str]\n        Set of strings to be formatted for SQL.\n\n    Returns\n    -------\n    str\n        Comma-separated string with items separated by \"','\".\n        Does not include outer quotes - those are added in SQL query.\n\n    Examples\n    --------\n    &gt;&gt;&gt; units = {'ml/hr', 'mcg/min', 'u/hr'}\n    &gt;&gt;&gt; _convert_set_to_str_for_sql(units)\n    \"ml/hr','mcg/min','u/hr\"\n\n    Usage in SQL queries:\n\n    &gt;&gt;&gt; # f\"WHERE unit IN ('{_convert_set_to_str_for_sql(units)}')\"\n\n    Notes\n    -----\n    This is a helper function for building DuckDB SQL queries that need to check\n    if values are in a set of acceptable units.\n    \"\"\"\n    return \"','\".join(s)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._concat_builders_by_patterns","title":"clifpy.utils.unit_converter._concat_builders_by_patterns","text":"<pre><code>_concat_builders_by_patterns(\n    builder, patterns, else_case=\"1\"\n)\n</code></pre> <p>Concatenate multiple SQL CASE WHEN statements from patterns.</p> <p>Helper function that combines multiple regex pattern builders into a single SQL CASE statement for DuckDB queries. Used internally to build conversion factor calculations for different unit components (amount, time, weight).</p> <p>Parameters:</p> Name Type Description Default <code>builder</code> <code>callable</code> <p>Function that generates CASE WHEN clauses from regex patterns. Should accept a pattern string and return a WHEN...THEN clause.</p> required <code>patterns</code> <code>list</code> <p>List of regex patterns to process with the builder function.</p> required <code>else_case</code> <code>str</code> <p>Value to use in the ELSE clause when no patterns match. Default is '1' (no conversion factor).</p> <code>'1'</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete SQL CASE statement with all pattern conditions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; patterns = ['/hr$', '/min$']\n&gt;&gt;&gt; builder = lambda p: f\"WHEN regexp_matches(col, '{p}') THEN factor\"\n&gt;&gt;&gt; result = _concat_builders_by_patterns(builder, patterns)\n&gt;&gt;&gt; 'CASE WHEN' in result and 'ELSE 1 END' in result\nTrue\n</code></pre> Notes <p>This function is used internally by conversion functions to build SQL queries that apply different conversion factors based on unit patterns.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _concat_builders_by_patterns(builder: callable, patterns: list, else_case: str = '1') -&gt; str:\n    \"\"\"Concatenate multiple SQL CASE WHEN statements from patterns.\n\n    Helper function that combines multiple regex pattern builders into a single\n    SQL CASE statement for DuckDB queries. Used internally to build conversion\n    factor calculations for different unit components (amount, time, weight).\n\n    Parameters\n    ----------\n    builder : callable\n        Function that generates CASE WHEN clauses from regex patterns.\n        Should accept a pattern string and return a WHEN...THEN clause.\n    patterns : list\n        List of regex patterns to process with the builder function.\n    else_case : str, default '1'\n        Value to use in the ELSE clause when no patterns match.\n        Default is '1' (no conversion factor).\n\n    Returns\n    -------\n    str\n        Complete SQL CASE statement with all pattern conditions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; patterns = ['/hr$', '/min$']\n    &gt;&gt;&gt; builder = lambda p: f\"WHEN regexp_matches(col, '{p}') THEN factor\"\n    &gt;&gt;&gt; result = _concat_builders_by_patterns(builder, patterns)\n    &gt;&gt;&gt; 'CASE WHEN' in result and 'ELSE 1 END' in result\n    True\n\n    Notes\n    -----\n    This function is used internally by conversion functions to build\n    SQL queries that apply different conversion factors based on unit patterns.\n    \"\"\"\n    return \"CASE \" + \" \".join([builder(pattern) for pattern in patterns]) + f\" ELSE {else_case} END\"\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_base","title":"clifpy.utils.unit_converter._pattern_to_factor_builder_for_base","text":"<pre><code>_pattern_to_factor_builder_for_base(pattern)\n</code></pre> <p>Build SQL CASE WHEN statement for regex pattern matching.</p> <p>Helper function that generates SQL CASE WHEN clauses for DuckDB queries based on regex patterns and their corresponding conversion factors.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regex pattern to match (must exist in REGEX_TO_FACTOR_MAPPER).</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL CASE WHEN clause string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pattern is not found in REGEX_TO_FACTOR_MAPPER.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clause = _pattern_to_factor_builder_for_base(HR_REGEX)\n&gt;&gt;&gt; 'WHEN regexp_matches' in clause and 'THEN' in clause\nTrue\n</code></pre> Notes <p>This function is used internally by _convert_clean_dose_units_to_base_units to build the SQL query for unit conversion.</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _pattern_to_factor_builder_for_base(pattern: str) -&gt; str:\n    \"\"\"Build SQL CASE WHEN statement for regex pattern matching.\n\n    Helper function that generates SQL CASE WHEN clauses for DuckDB queries\n    based on regex patterns and their corresponding conversion factors.\n\n    Parameters\n    ----------\n    pattern : str\n        Regex pattern to match (must exist in REGEX_TO_FACTOR_MAPPER).\n\n    Returns\n    -------\n    str\n        SQL CASE WHEN clause string.\n\n    Raises\n    ------\n    ValueError\n        If the pattern is not found in REGEX_TO_FACTOR_MAPPER.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clause = _pattern_to_factor_builder_for_base(HR_REGEX)\n    &gt;&gt;&gt; 'WHEN regexp_matches' in clause and 'THEN' in clause\n    True\n\n    Notes\n    -----\n    This function is used internally by _convert_clean_dose_units_to_base_units\n    to build the SQL query for unit conversion.\n    \"\"\"\n    if pattern in REGEX_TO_FACTOR_MAPPER:\n        return f\"WHEN regexp_matches(_clean_unit, '{pattern}') THEN {REGEX_TO_FACTOR_MAPPER.get(pattern)}\"\n    raise ValueError(f\"regex pattern {pattern} not found in REGEX_TO_FACTOR_MAPPER dict\")\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred","title":"clifpy.utils.unit_converter._pattern_to_factor_builder_for_preferred","text":"<pre><code>_pattern_to_factor_builder_for_preferred(pattern)\n</code></pre> <p>Build SQL CASE WHEN statement for preferred unit conversion.</p> <p>Generates SQL clauses for converting from base units back to preferred units by applying the inverse of the original conversion factor. Used when converting from standardized base units to medication-specific preferred units.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regex pattern to match in _preferred_unit column. Must exist in REGEX_TO_FACTOR_MAPPER dictionary.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL CASE WHEN clause with inverse conversion factor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the pattern is not found in REGEX_TO_FACTOR_MAPPER.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clause = _pattern_to_factor_builder_for_preferred('/hr$')\n&gt;&gt;&gt; 'WHEN regexp_matches(_preferred_unit' in clause and 'THEN 1/' in clause\nTrue\n</code></pre> Notes <p>This function applies the inverse of the factor used in _pattern_to_factor_builder_for_base, allowing bidirectional conversion between unit systems. The inverse is calculated as 1/(original_factor).</p> See Also <p>_pattern_to_factor_builder_for_base : Builds patterns for base unit conversion</p> Source code in <code>clifpy/utils/unit_converter.py</code> <pre><code>def _pattern_to_factor_builder_for_preferred(pattern: str) -&gt; str:\n    \"\"\"Build SQL CASE WHEN statement for preferred unit conversion.\n\n    Generates SQL clauses for converting from base units back to preferred units\n    by applying the inverse of the original conversion factor. Used when converting\n    from standardized base units to medication-specific preferred units.\n\n    Parameters\n    ----------\n    pattern : str\n        Regex pattern to match in _preferred_unit column.\n        Must exist in REGEX_TO_FACTOR_MAPPER dictionary.\n\n    Returns\n    -------\n    str\n        SQL CASE WHEN clause with inverse conversion factor.\n\n    Raises\n    ------\n    ValueError\n        If the pattern is not found in REGEX_TO_FACTOR_MAPPER.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clause = _pattern_to_factor_builder_for_preferred('/hr$')\n    &gt;&gt;&gt; 'WHEN regexp_matches(_preferred_unit' in clause and 'THEN 1/' in clause\n    True\n\n    Notes\n    -----\n    This function applies the inverse of the factor used in\n    _pattern_to_factor_builder_for_base, allowing bidirectional conversion\n    between unit systems. The inverse is calculated as 1/(original_factor).\n\n    See Also\n    --------\n    _pattern_to_factor_builder_for_base : Builds patterns for base unit conversion\n    \"\"\"\n    if pattern in REGEX_TO_FACTOR_MAPPER:\n        return f\"WHEN regexp_matches(_preferred_unit, '{pattern}') THEN 1/({REGEX_TO_FACTOR_MAPPER.get(pattern)})\"\n    raise ValueError(f\"regex pattern {pattern} not found in REGEX_TO_FACTOR_MAPPER dict\")\n</code></pre>"},{"location":"api/utilities/#core-data-processing","title":"Core Data Processing","text":""},{"location":"api/utilities/#encounter-stitching","title":"Encounter Stitching","text":"<p>Stitch together hospital encounters that occur within a specified time window, useful for treating rapid readmissions as a single continuous encounter.</p>"},{"location":"api/utilities/#clifpy.utils.stitching_encounters.stitch_encounters","title":"clifpy.utils.stitching_encounters.stitch_encounters","text":"<pre><code>stitch_encounters(hospitalization, adt, time_interval=6)\n</code></pre> <p>Stitches together related hospital encounters that occur within a specified time interval.</p> <p>This function identifies and groups hospitalizations that occur within a specified time window of each other (default 6 hours), treating them as a single continuous encounter. This is useful for handling cases where patients are discharged and readmitted quickly (e.g., ED to inpatient transfers).</p> <p>Parameters:</p> Name Type Description Default <code>hospitalization</code> <code>DataFrame</code> <p>Hospitalization table with required columns: - patient_id - hospitalization_id - admission_dttm - discharge_dttm - age_at_admission - admission_type_category - discharge_category</p> required <code>adt</code> <code>DataFrame</code> <p>ADT (Admission/Discharge/Transfer) table with required columns: - hospitalization_id - in_dttm - out_dttm - location_category - hospital_id</p> required <code>time_interval</code> <code>int</code> <p>Number of hours between discharge and next admission to consider encounters linked. If a patient is readmitted within this window, the encounters are stitched together.</p> <code>6</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>hospitalization_stitched : pd.DataFrame     Enhanced hospitalization data with encounter_block column adt_stitched : pd.DataFrame     Enhanced ADT data with encounter_block column encounter_mapping : pd.DataFrame     Mapping of hospitalization_id to encounter_block</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from input DataFrames</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hosp_stitched, adt_stitched, mapping = stitch_encounters(\n...     hospitalization_df, \n...     adt_df, \n...     time_interval=12  # 12-hour window\n... )\n</code></pre> Source code in <code>clifpy/utils/stitching_encounters.py</code> <pre><code>def stitch_encounters(\n    hospitalization: pd.DataFrame, \n    adt: pd.DataFrame, \n    time_interval: int = 6\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Stitches together related hospital encounters that occur within a specified time interval.\n\n    This function identifies and groups hospitalizations that occur within a specified time window\n    of each other (default 6 hours), treating them as a single continuous encounter. This is useful\n    for handling cases where patients are discharged and readmitted quickly (e.g., ED to inpatient\n    transfers).\n\n    Parameters\n    ----------\n    hospitalization : pd.DataFrame\n        Hospitalization table with required columns:\n        - patient_id\n        - hospitalization_id\n        - admission_dttm\n        - discharge_dttm\n        - age_at_admission\n        - admission_type_category\n        - discharge_category\n\n    adt : pd.DataFrame\n        ADT (Admission/Discharge/Transfer) table with required columns:\n        - hospitalization_id\n        - in_dttm\n        - out_dttm\n        - location_category\n        - hospital_id\n\n    time_interval : int, default=6\n        Number of hours between discharge and next admission to consider encounters linked.\n        If a patient is readmitted within this window, the encounters are stitched together.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n        hospitalization_stitched : pd.DataFrame\n            Enhanced hospitalization data with encounter_block column\n        adt_stitched : pd.DataFrame\n            Enhanced ADT data with encounter_block column\n        encounter_mapping : pd.DataFrame\n            Mapping of hospitalization_id to encounter_block\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing from input DataFrames\n\n    Examples\n    --------\n    &gt;&gt;&gt; hosp_stitched, adt_stitched, mapping = stitch_encounters(\n    ...     hospitalization_df, \n    ...     adt_df, \n    ...     time_interval=12  # 12-hour window\n    ... )\n    \"\"\"\n    # Validate input DataFrames\n    hosp_required_cols = [\n        \"patient_id\", \"hospitalization_id\", \"admission_dttm\", \n        \"discharge_dttm\", \"age_at_admission\", \"admission_type_category\", \n        \"discharge_category\"\n    ]\n    adt_required_cols = [\n        \"hospitalization_id\", \"in_dttm\", \"out_dttm\", \n        \"location_category\", \"hospital_id\"\n    ]\n\n    missing_hosp_cols = [col for col in hosp_required_cols if col not in hospitalization.columns]\n    if missing_hosp_cols:\n        raise ValueError(f\"Missing required columns in hospitalization DataFrame: {missing_hosp_cols}\")\n\n    missing_adt_cols = [col for col in adt_required_cols if col not in adt.columns]\n    if missing_adt_cols:\n        raise ValueError(f\"Missing required columns in ADT DataFrame: {missing_adt_cols}\")\n    hospitalization_filtered = hospitalization[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\n                                                \"discharge_dttm\",\"age_at_admission\", \"admission_type_category\", \"discharge_category\"]].copy()\n    hospitalization_filtered['admission_dttm'] = pd.to_datetime(hospitalization_filtered['admission_dttm'])\n    hospitalization_filtered['discharge_dttm'] = pd.to_datetime(hospitalization_filtered['discharge_dttm'])\n\n    hosp_adt_join = pd.merge(hospitalization_filtered[[\"patient_id\",\"hospitalization_id\",\"age_at_admission\",\"admission_type_category\",\n                                                       \"admission_dttm\",\"discharge_dttm\",\n                                                        \"discharge_category\"]], \n                      adt[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"location_category\",\"hospital_id\"]],\n                 on=\"hospitalization_id\",how=\"left\")\n\n    hospital_cat = hosp_adt_join[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"hospital_id\"]]\n\n    # Step 1: Sort by patient_id and admission_dttm\n    hospital_block = hosp_adt_join[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\", \"age_at_admission\",  \"discharge_category\", \"admission_type_category\"]]\n    hospital_block = hospital_block.drop_duplicates()\n    hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\"]).reset_index(drop=True)\n    hospital_block = hospital_block[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\", \"age_at_admission\",  \"discharge_category\", \"admission_type_category\"]]\n\n    # Step 2: Calculate time between discharge and next admission\n    hospital_block[\"next_admission_dttm\"] = hospital_block.groupby(\"patient_id\")[\"admission_dttm\"].shift(-1)\n    hospital_block[\"discharge_to_next_admission_hrs\"] = (\n        (hospital_block[\"next_admission_dttm\"] - hospital_block[\"discharge_dttm\"]).dt.total_seconds() / 3600\n    )\n\n    # Step 3: Create linked column based on time_interval\n    eps = 1e-6  # tiny tolerance for float rounding\n    hospital_block[\"linked_hrs\"] = (\n        hospital_block[\"discharge_to_next_admission_hrs\"].le(time_interval + eps).fillna(False)\n    )\n\n    # Sort values to ensure correct order\n    hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\"]).reset_index(drop=True)\n\n    # Initialize encounter_block with row indices + 1\n    hospital_block['encounter_block'] = hospital_block.index + 1\n\n    # Iteratively propagate the encounter_block values\n    while True:\n      shifted = hospital_block['encounter_block'].shift(-1)\n      mask = hospital_block['linked_hrs'] &amp; (hospital_block['patient_id'] == hospital_block['patient_id'].shift(-1))\n      old_values = hospital_block['encounter_block'].copy()\n      hospital_block.loc[mask, 'encounter_block'] = shifted[mask]\n      if hospital_block['encounter_block'].equals(old_values):\n          break\n\n    hospital_block['encounter_block'] = hospital_block['encounter_block'].bfill().astype('int32')\n    hospital_block = pd.merge(hospital_block,hospital_cat,how=\"left\",on=\"hospitalization_id\")\n    hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\",\"in_dttm\",\"out_dttm\"]).reset_index(drop=True)\n    hospital_block = hospital_block.drop_duplicates()\n\n    hospital_block2 = hospital_block.groupby(['patient_id','encounter_block']).agg(\n        admission_dttm=pd.NamedAgg(column='admission_dttm', aggfunc='min'),\n        discharge_dttm=pd.NamedAgg(column='discharge_dttm', aggfunc='max'),\n        admission_type_category=pd.NamedAgg(column='admission_type_category', aggfunc='first'),\n        discharge_category=pd.NamedAgg(column='discharge_category', aggfunc='last'),\n        hospital_id = pd.NamedAgg(column='hospital_id', aggfunc='last'),\n        age_at_admission=pd.NamedAgg(column='age_at_admission', aggfunc='last'),\n        list_hospitalization_id=pd.NamedAgg(column='hospitalization_id', aggfunc=lambda x: sorted(x.unique()))\n    ).reset_index()\n\n    df = pd.merge(hospital_block[[\"patient_id\",\n                                  \"hospitalization_id\",\n                                  \"encounter_block\"]].drop_duplicates(),\n             hosp_adt_join[[\"hospitalization_id\",\"location_category\",\"in_dttm\",\"out_dttm\"]], on=\"hospitalization_id\",how=\"left\")\n\n    df = pd.merge(df,hospital_block2[[\"encounter_block\",\n                                      \"admission_dttm\",\n                                      \"discharge_dttm\",\n                                      \"discharge_category\",\n                                      \"admission_type_category\",\n                                      \"age_at_admission\",\n                                      \"hospital_id\",\n                                     \"list_hospitalization_id\"]],on=\"encounter_block\",how=\"left\")\n    df = df.drop_duplicates(subset=[\"patient_id\",\"encounter_block\",\"in_dttm\",\"out_dttm\",\"location_category\"])\n\n    # Create the mapping DataFrame\n    encounter_mapping = hospital_block[[\"hospitalization_id\", \"encounter_block\"]].drop_duplicates()\n\n    # Create hospitalization_stitched DataFrame\n    hospitalization_stitched = hospitalization.merge(\n        encounter_mapping, \n        on=\"hospitalization_id\", \n        how=\"left\"\n    )\n\n    # Create adt_stitched DataFrame  \n    adt_stitched = adt.merge(\n        encounter_mapping,\n        on=\"hospitalization_id\",\n        how=\"left\"\n    )\n\n    return hospitalization_stitched, adt_stitched, encounter_mapping\n</code></pre>"},{"location":"api/utilities/#wide-dataset-creation","title":"Wide Dataset Creation","text":"<p>Transform CLIF tables into wide format for analysis, with automatic pivoting and high-performance processing.</p>"},{"location":"api/utilities/#clifpy.utils.wide_dataset.create_wide_dataset","title":"clifpy.utils.wide_dataset.create_wide_dataset","text":"<pre><code>create_wide_dataset(\n    clif_instance,\n    optional_tables=None,\n    category_filters=None,\n    sample=False,\n    hospitalization_ids=None,\n    cohort_df=None,\n    output_format=\"dataframe\",\n    save_to_data_location=False,\n    output_filename=None,\n    return_dataframe=True,\n    base_table_columns=None,\n    batch_size=1000,\n    memory_limit=None,\n    threads=None,\n    show_progress=True,\n)\n</code></pre> <p>Create a wide dataset by joining multiple CLIF tables with pivoting support.</p> <p>Parameters:</p> Name Type Description Default <code>clif_instance</code> <p>CLIF object with loaded data</p> required <code>optional_tables</code> <code>List[str]</code> <p>DEPRECATED - use category_filters to specify tables</p> <code>None</code> <code>category_filters</code> <code>Dict[str, List[str]]</code> <p>Dict specifying filtering/selection for each table. Behavior differs by table type:</p> <p>PIVOT TABLES (narrow to wide conversion): - Values are category values to filter and pivot into columns - Example: {'vitals': ['heart_rate', 'sbp', 'spo2'],             'labs': ['hemoglobin', 'sodium', 'creatinine']} - Acceptable values come from the category column's permissible values   defined in each table's schema file (clifpy/schemas/*_schema.yaml)</p> <p>WIDE TABLES (already in wide format): - Values are column names to keep from the table - Example: {'respiratory_support': ['device_category', 'fio2_set', 'peep_set']} - Acceptable values are any column names from the table schema</p> <p>Supported tables and their types are defined in: clifpy/schemas/wide_tables_config.yaml</p> <p>Table presence in this dict determines if it will be loaded. For complete lists of acceptable category values, see: - Table schemas: clifpy/schemas/*_schema.yaml - Wide dataset config: clifpy/schemas/wide_tables_config.yaml</p> <code>None</code> <code>sample</code> <code>bool</code> <p>if True, randomly select 20 hospitalizations</p> <code>False</code> <code>hospitalization_ids</code> <code>List[str]</code> <p>List of specific hospitalization IDs to filter</p> <code>None</code> <code>cohort_df</code> <code>DataFrame</code> <p>DataFrame with columns ['hospitalization_id', 'start_time', 'end_time'] If provided, data will be filtered to only include events within the specified time windows for each hospitalization</p> <code>None</code> <code>output_format</code> <code>str</code> <p>'dataframe', 'csv', or 'parquet'</p> <code>'dataframe'</code> <code>save_to_data_location</code> <code>bool</code> <p>save output to data directory</p> <code>False</code> <code>output_filename</code> <code>str</code> <p>Custom filename (default: 'wide_dataset_YYYYMMDD_HHMMSS')</p> <code>None</code> <code>return_dataframe</code> <code>bool</code> <p>return DataFrame even when saving to file</p> <code>True</code> <code>base_table_columns</code> <code>Dict[str, List[str]]</code> <p>DEPRECATED - columns are selected automatically</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of hospitalizations to process in each batch</p> <code>1000</code> <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '8GB')</p> <code>None</code> <code>threads</code> <code>int</code> <p>Number of threads for DuckDB to use</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bars for long operations</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>DataFrame if return_dataframe=True, None otherwise</p> Source code in <code>clifpy/utils/wide_dataset.py</code> <pre><code>def create_wide_dataset(\n    clif_instance,\n    optional_tables: Optional[List[str]] = None,\n    category_filters: Optional[Dict[str, List[str]]] = None,\n    sample: bool = False,\n    hospitalization_ids: Optional[List[str]] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    output_format: str = 'dataframe',\n    save_to_data_location: bool = False,\n    output_filename: Optional[str] = None,\n    return_dataframe: bool = True,\n    base_table_columns: Optional[Dict[str, List[str]]] = None,\n    batch_size: int = 1000,\n    memory_limit: Optional[str] = None,\n    threads: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Create a wide dataset by joining multiple CLIF tables with pivoting support.\n\n    Parameters\n    ----------\n    clif_instance\n        CLIF object with loaded data\n    optional_tables : List[str], optional\n        DEPRECATED - use category_filters to specify tables\n    category_filters : Dict[str, List[str]], optional\n        Dict specifying filtering/selection for each table. Behavior differs by table type:\n\n        **PIVOT TABLES** (narrow to wide conversion):\n        - Values are **category values** to filter and pivot into columns\n        - Example: {'vitals': ['heart_rate', 'sbp', 'spo2'],\n                    'labs': ['hemoglobin', 'sodium', 'creatinine']}\n        - Acceptable values come from the category column's permissible values\n          defined in each table's schema file (clifpy/schemas/*_schema.yaml)\n\n        **WIDE TABLES** (already in wide format):\n        - Values are **column names** to keep from the table\n        - Example: {'respiratory_support': ['device_category', 'fio2_set', 'peep_set']}\n        - Acceptable values are any column names from the table schema\n\n        **Supported tables and their types are defined in:**\n        clifpy/schemas/wide_tables_config.yaml\n\n        Table presence in this dict determines if it will be loaded.\n        For complete lists of acceptable category values, see:\n        - Table schemas: clifpy/schemas/*_schema.yaml\n        - Wide dataset config: clifpy/schemas/wide_tables_config.yaml\n    sample : bool, default=False\n        if True, randomly select 20 hospitalizations\n    hospitalization_ids : List[str], optional\n        List of specific hospitalization IDs to filter\n    cohort_df : pd.DataFrame, optional\n        DataFrame with columns ['hospitalization_id', 'start_time', 'end_time']\n        If provided, data will be filtered to only include events within the specified\n        time windows for each hospitalization\n    output_format : str, default='dataframe'\n        'dataframe', 'csv', or 'parquet'\n    save_to_data_location : bool, default=False\n        save output to data directory\n    output_filename : str, optional\n        Custom filename (default: 'wide_dataset_YYYYMMDD_HHMMSS')\n    return_dataframe : bool, default=True\n        return DataFrame even when saving to file\n    base_table_columns : Dict[str, List[str]], optional\n        DEPRECATED - columns are selected automatically\n    batch_size : int, default=1000\n        Number of hospitalizations to process in each batch\n    memory_limit : str, optional\n        DuckDB memory limit (e.g., '8GB')\n    threads : int, optional\n        Number of threads for DuckDB to use\n    show_progress : bool, default=True\n        Show progress bars for long operations\n\n    Returns\n    -------\n    pd.DataFrame or None\n        DataFrame if return_dataframe=True, None otherwise\n    \"\"\"\n\n\n    logger.info(\"Phase 4: Wide Dataset Processing (utility function)\")\n    logger.debug(\"  4.1: Starting wide dataset creation\")\n\n    # Validate cohort_df if provided\n    if cohort_df is not None:\n        required_cols = ['hospitalization_id', 'start_time', 'end_time']\n        missing_cols = [col for col in required_cols if col not in cohort_df.columns]\n        if missing_cols:\n            raise ValueError(f\"cohort_df must contain columns: {required_cols}. Missing: {missing_cols}\")\n\n        # Ensure hospitalization_id is string type to match with other tables\n        cohort_df['hospitalization_id'] = cohort_df['hospitalization_id'].astype(str)\n\n        # Ensure time columns are datetime\n        for time_col in ['start_time', 'end_time']:\n            if not pd.api.types.is_datetime64_any_dtype(cohort_df[time_col]):\n                cohort_df[time_col] = pd.to_datetime(cohort_df[time_col])\n\n        logger.info(\"  === SPECIAL: COHORT TIME WINDOW FILTERING ===\")\n        logger.info(f\"       - Processing {len(cohort_df)} hospitalizations with time windows\")\n        logger.debug(f\"       - Ensuring datetime types for start_time, end_time\")\n\n    # Get table types from config\n    PIVOT_TABLES = _get_supported_tables(table_type='pivot')\n    WIDE_TABLES = _get_supported_tables(table_type='wide')\n\n    # Determine which tables to load from category_filters\n    if category_filters is None:\n        category_filters = {}\n\n    # For backward compatibility with optional_tables\n    if optional_tables and not category_filters:\n        logger.warning(\"optional_tables parameter is deprecated. Converting to category_filters format\")\n        category_filters = {table: [] for table in optional_tables}\n\n    tables_to_load = list(category_filters.keys())\n\n    # Create DuckDB connection with optimized settings\n    conn_config = {\n        'preserve_insertion_order': 'false'\n    }\n\n    if memory_limit:\n        conn_config['memory_limit'] = memory_limit\n    if threads:\n        conn_config['threads'] = str(threads)\n\n    # Use context manager for connection\n    with duckdb.connect(':memory:', config=conn_config) as conn:\n        # Preserve timezone from clif_instance configuration\n        conn.execute(f\"SET timezone = '{clif_instance.timezone}'\")\n        # Set additional optimization settings\n        conn.execute(\"SET preserve_insertion_order = false\")\n\n        # Get hospitalization IDs to process\n        hospitalization_df = clif_instance.hospitalization.df.copy()\n\n        if hospitalization_ids is not None:\n            logger.info(f\"Filtering to specific hospitalization IDs: {len(hospitalization_ids)} encounters\")\n            required_ids = hospitalization_ids\n        elif cohort_df is not None:\n            # Use hospitalization IDs from cohort_df\n            required_ids = cohort_df['hospitalization_id'].unique().tolist()\n            logger.info(f\"Using {len(required_ids)} hospitalization IDs from cohort_df\")\n        elif sample:\n            logger.info(\"Sampling 20 random hospitalizations\")\n            all_ids = hospitalization_df['hospitalization_id'].unique()\n            required_ids = np.random.choice(all_ids, size=min(20, len(all_ids)), replace=False).tolist()\n            logger.info(f\"Selected {len(required_ids)} hospitalizations for sampling\")\n        else:\n            required_ids = hospitalization_df['hospitalization_id'].unique().tolist()\n            logger.info(f\"Processing all {len(required_ids)} hospitalizations\")\n\n        # Filter all base tables by required IDs immediately\n        logger.info(\"Loading and filtering base tables\")\n        # Only keep required columns from hospitalization table\n        hosp_required_cols = ['hospitalization_id', 'patient_id', 'age_at_admission']\n        hosp_available_cols = [col for col in hosp_required_cols if col in hospitalization_df.columns]\n        hospitalization_df = hospitalization_df[hosp_available_cols]\n        hospitalization_df = hospitalization_df[hospitalization_df['hospitalization_id'].isin(required_ids)]\n        patient_df = clif_instance.patient.df[['patient_id']].copy()\n\n        # Get ADT with selected columns\n        adt_df = clif_instance.adt.df.copy()\n        adt_df = adt_df[adt_df['hospitalization_id'].isin(required_ids)]\n\n        # Apply time filtering to ADT if cohort_df is provided\n        if cohort_df is not None and 'in_dttm' in adt_df.columns:\n            pre_filter_count = len(adt_df)\n            # Merge with cohort_df to get time windows\n            adt_df = pd.merge(\n                adt_df,\n                cohort_df[['hospitalization_id', 'start_time', 'end_time']],\n                on='hospitalization_id',\n                how='inner'\n            )\n\n            # Ensure in_dttm column is datetime\n            if not pd.api.types.is_datetime64_any_dtype(adt_df['in_dttm']):\n                adt_df['in_dttm'] = pd.to_datetime(adt_df['in_dttm'])\n\n            # Filter to time window\n            adt_df = adt_df[\n                (adt_df['in_dttm'] &gt;= adt_df['start_time']) &amp;\n                (adt_df['in_dttm'] &lt;= adt_df['end_time'])\n            ].copy()\n\n            # Drop the time window columns\n            adt_df = adt_df.drop(columns=['start_time', 'end_time'])\n\n            logger.info(f\"  ADT time filtering: {pre_filter_count} \u2192 {len(adt_df)} records\")\n\n        # Remove duplicate columns and _name columns\n        adt_cols = [col for col in adt_df.columns if not col.endswith('_name') and col != 'patient_id']\n        adt_df = adt_df[adt_cols]\n\n        logger.info(f\"       - Base tables filtered - Hospitalization: {len(hospitalization_df)}, Patient: {len(patient_df)}, ADT: {len(adt_df)}\")\n\n        logger.info(\"  4.2: Determining processing mode\")\n        # Process in batches to avoid memory issues\n        if batch_size &gt; 0 and len(required_ids) &gt; batch_size:\n            logger.info(f\"       - Batch mode: {len(required_ids)} hospitalizations in {len(required_ids)//batch_size + 1} batches of {batch_size}\")\n            logger.info(\"  4.B: === BATCH PROCESSING MODE ===\")\n            return _process_in_batches(\n                conn, clif_instance, required_ids, patient_df, hospitalization_df, adt_df,\n                tables_to_load, category_filters, PIVOT_TABLES, WIDE_TABLES,\n                batch_size, show_progress, save_to_data_location, output_filename,\n                output_format, return_dataframe, cohort_df\n            )\n        else:\n            logger.info(f\"       - Single mode: Processing all {len(required_ids)} hospitalizations at once\")\n            logger.info(\"  4.S: === SINGLE PROCESSING MODE ===\")\n            # Process all at once for small datasets\n            return _process_hospitalizations(\n                conn, clif_instance, required_ids, patient_df, hospitalization_df, adt_df,\n                tables_to_load, category_filters, PIVOT_TABLES, WIDE_TABLES,\n                show_progress, cohort_df\n            )\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.wide_dataset.convert_wide_to_hourly","title":"clifpy.utils.wide_dataset.convert_wide_to_hourly","text":"<pre><code>convert_wide_to_hourly(\n    wide_df,\n    aggregation_config,\n    id_name=\"hospitalization_id\",\n    hourly_window=1,\n    fill_gaps=False,\n    memory_limit=\"4GB\",\n    temp_directory=None,\n    batch_size=None,\n    timezone=\"UTC\",\n)\n</code></pre> <p>Convert a wide dataset to temporal aggregation with user-defined aggregation methods.</p> <p>This function uses DuckDB for high-performance aggregation with event-based windowing.</p> <p>Parameters:</p> Name Type Description Default <code>wide_df</code> <code>DataFrame</code> <p>Wide dataset DataFrame from create_wide_dataset()</p> required <code>aggregation_config</code> <code>Dict[str, List[str]]</code> <p>Dict mapping aggregation methods to list of columns Example: {     'max': ['map', 'temp_c', 'sbp'],     'mean': ['heart_rate', 'respiratory_rate'],     'min': ['spo2'],     'median': ['glucose'],     'first': ['gcs_total', 'rass'],     'last': ['assessment_value'],     'boolean': ['norepinephrine', 'propofol'],     'one_hot_encode': ['medication_name', 'assessment_category'] }</p> required <code>id_name</code> <code>str</code> <p>Column name to use for grouping aggregation - 'hospitalization_id': Group by individual hospitalizations (default) - 'encounter_block': Group by encounter blocks (after encounter stitching) - Any other ID column present in the wide dataset</p> <code>'hospitalization_id'</code> <code>hourly_window</code> <code>int</code> <p>Time window for aggregation in hours (1-72).</p> <p>Windows are event-based (relative to each group's first event): - Window 0: [first_event, first_event + hourly_window hours) - Window 1: [first_event + hourly_window, first_event + 2hourly_window) - Window N: [first_event + Nhourly_window, ...)</p> <p>Common values: 1 (hourly), 2 (bi-hourly), 6 (quarter-day), 12 (half-day),                24 (daily), 72 (3-day - maximum)</p> <code>1</code> <code>fill_gaps</code> <code>bool</code> <p>Whether to create rows for time windows with no data.</p> <ul> <li>False (default): Sparse output - only windows with actual data appear</li> <li>True: Dense output - create ALL windows from 0 to max_window per group,         filling gaps with NaN values (no forward-filling)</li> </ul> <p>Example with events at window 0, 1, 5: - fill_gaps=False: Output has 3 rows (windows 0, 1, 5) - fill_gaps=True: Output has 6 rows (windows 0, 1, 2, 3, 4, 5)                   Windows 2, 3, 4 have NaN for all aggregated columns</p> <code>False</code> <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '4GB', '8GB')</p> <code>'4GB'</code> <code>temp_directory</code> <code>str</code> <p>Directory for temporary files (default: system temp)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Process in batches if dataset is large (auto-determined if None)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime operations in DuckDB (e.g., 'UTC', 'America/New_York')</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Aggregated dataset with columns:</p> <p>Group &amp; Window Identifiers: - {id_name}: Group identifier (hospitalization_id or encounter_block) - window_number: Sequential window index (0-indexed, starts at 0 for each group) - window_start_dttm: Window start timestamp (inclusive) - window_end_dttm: Window end timestamp (exclusive)</p> <p>Context Columns: - patient_id: Patient identifier - day_number: Day number within hospitalization</p> <p>Aggregated Columns: - All columns specified in aggregation_config with appropriate suffixes   (_max, _min, _mean, _median, _first, _last, _boolean, one-hot encoded)</p> <p>Notes: - Windows are relative to each group's first event, not calendar boundaries - window_end_dttm - window_start_dttm = hourly_window hours (always) - When fill_gaps=True, gap windows contain NaN (not forward-filled) - When fill_gaps=False, only windows with data appear (sparse output)</p> Source code in <code>clifpy/utils/wide_dataset.py</code> <pre><code>def convert_wide_to_hourly(\n    wide_df: pd.DataFrame,\n    aggregation_config: Dict[str, List[str]],\n    id_name: str = 'hospitalization_id',\n    hourly_window: int = 1,\n    fill_gaps: bool = False,\n    memory_limit: str = '4GB',\n    temp_directory: Optional[str] = None,\n    batch_size: Optional[int] = None,\n    timezone: str = 'UTC'\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a wide dataset to temporal aggregation with user-defined aggregation methods.\n\n    This function uses DuckDB for high-performance aggregation with event-based windowing.\n\n    Parameters\n    ----------\n    wide_df : pd.DataFrame\n        Wide dataset DataFrame from create_wide_dataset()\n    aggregation_config : Dict[str, List[str]]\n        Dict mapping aggregation methods to list of columns\n        Example: {\n            'max': ['map', 'temp_c', 'sbp'],\n            'mean': ['heart_rate', 'respiratory_rate'],\n            'min': ['spo2'],\n            'median': ['glucose'],\n            'first': ['gcs_total', 'rass'],\n            'last': ['assessment_value'],\n            'boolean': ['norepinephrine', 'propofol'],\n            'one_hot_encode': ['medication_name', 'assessment_category']\n        }\n    id_name : str, default='hospitalization_id'\n        Column name to use for grouping aggregation\n        - 'hospitalization_id': Group by individual hospitalizations (default)\n        - 'encounter_block': Group by encounter blocks (after encounter stitching)\n        - Any other ID column present in the wide dataset\n    hourly_window : int, default=1\n        Time window for aggregation in hours (1-72).\n\n        Windows are event-based (relative to each group's first event):\n        - Window 0: [first_event, first_event + hourly_window hours)\n        - Window 1: [first_event + hourly_window, first_event + 2*hourly_window)\n        - Window N: [first_event + N*hourly_window, ...)\n\n        Common values: 1 (hourly), 2 (bi-hourly), 6 (quarter-day), 12 (half-day),\n                       24 (daily), 72 (3-day - maximum)\n    fill_gaps : bool, default=False\n        Whether to create rows for time windows with no data.\n\n        - False (default): Sparse output - only windows with actual data appear\n        - True: Dense output - create ALL windows from 0 to max_window per group,\n                filling gaps with NaN values (no forward-filling)\n\n        Example with events at window 0, 1, 5:\n        - fill_gaps=False: Output has 3 rows (windows 0, 1, 5)\n        - fill_gaps=True: Output has 6 rows (windows 0, 1, 2, 3, 4, 5)\n                          Windows 2, 3, 4 have NaN for all aggregated columns\n    memory_limit : str, default='4GB'\n        DuckDB memory limit (e.g., '4GB', '8GB')\n    temp_directory : str, optional\n        Directory for temporary files (default: system temp)\n    batch_size : int, optional\n        Process in batches if dataset is large (auto-determined if None)\n    timezone : str, default='UTC'\n        Timezone for datetime operations in DuckDB (e.g., 'UTC', 'America/New_York')\n\n    Returns\n    -------\n    pd.DataFrame\n        Aggregated dataset with columns:\n\n        **Group &amp; Window Identifiers:**\n        - {id_name}: Group identifier (hospitalization_id or encounter_block)\n        - window_number: Sequential window index (0-indexed, starts at 0 for each group)\n        - window_start_dttm: Window start timestamp (inclusive)\n        - window_end_dttm: Window end timestamp (exclusive)\n\n        **Context Columns:**\n        - patient_id: Patient identifier\n        - day_number: Day number within hospitalization\n\n        **Aggregated Columns:**\n        - All columns specified in aggregation_config with appropriate suffixes\n          (_max, _min, _mean, _median, _first, _last, _boolean, one-hot encoded)\n\n        **Notes:**\n        - Windows are relative to each group's first event, not calendar boundaries\n        - window_end_dttm - window_start_dttm = hourly_window hours (always)\n        - When fill_gaps=True, gap windows contain NaN (not forward-filled)\n        - When fill_gaps=False, only windows with data appear (sparse output)\n    \"\"\"\n\n    # Validate hourly_window parameter\n    if not isinstance(hourly_window, int):\n        raise ValueError(f\"hourly_window must be an integer, got: {type(hourly_window).__name__}\")\n    if hourly_window &lt; 1 or hourly_window &gt; 72:\n        raise ValueError(f\"hourly_window must be between 1 and 72 hours, got: {hourly_window}\")\n\n    # Validate fill_gaps parameter\n    if not isinstance(fill_gaps, bool):\n        raise ValueError(f\"fill_gaps must be a boolean, got: {type(fill_gaps).__name__}\")\n\n    # Strip timezone from datetime columns (no conversion, just remove tz metadata)\n    wide_df = wide_df.copy()\n    for col in wide_df.columns:\n        if pd.api.types.is_datetime64_any_dtype(wide_df[col]):\n            if hasattr(wide_df[col].dtype, 'tz') and wide_df[col].dtype.tz is not None:\n                wide_df[col] = wide_df[col].dt.tz_localize(None)\n\n    # Update log statements\n    window_label = \"hourly\" if hourly_window == 1 else f\"{hourly_window}-hour\"\n    gap_handling = \"with gap filling\" if fill_gaps else \"sparse (no gap filling)\"\n    logger.info(f\"Starting optimized {window_label} aggregation using DuckDB {gap_handling}\")\n    logger.info(f\"Input dataset shape: {wide_df.shape}\")\n    logger.debug(f\"Memory limit: {memory_limit}\")\n    logger.debug(f\"Aggregation window: {hourly_window} hour(s)\")\n    logger.debug(f\"Gap filling: {'enabled' if fill_gaps else 'disabled'}\")\n\n    # Validate input\n    required_columns = ['event_time', id_name, 'day_number']\n    for col in required_columns:\n        if col not in wide_df.columns:\n            raise ValueError(f\"wide_df must contain '{col}' column\")\n\n    # Auto-determine batch size for very large datasets\n    if batch_size is None:\n        n_rows = len(wide_df)\n        n_ids = wide_df[id_name].nunique()\n\n        # Use batching if dataset is very large\n        if n_rows &gt; 1_000_000 or n_ids &gt; 10_000:\n            batch_size = min(5000, n_ids // 4)\n            logger.info(f\"Large dataset detected ({n_rows:,} rows, {n_ids:,} {id_name}s)\")\n            logger.info(f\"Will process in batches of {batch_size} {id_name}s\")\n        else:\n            batch_size = 0  # Process all at once\n\n    # Configure DuckDB connection\n    config = {\n        'memory_limit': memory_limit,\n        'temp_directory': temp_directory or '/tmp/duckdb_temp',\n        'preserve_insertion_order': 'false',\n        'threads': '4'\n    }\n\n    # Remove None values from config\n    config = {k: v for k, v in config.items() if v is not None}\n\n    try:\n        # Create DuckDB connection with error handling\n        with duckdb.connect(':memory:', config=config) as conn:\n            # Use timezone from parameter (passed from orchestrator)\n            conn.execute(f\"SET timezone = '{timezone}'\")\n            # Set additional optimization settings\n            conn.execute(\"SET preserve_insertion_order = false\")\n\n            if batch_size &gt; 0:\n                return _process_hourly_in_batches(conn, wide_df, aggregation_config, id_name, batch_size, hourly_window, fill_gaps)\n            else:\n                return _process_hourly_single_batch(conn, wide_df, aggregation_config, id_name, hourly_window, fill_gaps)\n\n    except Exception as e:\n        logger.error(f\"DuckDB processing failed: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/utilities/#respiratory-support-processing","title":"Respiratory Support Processing","text":""},{"location":"api/utilities/#waterfall-processing","title":"Waterfall Processing","text":"<p>Apply sophisticated data cleaning and imputation to respiratory support data for complete ventilator timelines.</p>"},{"location":"api/utilities/#clifpy.utils.waterfall.process_resp_support_waterfall","title":"clifpy.utils.waterfall.process_resp_support_waterfall","text":"<pre><code>process_resp_support_waterfall(\n    resp_support,\n    *,\n    id_col=\"hospitalization_id\",\n    bfill=False,\n    verbose=True\n)\n</code></pre> <p>Clean + waterfall-fill the CLIF <code>resp_support</code> table (Python port of Nick's reference R pipeline).</p> <p>Parameters:</p> Name Type Description Default <code>resp_support</code> <code>DataFrame</code> <p>Raw CLIF respiratory-support table already in UTC.</p> required <code>id_col</code> <code>str</code> <p>Encounter-level identifier column.</p> <code>``\"hospitalization_id\"``</code> <code>bfill</code> <code>bool</code> <p>If True, numeric setters are back-filled after forward-fill. If False (default) only forward-fill is used.</p> <code>``False``</code> <code>verbose</code> <code>bool</code> <p>Prints progress banners when True.</p> <code>``True``</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Fully processed table with</p> <ul> <li>hourly scaffold rows (<code>HH:59:59</code>) inserted,</li> <li>device / mode heuristics applied,</li> <li>hierarchical episode IDs (<code>device_cat_id \u2192 \u2026</code>),</li> <li>numeric waterfall fill inside each <code>mode_name_id</code> block   (forward-only or bi-directional per bfill),</li> <li>tracheostomy flag forward-filled,</li> <li>one unique row per <code>(id_col, recorded_dttm)</code> in   chronological order.</li> </ul> Notes <p>The function does not change time-zones; convert before calling if needed.</p> Source code in <code>clifpy/utils/waterfall.py</code> <pre><code>def process_resp_support_waterfall(\n    resp_support: pd.DataFrame,\n    *,\n    id_col: str = \"hospitalization_id\",\n    bfill: bool = False,                \n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Clean + waterfall-fill the CLIF **`resp_support`** table\n    (Python port of Nick's reference R pipeline).\n\n    Parameters\n    ----------\n    resp_support : pd.DataFrame\n        Raw CLIF respiratory-support table **already in UTC**.\n    id_col : str, default ``\"hospitalization_id\"``\n        Encounter-level identifier column.\n    bfill : bool, default ``False``\n        If *True*, numeric setters are back-filled after forward-fill.\n        If *False* (default) only forward-fill is used.\n    verbose : bool, default ``True``\n        Prints progress banners when *True*.\n\n    Returns\n    -------\n    pd.DataFrame\n        Fully processed table with\n\n        * hourly scaffold rows (``HH:59:59``) inserted,\n        * device / mode heuristics applied,\n        * hierarchical episode IDs (``device_cat_id \u2192 \u2026``),\n        * numeric waterfall fill inside each ``mode_name_id`` block\n          (forward-only or bi-directional per *bfill*),\n        * tracheostomy flag forward-filled,\n        * one unique row per ``(id_col, recorded_dttm)`` in\n          chronological order.\n\n    Notes\n    -----\n    The function **does not** change time-zones; convert before\n    calling if needed.\n    \"\"\"\n\n    p = print if verbose else (lambda *_, **__: None)\n\n    # ------------------------------------------------------------------ #\n    # Helper: forward-fill only or forward + back depending on flag      #\n    # ------------------------------------------------------------------ #\n    def fb(obj):\n        if isinstance(obj, (pd.DataFrame, pd.Series)):\n            return obj.ffill().bfill() if bfill else obj.ffill()\n        raise TypeError(\"obj must be a pandas DataFrame or Series\")\n\n    # ------------------------------------------------------------------ #\n    # Small helper to build the hourly scaffold                          #\n    #   - tries DuckDB (fast), falls back to pandas                      #\n    # ------------------------------------------------------------------ #\n    def _build_hourly_scaffold(rs: pd.DataFrame) -&gt; pd.DataFrame:\n        # Try DuckDB first\n        try:\n            # local import so package doesn't hard-depend on it\n            if verbose:\n                p(\"  \u2022 Building hourly scaffold via DuckDB\")\n\n            con = duckdb.connect()\n            # Only need id + timestamps for bounds\n            con.register(\"rs\", rs[[id_col, \"recorded_dttm\"]].dropna(subset=[\"recorded_dttm\"]))\n\n            # Generate hourly series from floor(min) to floor(max), then add :59:59\n            sql = f\"\"\"\n            WITH bounds AS (\n              SELECT\n                {id_col} AS id,\n                date_trunc('hour', MIN(recorded_dttm)) AS tmin_h,\n                date_trunc('hour', MAX(recorded_dttm)) AS tmax_h\n              FROM rs\n              GROUP BY 1\n            ),\n            hour_sequence AS (\n              SELECT\n                b.id AS {id_col},\n                gs.ts + INTERVAL '59 minutes 59 seconds' AS recorded_dttm\n              FROM bounds b,\n                   LATERAL generate_series(b.tmin_h, b.tmax_h, INTERVAL 1 HOUR) AS gs(ts)\n            )\n            SELECT {id_col}, recorded_dttm\n            FROM hour_sequence\n            ORDER BY {id_col}, recorded_dttm\n            \"\"\"\n            scaffold = con.execute(sql).df()\n            con.close()\n\n            # Ensure pandas datetime with UTC if input was tz-aware\n            # (function contract says already UTC; this keeps dtype consistent)\n            scaffold[\"recorded_dttm\"] = pd.to_datetime(scaffold[\"recorded_dttm\"], utc=True, errors=\"coerce\")\n            scaffold[\"recorded_date\"] = scaffold[\"recorded_dttm\"].dt.date\n            scaffold[\"recorded_hour\"] = scaffold[\"recorded_dttm\"].dt.hour\n            scaffold[\"is_scaffold\"]   = True\n            return scaffold\n\n        except Exception as e:\n            if verbose:\n                p(f\"  \u2022 DuckDB scaffold unavailable ({type(e).__name__}: {e}). Falling back to pandas...\")\n            # ---- Original pandas scaffold (ground truth) ----\n            rs_copy = rs.copy()\n            rs_copy[\"recorded_date\"] = rs_copy[\"recorded_dttm\"].dt.date\n            rs_copy[\"recorded_hour\"] = rs_copy[\"recorded_dttm\"].dt.hour\n\n            min_max = rs_copy.groupby(id_col)[\"recorded_dttm\"].agg([\"min\", \"max\"]).reset_index()\n            tqdm.pandas(disable=not verbose, desc=\"Creating hourly scaffolds\")\n            scaffold = (\n                min_max.progress_apply(\n                    lambda r: pd.date_range(\n                        r[\"min\"].floor(\"h\"),\n                        r[\"max\"].floor(\"h\"),\n                        freq=\"1h\", tz=\"UTC\"\n                    ),\n                    axis=1,\n                )\n                .explode()\n                .rename(\"recorded_dttm\")\n            )\n            scaffold = (\n                min_max[[id_col]].join(scaffold)\n                .assign(recorded_dttm=lambda d: d[\"recorded_dttm\"].dt.floor(\"h\")\n                                               + pd.Timedelta(minutes=59, seconds=59))\n            )\n            scaffold[\"recorded_date\"] = scaffold[\"recorded_dttm\"].dt.date\n            scaffold[\"recorded_hour\"] = scaffold[\"recorded_dttm\"].dt.hour\n            scaffold[\"is_scaffold\"]   = True\n            return scaffold\n\n    # ------------------------------------------------------------------ #\n    # Phase 0 \u2013 set-up &amp; hourly scaffold                                 #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 0: initialise &amp; create hourly scaffold\")\n    rs = resp_support.copy()\n\n    # Lower-case categorical strings\n    for c in [\"device_category\", \"device_name\", \"mode_category\", \"mode_name\"]:\n        if c in rs.columns:\n            rs[c] = rs[c].str.lower()\n\n    # Numeric coercion\n    num_cols = [\n        \"tracheostomy\", \"fio2_set\", \"lpm_set\", \"peep_set\",\n        \"tidal_volume_set\", \"resp_rate_set\", \"resp_rate_obs\",\n        \"pressure_support_set\", \"peak_inspiratory_pressure_set\",\n    ]\n    num_cols = [c for c in num_cols if c in rs.columns]\n    if num_cols:\n        rs[num_cols] = rs[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n\n    # FiO\u2082 scaling if documented 40 \u2192 0.40\n    if \"fio2_set\" in rs.columns:\n        fio2_mean = rs[\"fio2_set\"].mean(skipna=True)\n        if pd.notna(fio2_mean) and fio2_mean &gt; 1.0:\n            rs.loc[rs[\"fio2_set\"] &gt; 1, \"fio2_set\"] /= 100\n            p(\"  \u2022 Scaled FiO\u2082 values &gt; 1 down by /100\")\n\n    # Build hourly scaffold (DuckDB if available, else pandas)\n    scaffold = _build_hourly_scaffold(rs)\n    if verbose:\n        p(f\"  \u2022 Scaffold rows created: {len(scaffold):,}\")\n\n    # We keep recorded_date/hour on rs only for temporary ops below\n    rs[\"recorded_date\"] = rs[\"recorded_dttm\"].dt.date\n    rs[\"recorded_hour\"] = rs[\"recorded_dttm\"].dt.hour\n\n    # ------------------------------------------------------------------ #\n    # Phase 1 \u2013 heuristic device / mode inference                        #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 1: heuristic inference of device &amp; mode\")\n\n    # Most-frequent fall-back labels\n    device_counts = rs[[\"device_name\", \"device_category\"]].value_counts().reset_index()\n\n    imv_devices = device_counts.loc[device_counts[\"device_category\"] == \"imv\", \"device_name\"]\n    most_common_imv_name = imv_devices.iloc[0] if len(imv_devices) &gt; 0 else \"ventilator\"\n\n    nippv_devices = device_counts.loc[device_counts[\"device_category\"] == \"nippv\", \"device_name\"]\n    most_common_nippv_name = nippv_devices.iloc[0] if len(nippv_devices) &gt; 0 else \"bipap\"\n\n    mode_counts = rs[[\"mode_name\", \"mode_category\"]].value_counts().reset_index()\n    cmv_modes = mode_counts.loc[\n        mode_counts[\"mode_category\"] == \"assist control-volume control\", \"mode_name\"\n    ]\n    most_common_cmv_name = cmv_modes.iloc[0] if len(cmv_modes) &gt; 0 else \"AC/VC\"\n\n    # --- 1-a IMV from mode_category\n    mask = (\n        rs[\"device_category\"].isna() &amp; rs[\"device_name\"].isna()\n        &amp; rs[\"mode_category\"].str.contains(\n            r\"(?:assist control-volume control|simv|pressure control)\", na=False, regex=True\n            )\n    )\n    rs.loc[mask, [\"device_category\", \"device_name\"]] = [\"imv\", most_common_imv_name]\n\n    # --- 1-b IMV look-behind/ahead\n    rs = rs.sort_values([id_col, \"recorded_dttm\"])\n    prev_cat = rs.groupby(id_col)[\"device_category\"].shift()\n    next_cat = rs.groupby(id_col)[\"device_category\"].shift(-1)\n    imv_like = (\n        rs[\"device_category\"].isna()\n        &amp; ((prev_cat == \"imv\") | (next_cat == \"imv\"))\n        &amp; rs[\"peep_set\"].gt(1) &amp; rs[\"resp_rate_set\"].gt(1) &amp; rs[\"tidal_volume_set\"].gt(1)\n    )\n    rs.loc[imv_like, [\"device_category\", \"device_name\"]] = [\"imv\", most_common_imv_name]\n\n    # --- 1-c NIPPV heuristics\n    prev_cat = rs.groupby(id_col)[\"device_category\"].shift()\n    next_cat = rs.groupby(id_col)[\"device_category\"].shift(-1)\n    nippv_like = (\n        rs[\"device_category\"].isna()\n        &amp; ((prev_cat == \"nippv\") | (next_cat == \"nippv\"))\n        &amp; rs[\"peak_inspiratory_pressure_set\"].gt(1)\n        &amp; rs[\"pressure_support_set\"].gt(1)\n    )\n    rs.loc[nippv_like, \"device_category\"] = \"nippv\"\n    rs.loc[nippv_like &amp; rs[\"device_name\"].isna(), \"device_name\"] = most_common_nippv_name\n\n    # --- 1-d Clean duplicates &amp; empty rows\n    rs = rs.sort_values([id_col, \"recorded_dttm\"])\n    rs[\"dup_count\"] = rs.groupby([id_col, \"recorded_dttm\"])[\"recorded_dttm\"].transform(\"size\")\n    rs = rs[~((rs[\"dup_count\"] &gt; 1) &amp; (rs[\"device_category\"] == \"nippv\"))]\n    rs[\"dup_count\"] = rs.groupby([id_col, \"recorded_dttm\"])[\"recorded_dttm\"].transform(\"size\")\n    rs = rs[~((rs[\"dup_count\"] &gt; 1) &amp; rs[\"device_category\"].isna())].drop(columns=\"dup_count\")\n\n    # --- 1-e Guard: nasal-cannula rows must never carry PEEP\n    if \"peep_set\" in rs.columns:\n        mask_bad_nc = (rs[\"device_category\"] == \"nasal cannula\") &amp; rs[\"peep_set\"].gt(0)\n        if mask_bad_nc.any():\n            rs.loc[mask_bad_nc, \"device_category\"] = np.nan\n            p(f\"{mask_bad_nc.sum():,} rows had PEEP&gt;0 on nasal cannula device_category reset\")\n\n    # Drop rows with nothing useful\n    all_na_cols = [\n        \"device_category\", \"device_name\", \"mode_category\", \"mode_name\",\n        \"tracheostomy\", \"fio2_set\", \"lpm_set\", \"peep_set\", \"tidal_volume_set\",\n        \"resp_rate_set\", \"resp_rate_obs\", \"pressure_support_set\",\n        \"peak_inspiratory_pressure_set\",\n    ]\n    rs = rs.dropna(subset=[c for c in all_na_cols if c in rs.columns], how=\"all\")\n\n    # Unique per timestamp\n    rs = rs.drop_duplicates(subset=[id_col, \"recorded_dttm\"], keep=\"first\")\n\n    # Merge scaffold (exactly like original)\n    rs[\"is_scaffold\"] = False\n    rs = pd.concat([rs, scaffold], ignore_index=True).sort_values(\n        [id_col, \"recorded_dttm\", \"recorded_date\", \"recorded_hour\"]\n    )\n\n    # ------------------------------------------------------------------ #\n    # Phase 2 \u2013 hierarchical IDs                                         #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 2: build hierarchical IDs\")\n\n    def change_id(col: pd.Series, by: pd.Series) -&gt; pd.Series:\n        return (\n            col.fillna(\"missing\")\n            .groupby(by)\n            .transform(lambda s: s.ne(s.shift()).cumsum())\n            .astype(\"int32\")\n        )\n\n    rs[\"device_category\"] = rs.groupby(id_col)[\"device_category\"].ffill()\n    rs[\"device_cat_id\"]   = change_id(rs[\"device_category\"], rs[id_col])\n\n    rs[\"device_name\"] = (\n        rs.sort_values(\"recorded_dttm\")\n          .groupby([id_col, \"device_cat_id\"])[\"device_name\"]\n          .transform(fb).infer_objects(copy=False)\n    )\n    rs[\"device_id\"] = change_id(rs[\"device_name\"], rs[id_col])\n\n    rs = rs.sort_values([id_col, \"recorded_dttm\"])\n    rs[\"mode_category\"] = (\n        rs.groupby([id_col, \"device_id\"])[\"mode_category\"]\n          .transform(fb).infer_objects(copy=False)\n    )\n    rs[\"mode_cat_id\"] = change_id(\n        rs[\"mode_category\"].fillna(\"missing\"), rs[id_col]\n    )\n\n    rs[\"mode_name\"] = (\n        rs.groupby([id_col, \"mode_cat_id\"])[\"mode_name\"]\n          .transform(fb).infer_objects(copy=False)\n    )\n    rs[\"mode_name_id\"] = change_id(\n        rs[\"mode_name\"].fillna(\"missing\"), rs[id_col]\n    )\n\n    # ------------------------------------------------------------------ #\n    # Phase 3 \u2013 numeric waterfall                                        #\n    # ------------------------------------------------------------------ #\n    fill_type = \"bi-directional\" if bfill else \"forward-only\"\n    p(f\"\u2726 Phase 3: {fill_type} numeric fill inside mode_name_id blocks\")\n\n    # FiO\u2082 default for room-air\n    if \"fio2_set\" in rs.columns:\n        rs.loc[(rs[\"device_category\"] == \"room air\") &amp; rs[\"fio2_set\"].isna(), \"fio2_set\"] = 0.21\n\n    # Tidal-volume clean-up\n    if \"tidal_volume_set\" in rs.columns:\n        bad_tv = (\n            ((rs[\"mode_category\"] == \"pressure support/cpap\") &amp; rs.get(\"pressure_support_set\").notna())\n            | (rs[\"mode_category\"].isna() &amp; rs.get(\"device_name\").str.contains(\"trach\", na=False))\n            | ((rs[\"mode_category\"] == \"pressure support/cpap\") &amp; rs.get(\"device_name\").str.contains(\"trach\", na=False))\n        )\n        rs.loc[bad_tv, \"tidal_volume_set\"] = np.nan\n\n    num_cols_fill = [\n        c for c in [\n            \"fio2_set\", \"lpm_set\", \"peep_set\", \"tidal_volume_set\",\n            \"pressure_support_set\", \"resp_rate_set\", \"resp_rate_obs\",\n            \"peak_inspiratory_pressure_set\",\n        ] if c in rs.columns\n    ]\n\n    def fill_block(g: pd.DataFrame) -&gt; pd.DataFrame:\n        if (g[\"device_category\"] == \"trach collar\").any():\n            breaker = (g[\"device_category\"] == \"trach collar\").cumsum()\n            return g.groupby(breaker)[num_cols_fill].apply(fb)\n        return fb(g[num_cols_fill])\n\n    p(f\"  \u2022 applying waterfall fill to {rs[id_col].nunique():,} encounters\")\n    tqdm.pandas(disable=not verbose, desc=\"Waterfall fill by mode_name_id\")\n    rs[num_cols_fill] = (\n        rs.groupby([id_col, \"mode_name_id\"], group_keys=False, sort=False)\n          .progress_apply(fill_block)\n    )\n\n    # \u201cT-piece\u201d \u2192 classify as blow-by\n    tpiece = rs[\"mode_category\"].isna() &amp; rs.get(\"device_name\").str.contains(\"t-piece\", na=False)\n    rs.loc[tpiece, \"mode_category\"] = \"blow by\"\n\n    # Tracheostomy flag forward-fill per encounter\n    if \"tracheostomy\" in rs.columns:\n        rs[\"tracheostomy\"] = rs.groupby(id_col)[\"tracheostomy\"].ffill()\n\n    # ------------------------------------------------------------------ #\n    # Phase 4 \u2013 final tidy-up                                            #\n    # ------------------------------------------------------------------ #\n    p(\"\u2726 Phase 4: final dedup &amp; ordering\")\n    rs = (\n        rs.drop_duplicates()\n          .sort_values([id_col, \"recorded_dttm\"])\n          .reset_index(drop=True)\n    )\n\n    # Drop helper cols\n    rs = rs.drop(columns=[c for c in [\"recorded_date\", \"recorded_hour\"] if c in rs.columns])\n\n    p(\"[OK] Respiratory-support waterfall complete.\")\n    return rs\n</code></pre>"},{"location":"api/utilities/#clinical-calculations","title":"Clinical Calculations","text":""},{"location":"api/utilities/#comorbidity-indices","title":"Comorbidity Indices","text":"<p>Calculate Charlson and Elixhauser comorbidity indices from diagnosis data.</p>"},{"location":"api/utilities/#clifpy.utils.comorbidity.calculate_cci","title":"clifpy.utils.comorbidity.calculate_cci","text":"<pre><code>calculate_cci(hospital_diagnosis, hierarchy=True)\n</code></pre> <p>Calculate Charlson Comorbidity Index (CCI) for hospitalizations.</p> <p>This function processes hospital diagnosis data to calculate CCI scores using the Quan (2011) adaptation with ICD-10-CM codes.</p> <p>Parameters:</p> Name Type Description Default <code>hospital_diagnosis</code> <code>HospitalDiagnosis object, pandas DataFrame, or polars DataFrame</code> <p>containing diagnosis data with columns: - hospitalization_id - diagnosis_code - diagnosis_code_format</p> required <code>hierarchy</code> <code>bool</code> <p>Apply assign0 logic to prevent double counting of conditions when both mild and severe forms are present</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: - hospitalization_id (index) - 17 binary condition columns (0/1) - cci_score (weighted sum)</p> Source code in <code>clifpy/utils/comorbidity.py</code> <pre><code>def calculate_cci(\n    hospital_diagnosis: Union['HospitalDiagnosis', pd.DataFrame, pl.DataFrame],\n    hierarchy: bool = True\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Calculate Charlson Comorbidity Index (CCI) for hospitalizations.\n\n    This function processes hospital diagnosis data to calculate CCI scores\n    using the Quan (2011) adaptation with ICD-10-CM codes.\n\n    Parameters\n    ----------\n    hospital_diagnosis : HospitalDiagnosis object, pandas DataFrame, or polars DataFrame\n        containing diagnosis data with columns:\n        - hospitalization_id\n        - diagnosis_code\n        - diagnosis_code_format\n    hierarchy : bool, default=True\n        Apply assign0 logic to prevent double counting\n        of conditions when both mild and severe forms are present\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with columns:\n        - hospitalization_id (index)\n        - 17 binary condition columns (0/1)\n        - cci_score (weighted sum)\n    \"\"\"\n\n    # Load CCI configuration\n    cci_config = _load_cci_config()\n\n    # Print configuration info as requested\n    print(f\"name: \\\"{cci_config['name']}\\\"\")\n    print(f\"version: \\\"{cci_config['version']}\\\"\")\n    print(f\"supported_formats:\")\n    for fmt in cci_config['supported_formats']:\n        print(f\"  - {fmt}\")\n\n    # Convert input to polars DataFrame\n    if hasattr(hospital_diagnosis, 'df'):\n        # HospitalDiagnosis object\n        df = pl.from_pandas(hospital_diagnosis.df)\n    elif isinstance(hospital_diagnosis, pd.DataFrame):\n        df = pl.from_pandas(hospital_diagnosis)\n    elif isinstance(hospital_diagnosis, pl.DataFrame):\n        df = hospital_diagnosis\n    else:\n        raise ValueError(\"hospital_diagnosis must be HospitalDiagnosis object, pandas DataFrame, or polars DataFrame\")\n\n    # Filter to only ICD10CM codes (discard other formats)\n    df_filtered = df.filter(pl.col(\"diagnosis_code_format\") == \"ICD10CM\")\n\n    # Preprocess diagnosis codes: remove decimal parts (e.g., \"I21.45\" -&gt; \"I21\") and uppercase\n    df_processed = df_filtered.with_columns([\n        pl.col(\"diagnosis_code\").str.to_uppercase().str.split(\".\").list.get(0).alias(\"diagnosis_code_clean\")\n    ])\n\n    # Map diagnosis codes to CCI conditions\n    condition_mappings = cci_config['diagnosis_code_mappings']['ICD10CM']\n    weights = cci_config['weights']\n\n    # Create condition presence indicators\n    condition_columns = []\n\n    for condition_name, condition_info in tqdm(condition_mappings.items(), desc=\"Mapping ICD codes to CCI conditions\"):\n        condition_codes = condition_info['codes']\n\n        # Create a boolean expression for this condition\n        condition_expr = pl.lit(False)\n        for code in condition_codes:\n            condition_expr = condition_expr | pl.col(\"diagnosis_code_clean\").str.starts_with(code)\n\n        condition_columns.append(condition_expr.alias(f\"{condition_name}_present\"))\n\n    # Add condition indicators to dataframe\n    df_with_conditions = df_processed.with_columns(condition_columns)\n\n    # Group by hospitalization_id and aggregate condition presence\n    condition_names = list(condition_mappings.keys())\n\n    # Create aggregation expressions\n    agg_exprs = []\n    for condition_name in condition_names:\n        agg_exprs.append(\n            pl.col(f\"{condition_name}_present\").max().alias(condition_name)\n        )\n\n    # Group by hospitalization and get condition presence\n    df_grouped = df_with_conditions.group_by(\"hospitalization_id\").agg(agg_exprs)\n\n    # Apply hierarchy logic if enabled (assign0)\n    if hierarchy:\n        df_grouped = _apply_hierarchy_logic(df_grouped, cci_config['hierarchies'])\n\n    # Calculate CCI score\n    df_with_score = _calculate_cci_score(df_grouped, weights)\n\n    # Convert boolean columns to integers for consistency\n    condition_names = list(condition_mappings.keys())\n    cast_exprs = []\n    for col in df_with_score.columns:\n        if col in condition_names:\n            cast_exprs.append(pl.col(col).cast(pl.Int32).alias(col))\n        else:\n            cast_exprs.append(pl.col(col))\n\n    df_with_score = df_with_score.select(cast_exprs)\n\n    # Ensure hospitalization_id is string type\n    df_with_score = df_with_score.with_columns([\n        pl.col(\"hospitalization_id\").cast(pl.Utf8).alias(\"hospitalization_id\")\n    ])\n\n    # Convert to pandas DataFrame before returning\n    return df_with_score.to_pandas()\n</code></pre>"},{"location":"api/utilities/#data-quality-management","title":"Data Quality Management","text":""},{"location":"api/utilities/#outlier-handling","title":"Outlier Handling","text":"<p>Detect and handle physiologically implausible values using configurable ranges.</p>"},{"location":"api/utilities/#clifpy.utils.outlier_handler.apply_outlier_handling","title":"clifpy.utils.outlier_handler.apply_outlier_handling","text":"<pre><code>apply_outlier_handling(table_obj, outlier_config_path=None)\n</code></pre> <p>Apply outlier handling to a table object's dataframe.</p> <p>This function identifies numeric values that fall outside acceptable ranges and converts them to NaN. For category-dependent columns (vitals, labs, medications, assessments), ranges are applied based on the category value.</p> <p>Uses ultra-fast Polars implementation with progress tracking.</p> <p>Parameters:</p> Name Type Description Default <code>table_obj</code> <p>A pyCLIF table object with .df (DataFrame) and .table_name attributes</p> required <code>outlier_config_path</code> <code>str</code> <p>Path to custom outlier configuration YAML. If None, uses internal CLIF standard config.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>modifies table_obj.df in-place</p> Source code in <code>clifpy/utils/outlier_handler.py</code> <pre><code>def apply_outlier_handling(table_obj, outlier_config_path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Apply outlier handling to a table object's dataframe.\n\n    This function identifies numeric values that fall outside acceptable ranges\n    and converts them to NaN. For category-dependent columns (vitals, labs,\n    medications, assessments), ranges are applied based on the category value.\n\n    Uses ultra-fast Polars implementation with progress tracking.\n\n    Parameters\n    ----------\n    table_obj\n        A pyCLIF table object with .df (DataFrame) and .table_name attributes\n    outlier_config_path : str, optional\n        Path to custom outlier configuration YAML.\n        If None, uses internal CLIF standard config.\n\n    Returns\n    -------\n    None\n        modifies table_obj.df in-place\n    \"\"\"\n    if table_obj.df is None or table_obj.df.empty:\n        print(\"No data to process for outlier handling.\")\n        return\n\n    # Load outlier configuration\n    config = _load_outlier_config(outlier_config_path)\n    if not config:\n        print(\"Failed to load outlier configuration.\")\n        return\n\n    # Print which configuration is being used\n    if outlier_config_path is None:\n        print(\"Using CLIF standard outlier ranges\\n\")\n    else:\n        print(f\"Using custom outlier ranges from: {outlier_config_path}\\n\")\n\n    # Get table-specific configuration\n    table_config = config.get('tables', {}).get(table_obj.table_name, {})\n    if not table_config:\n        print(f\"No outlier configuration found for table: {table_obj.table_name}\")\n        return\n\n    # Filter columns that exist in the dataframe\n    existing_columns = {col: conf for col, conf in table_config.items() if col in table_obj.df.columns}\n\n    if not existing_columns:\n        print(\"No configured columns found in dataframe.\")\n        return\n\n    # Ultra-fast processing with single conversion\n    _process_all_columns_ultra_fast(table_obj, existing_columns)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.outlier_handler.get_outlier_summary","title":"clifpy.utils.outlier_handler.get_outlier_summary","text":"<pre><code>get_outlier_summary(table_obj, outlier_config_path=None)\n</code></pre> <p>Get a summary of potential outliers without modifying the data.</p> <p>This is a convenience wrapper around validate_numeric_ranges_from_config() for interactive use with table objects. It provides actual outlier counts and percentages without modifying the data.</p> <p>Parameters:</p> Name Type Description Default <code>table_obj</code> <p>A pyCLIF table object with .df, .table_name, and .schema attributes</p> required <code>outlier_config_path</code> <code>str</code> <p>Path to custom outlier configuration. If None, uses CLIF standard config.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Summary of outliers with keys: - table_name: Name of the table - total_rows: Total number of rows - config_source: \"CLIF standard\" or \"Custom\" - outliers: List of outlier validation results with counts and percentages</p> See Also <p>clifpy.utils.validator.validate_numeric_ranges_from_config : Core validation function</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from clifpy.tables.vitals import Vitals\n&gt;&gt;&gt; from clifpy.utils.outlier_handler import get_outlier_summary\n&gt;&gt;&gt;\n&gt;&gt;&gt; vitals = Vitals.from_file()\n&gt;&gt;&gt; summary = get_outlier_summary(vitals)\n&gt;&gt;&gt; print(f\"Found {len(summary['outliers'])} outlier patterns\")\n</code></pre> Source code in <code>clifpy/utils/outlier_handler.py</code> <pre><code>def get_outlier_summary(table_obj, outlier_config_path: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get a summary of potential outliers without modifying the data.\n\n    This is a convenience wrapper around validate_numeric_ranges_from_config()\n    for interactive use with table objects. It provides actual outlier counts\n    and percentages without modifying the data.\n\n    Parameters\n    ----------\n    table_obj\n        A pyCLIF table object with .df, .table_name, and .schema attributes\n    outlier_config_path : str, optional\n        Path to custom outlier configuration. If None, uses CLIF standard config.\n\n    Returns\n    -------\n    dict\n        Summary of outliers with keys:\n        - table_name: Name of the table\n        - total_rows: Total number of rows\n        - config_source: \"CLIF standard\" or \"Custom\"\n        - outliers: List of outlier validation results with counts and percentages\n\n    See Also\n    --------\n    clifpy.utils.validator.validate_numeric_ranges_from_config : Core validation function\n\n    Examples\n    --------\n    &gt;&gt;&gt; from clifpy.tables.vitals import Vitals\n    &gt;&gt;&gt; from clifpy.utils.outlier_handler import get_outlier_summary\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; vitals = Vitals.from_file()\n    &gt;&gt;&gt; summary = get_outlier_summary(vitals)\n    &gt;&gt;&gt; print(f\"Found {len(summary['outliers'])} outlier patterns\")\n    \"\"\"\n    if table_obj.df is None or table_obj.df.empty:\n        return {\"status\": \"No data to analyze\"}\n\n    # Load outlier configuration\n    config = _load_outlier_config(outlier_config_path)\n    if not config:\n        return {\"status\": \"Failed to load configuration\"}\n\n    # Check if table has schema\n    if not hasattr(table_obj, 'schema') or table_obj.schema is None:\n        return {\"status\": \"Table schema not available\"}\n\n    # Check if table has outlier configuration\n    table_config = config.get('tables', {}).get(table_obj.table_name, {})\n    if not table_config:\n        return {\"status\": f\"No outlier configuration for table: {table_obj.table_name}\"}\n\n    # Use the validator to get actual outlier analysis\n    from clifpy.utils import validator\n\n    outlier_results = validator.validate_numeric_ranges_from_config(\n        table_obj.df,\n        table_obj.table_name,\n        table_obj.schema,\n        config\n    )\n\n    # Build summary\n    summary = {\n        \"table_name\": table_obj.table_name,\n        \"total_rows\": len(table_obj.df),\n        \"config_source\": \"CLIF standard\" if outlier_config_path is None else \"Custom\",\n        \"outliers\": outlier_results\n    }\n\n    return summary\n</code></pre>"},{"location":"api/utilities/#data-validation","title":"Data Validation","text":"<p>Comprehensive validation functions for ensuring data quality and CLIF compliance.</p>"},{"location":"api/utilities/#clifpy.utils.validator.validate_dataframe","title":"clifpy.utils.validator.validate_dataframe","text":"<pre><code>validate_dataframe(df, spec)\n</code></pre> <p>Validate df against spec.</p> <p>Returns a list of error dictionaries. An empty list means success.</p> <p>For datatype validation:</p> <ul> <li>If a column doesn't match the expected type exactly, the validator checks   if the data can be cast to the correct type</li> <li>Castable type mismatches return warnings with type \"datatype_castable\"</li> <li>Non-castable type mismatches return errors with type \"datatype_mismatch\"</li> <li>Both include descriptive messages about the casting capability</li> </ul> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def validate_dataframe(df: pd.DataFrame, spec: dict[str, Any]) -&gt; List[dict[str, Any]]:\n    \"\"\"Validate *df* against *spec*.\n\n    Returns a list of error dictionaries. An empty list means success.\n\n    For datatype validation:\n\n    - If a column doesn't match the expected type exactly, the validator checks\n      if the data can be cast to the correct type\n    - Castable type mismatches return warnings with type \"datatype_castable\"\n    - Non-castable type mismatches return errors with type \"datatype_mismatch\"\n    - Both include descriptive messages about the casting capability\n    \"\"\"\n\n    errors: List[dict[str, Any]] = []\n\n    # 1. Required columns present ------------------------------------------------\n    req_cols = set(spec.get(\"required_columns\", []))\n    missing = req_cols - set(df.columns)\n    if missing:\n        missing_list = sorted(missing)\n        errors.append({\n            \"type\": \"missing_columns\",\n            \"columns\": missing_list,\n            \"message\": f\"Missing required columns: {', '.join(missing_list)}\"\n        })\n\n    # 2. Per-column checks -------------------------------------------------------\n    for col_spec in spec.get(\"columns\", []):\n        name = col_spec[\"name\"]\n        if name not in df.columns:\n            # If it's required the above block already captured the issue.\n            continue\n\n        series = df[name]\n\n        # 2a. NULL checks -----------------------------------------------------\n        if col_spec.get(\"required\", False):\n            null_cnt = int(series.isna().sum())\n            total_cnt = int(len(series))\n            null_pct = (null_cnt / total_cnt * 100) if total_cnt &gt; 0 else 0.0\n            if null_cnt:\n                errors.append({\n                    \"type\": \"null_values\",\n                    \"column\": name,\n                    \"count\": null_cnt,\n                    \"percent\": round(null_pct, 2),\n                    \"message\": f\"Column '{name}' has {null_cnt} null values ({null_pct:.2f}%) in required field\"\n                })\n\n        # 2b. Datatype checks -------------------------------------------------\n        expected_type = col_spec.get(\"data_type\")\n        checker = _DATATYPE_CHECKERS.get(expected_type)\n        cast_checker = _DATATYPE_CAST_CHECKERS.get(expected_type)\n\n        if checker and not checker(series):\n            # Check if data can be cast to the correct type\n            if cast_checker and cast_checker(series):\n                # Data can be cast - this is a warning, not an error\n                errors.append({\n                    \"type\": \"datatype_castable\",\n                    \"column\": name,\n                    \"expected\": expected_type,\n                    \"actual\": str(series.dtype),\n                    \"message\": f\"Column '{name}' has type {series.dtype} but can be cast to {expected_type}\"\n                })\n            else:\n                # Data cannot be cast - this is an error\n                errors.append({\n                    \"type\": \"datatype_mismatch\",\n                    \"column\": name,\n                    \"expected\": expected_type,\n                    \"actual\": str(series.dtype),\n                    \"message\": f\"Column '{name}' has type {series.dtype} and cannot be cast to {expected_type}\"\n                })\n\n        # # 2c. Category values -------------------------------------------------\n        # if col_spec.get(\"is_category_column\") and col_spec.get(\"permissible_values\"):\n        #     allowed = set(col_spec[\"permissible_values\"])\n        #     actual_values = set(series.dropna().unique())\n\n        #     # Check for missing expected values (permissible values not present in data)\n        #     missing_values = [v for v in allowed if v not in actual_values]\n        #     if missing_values:\n        #         errors.append({\n        #             \"type\": \"missing_category_values\",\n        #             \"column\": name,\n        #             \"missing_values\": missing_values,\n        #             \"message\": f\"Column '{name}' is missing expected category values: {missing_values}\"\n        #         })\n\n    return errors\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.validate_table","title":"clifpy.utils.validator.validate_table","text":"<pre><code>validate_table(df, table_name, spec_dir=None)\n</code></pre> <p>Validate df using the JSON spec for table_name.</p> <p>Convenience wrapper combining :pyfunc:<code>_load_spec</code> and :pyfunc:<code>validate_dataframe</code>.</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def validate_table(\n    df: pd.DataFrame, table_name: str, spec_dir: str | None = None\n) -&gt; List[dict[str, Any]]:\n    \"\"\"Validate *df* using the JSON spec for *table_name*.\n\n    Convenience wrapper combining :pyfunc:`_load_spec` and\n    :pyfunc:`validate_dataframe`.\n    \"\"\"\n\n    spec = _load_spec(table_name, spec_dir)\n    return validate_dataframe(df, spec)\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.check_required_columns","title":"clifpy.utils.validator.check_required_columns","text":"<pre><code>check_required_columns(df, column_names, table_name)\n</code></pre> <p>Validate that required columns are present in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>column_names</code> <code>List[str]</code> <p>List of required column names</p> required <code>table_name</code> <code>str</code> <p>Name of the table being validated</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with validation results including missing columns</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def check_required_columns(\n    df: pd.DataFrame, \n    column_names: List[str], \n    table_name: str\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate that required columns are present in the dataframe.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe to validate\n    column_names : List[str]\n        List of required column names\n    table_name : str\n        Name of the table being validated\n\n    Returns\n    -------\n    dict\n        Dictionary with validation results including missing columns\n    \"\"\"\n    try:\n        missing_columns = [col for col in column_names if col not in df.columns]\n\n        if missing_columns:\n            return {\n                \"type\": \"missing_required_columns\",\n                \"table\": table_name,\n                \"missing_columns\": missing_columns,\n                \"status\": \"error\",\n                \"message\": f\"Table '{table_name}' is missing required columns: {', '.join(missing_columns)}\"\n            }\n\n        return {\n            \"type\": \"missing_required_columns\",\n            \"table\": table_name,\n            \"status\": \"success\",\n            \"message\": f\"Table '{table_name}' has all required columns\"\n        }\n\n    except Exception as e:\n        return {\n            \"type\": \"missing_required_columns\",\n            \"table\": table_name,\n            \"status\": \"error\",\n            \"error_message\": str(e),\n            \"message\": f\"Error checking required columns for table '{table_name}': {str(e)}\"\n        }\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.verify_column_dtypes","title":"clifpy.utils.validator.verify_column_dtypes","text":"<pre><code>verify_column_dtypes(df, schema)\n</code></pre> <p>Ensure columns have correct data types per schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>schema</code> <code>dict</code> <p>Schema containing column definitions</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of datatype mismatch errors</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def verify_column_dtypes(df: pd.DataFrame, schema: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Ensure columns have correct data types per schema.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe to validate\n    schema : dict\n        Schema containing column definitions\n\n    Returns\n    -------\n    List[dict]\n        List of datatype mismatch errors\n    \"\"\"\n    errors = []\n\n    try:\n        for col_spec in schema.get(\"columns\", []):\n            name = col_spec[\"name\"]\n            if name not in df.columns:\n                continue\n\n            expected_type = col_spec.get(\"data_type\")\n            if not expected_type:\n                continue\n\n            series = df[name]\n            checker = _DATATYPE_CHECKERS.get(expected_type)\n            cast_checker = _DATATYPE_CAST_CHECKERS.get(expected_type)\n\n            if checker and not checker(series):\n                # Check if data can be cast to the correct type\n                if cast_checker and cast_checker(series):\n                    # Data can be cast - this is a warning, not an error\n                    errors.append({\n                        \"type\": \"datatype_verification_castable\",\n                        \"column\": name,\n                        \"expected\": expected_type,\n                        \"actual\": str(series.dtype),\n                        \"status\": \"warning\",\n                        \"message\": f\"Column '{name}' has type {series.dtype} but can be cast to {expected_type}\"\n                    })\n                else:\n                    # Data cannot be cast - this is an error\n                    errors.append({\n                        \"type\": \"datatype_verification\",\n                        \"column\": name,\n                        \"expected\": expected_type,\n                        \"actual\": str(series.dtype),\n                        \"status\": \"error\",\n                        \"message\": f\"Column '{name}' has type {series.dtype} and cannot be cast to {expected_type}\"\n                    })\n\n    except Exception as e:\n        errors.append({\n            \"type\": \"datatype_verification\",\n            \"status\": \"error\",\n            \"error_message\": str(e),\n            \"message\": f\"Error during datatype verification: {str(e)}\"\n        })\n\n    return errors\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.validator.validate_categorical_values","title":"clifpy.utils.validator.validate_categorical_values","text":"<pre><code>validate_categorical_values(\n    df,\n    schema,\n    detect_invalid_values=False,\n    return_invalid_df=False,\n)\n</code></pre> <p>Check values against permitted categories.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to validate</p> required <code>schema</code> <code>dict</code> <p>Schema containing category definitions</p> required <code>detect_invalid_values</code> <code>bool</code> <p>If True, detect and report values in data that are not in permissible_values list. If False (default), only check for missing expected values (backward compatible).</p> <code>False</code> <code>return_invalid_df</code> <code>bool</code> <p>If True and detect_invalid_values=True, return a tuple of (errors, invalid_values_df). The invalid_values_df contains all rows with invalid categorical values. Default is False for backward compatibility.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[dict] or Tuple[List[dict], DataFrame]</code> <p>If return_invalid_df=False: List of validation errors (backward compatible) If return_invalid_df=True: Tuple of (errors_list, invalid_values_dataframe)</p> Notes <p>Categorical matching is case-insensitive (e.g., \"Male\" matches \"MALE\").</p> <p>For backward compatibility, set detect_invalid_values=False (default). This will only report missing expected values, not invalid values in the data.</p> Source code in <code>clifpy/utils/validator.py</code> <pre><code>def validate_categorical_values(\n    df: pd.DataFrame,\n    schema: Dict[str, Any],\n    detect_invalid_values: bool = False,\n    return_invalid_df: bool = False\n) -&gt; List[Dict[str, Any]] | Tuple[List[Dict[str, Any]], pd.DataFrame]:\n    \"\"\"\n    Check values against permitted categories.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe to validate\n    schema : dict\n        Schema containing category definitions\n    detect_invalid_values : bool, optional\n        If True, detect and report values in data that are not in permissible_values list.\n        If False (default), only check for missing expected values (backward compatible).\n    return_invalid_df : bool, optional\n        If True and detect_invalid_values=True, return a tuple of (errors, invalid_values_df).\n        The invalid_values_df contains all rows with invalid categorical values.\n        Default is False for backward compatibility.\n\n    Returns\n    -------\n    List[dict] or Tuple[List[dict], pd.DataFrame]\n        If return_invalid_df=False: List of validation errors (backward compatible)\n        If return_invalid_df=True: Tuple of (errors_list, invalid_values_dataframe)\n\n    Notes\n    -----\n    Categorical matching is case-insensitive (e.g., \"Male\" matches \"MALE\").\n\n    For backward compatibility, set detect_invalid_values=False (default).\n    This will only report missing expected values, not invalid values in the data.\n    \"\"\"\n    errors = []\n    invalid_rows_indices = []  # Track rows with invalid values\n\n    try:\n        category_columns = schema.get(\"category_columns\") or []\n\n        for col_spec in schema.get(\"columns\", []):\n            name = col_spec[\"name\"]\n\n            if name not in df.columns or name not in category_columns:\n                continue\n\n            if col_spec.get(\"permissible_values\"):\n                # Original permissible values from schema\n                allowed = set(col_spec[\"permissible_values\"])\n                # Convert to lowercase for case-insensitive comparison\n                allowed_lower = {str(v).lower() for v in allowed}\n\n                # Get actual unique values from data\n                unique_values_raw = df[name].dropna().unique()\n                unique_values_lower = {str(v).lower() for v in unique_values_raw}\n\n                # 1. Check for missing expected values (case-insensitive)\n                # This check is always performed for backward compatibility\n                missing_values = [v for v in allowed if str(v).lower() not in unique_values_lower]\n\n                # Report missing expected values\n                if missing_values:\n                    errors.append({\n                                \"type\": \"missing_categorical_values\",\n                                \"column\": name,\n                                \"missing_values\": missing_values,\n                                \"total_missing\": len(missing_values),\n                                \"message\": f\"Column '{name}' is missing {len(missing_values)} expected category values: {missing_values}\"\n                            })\n\n                # 2. Check for INVALID values (ONLY if detect_invalid_values=True)\n                if detect_invalid_values:\n                    invalid_values = []\n                    invalid_value_counts = {}\n\n                    # Identify and count occurrences of each invalid value\n                    for val in unique_values_raw:\n                        if str(val).lower() not in allowed_lower:\n                            invalid_values.append(val)\n                            # Count how many times this invalid value appears\n                            count = int((df[name] == val).sum())\n                            invalid_value_counts[str(val)] = count\n\n                            # Track row indices for creating invalid_df later\n                            if return_invalid_df:\n                                invalid_mask = (df[name] == val)\n                                invalid_rows_indices.extend(df[invalid_mask].index.tolist())\n\n                    # Sort invalid values by frequency (most common first)\n                    invalid_values_sorted = sorted(invalid_values,\n                                                   key=lambda x: invalid_value_counts.get(str(x), 0),\n                                                   reverse=True)\n\n                    # Report INVALID values found in data\n                    if invalid_values:\n                        # Show top 10 most common invalid values with their counts\n                        top_invalid_display = []\n                        for val in invalid_values_sorted[:10]:\n                            count = invalid_value_counts[str(val)]\n                            top_invalid_display.append(f\"{val} ({count:,} occurrences)\")\n\n                        errors.append({\n                            \"type\": \"invalid_categorical_values\",\n                            \"column\": name,\n                            \"invalid_values\": invalid_values_sorted[:20],  # Store top 20 for reference\n                            \"invalid_value_counts\": invalid_value_counts,  # Full counts for analysis\n                            \"total_invalid_unique\": len(invalid_values),\n                            \"total_invalid_rows\": sum(invalid_value_counts.values()),\n                            \"permissible_values\": list(allowed),  # Include what IS allowed for reference\n                            \"status\": \"error\",\n                            \"message\": f\"Column '{name}' contains {len(invalid_values)} unique invalid categorical values affecting {sum(invalid_value_counts.values()):,} rows. Top invalid: {', '.join(top_invalid_display[:5])}\"\n                        })\n\n    except Exception as e:\n        errors.append({\n            \"type\": \"categorical_validation\",\n            \"status\": \"error\",\n            \"error_message\": str(e),\n            \"message\": f\"Error validating categorical values: {str(e)}\"\n        })\n\n    # Return based on parameters\n    if return_invalid_df and detect_invalid_values:\n        # Create DataFrame of rows with invalid categorical values\n        if invalid_rows_indices:\n            # Remove duplicates and sort\n            unique_indices = sorted(set(invalid_rows_indices))\n            invalid_df = df.loc[unique_indices].copy()\n        else:\n            # Return empty DataFrame with same columns if no invalid rows\n            invalid_df = pd.DataFrame(columns=df.columns)\n\n        return errors, invalid_df\n    else:\n        # Backward compatible return\n        return errors\n</code></pre>"},{"location":"api/utilities/#configuration-and-io","title":"Configuration and I/O","text":""},{"location":"api/utilities/#configuration-management","title":"Configuration Management","text":"<p>Load and manage CLIF configuration files for consistent settings.</p>"},{"location":"api/utilities/#clifpy.utils.config.load_config","title":"clifpy.utils.config.load_config","text":"<pre><code>load_config(config_path=None)\n</code></pre> <p>Load CLIF configuration from JSON or YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file. If None, looks for 'config.json' or 'config.yaml' in current directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Configuration dictionary with required fields validated</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If config file doesn't exist</p> <code>ValueError</code> <p>If required fields are missing or invalid</p> <code>JSONDecodeError</code> <p>If JSON config file is not valid</p> <code>YAMLError</code> <p>If YAML config file is not valid</p> Source code in <code>clifpy/utils/config.py</code> <pre><code>def load_config(config_path: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load CLIF configuration from JSON or YAML file.\n\n    Parameters\n    ----------\n    config_path : str, optional\n        Path to the configuration file.\n        If None, looks for 'config.json' or 'config.yaml' in current directory.\n\n    Returns\n    -------\n    dict\n        Configuration dictionary with required fields validated\n\n    Raises\n    ------\n    FileNotFoundError\n        If config file doesn't exist\n    ValueError\n        If required fields are missing or invalid\n    json.JSONDecodeError\n        If JSON config file is not valid\n    yaml.YAMLError\n        If YAML config file is not valid\n    \"\"\"\n    # Determine config file path\n    if config_path is None:\n        # Look for config files in order of preference: JSON, YAML, YML\n        cwd = os.getcwd()\n        for filename in ['config.json', 'config.yaml', 'config.yml']:\n            potential_path = os.path.join(cwd, filename)\n            if os.path.exists(potential_path):\n                config_path = potential_path\n                break\n\n        if config_path is None:\n            raise FileNotFoundError(\n                f\"Configuration file not found in {cwd}\\n\"\n                \"Please either:\\n\"\n                \"  1. Create a config.json or config.yaml file in the current directory\\n\"\n                \"  2. Provide config_path parameter pointing to your config file\\n\"\n                \"  3. Provide data_directory, filetype, and timezone parameters directly\"\n            )\n\n    # Check if config file exists\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(\n            f\"Configuration file not found: {config_path}\\n\"\n            \"Please either:\\n\"\n            \"  1. Create a config.json or config.yaml file in the current directory\\n\"\n            \"  2. Provide config_path parameter pointing to your config file\\n\"\n            \"  3. Provide data_directory, filetype, and timezone parameters directly\"\n        )\n\n    # Load configuration using helper function\n    config = _load_config_file(config_path)\n\n    # Validate required fields\n    required_fields = ['data_directory', 'filetype', 'timezone']\n    missing_fields = [field for field in required_fields if field not in config]\n\n    if missing_fields:\n        raise ValueError(\n            f\"Missing required fields in configuration file {config_path}: {missing_fields}\\n\"\n            f\"Required fields are: {required_fields}\"\n        )\n\n    # Validate data_directory exists\n    data_dir = config['data_directory']\n    if not os.path.exists(data_dir):\n        raise ValueError(\n            f\"Data directory specified in config does not exist: {data_dir}\\n\"\n            f\"Please check the 'data_directory' path in {config_path}\"\n        )\n\n    # Validate filetype\n    supported_filetypes = ['csv', 'parquet']\n    if config['filetype'] not in supported_filetypes:\n        raise ValueError(\n            f\"Unsupported filetype '{config['filetype']}' in {config_path}\\n\"\n            f\"Supported filetypes are: {supported_filetypes}\"\n        )\n\n    logger.info(f\"Configuration loaded from {config_path}\")\n    return config\n</code></pre>"},{"location":"api/utilities/#clifpy.utils.config.get_config_or_params","title":"clifpy.utils.config.get_config_or_params","text":"<pre><code>get_config_or_params(\n    config_path=None,\n    data_directory=None,\n    filetype=None,\n    timezone=None,\n    output_directory=None,\n)\n</code></pre> <p>Get configuration from either config file or direct parameters.</p> <p>Loading priority:</p> <ol> <li>If all required params provided directly \u2192 use them</li> <li>If config_path provided \u2192 load from that path, allow param overrides</li> <li>If no params and no config_path \u2192 auto-detect config.json/yaml/yml</li> <li>Parameters override config file values when both are provided</li> </ol> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to configuration file</p> <code>None</code> <code>data_directory</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Direct parameter</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Final configuration dictionary</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither config nor required params are provided</p> Source code in <code>clifpy/utils/config.py</code> <pre><code>def get_config_or_params(\n    config_path: Optional[str] = None,\n    data_directory: Optional[str] = None,\n    filetype: Optional[str] = None,\n    timezone: Optional[str] = None,\n    output_directory: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get configuration from either config file or direct parameters.\n\n    Loading priority:\n\n    1. If all required params provided directly \u2192 use them\n    2. If config_path provided \u2192 load from that path, allow param overrides\n    3. If no params and no config_path \u2192 auto-detect config.json/yaml/yml\n    4. Parameters override config file values when both are provided\n\n    Parameters\n    ----------\n    config_path : str, optional\n        Path to configuration file\n    data_directory : str, optional\n        Direct parameter\n    filetype : str, optional\n        Direct parameter  \n    timezone : str, optional\n        Direct parameter\n    output_directory : str, optional\n        Direct parameter\n\n    Returns\n    -------\n    dict\n        Final configuration dictionary\n\n    Raises\n    ------\n    ValueError\n        If neither config nor required params are provided\n    \"\"\"\n    # Check if all required params are provided directly\n    required_params = [data_directory, filetype, timezone]\n    if all(param is not None for param in required_params):\n        # All required params provided - use them directly\n        config = {\n            'data_directory': data_directory,\n            'filetype': filetype,\n            'timezone': timezone\n        }\n        if output_directory is not None:\n            config['output_directory'] = output_directory\n        logger.debug(\"Using directly provided parameters\")\n        return config\n\n    # Try to load from config file\n    try:\n        config = load_config(config_path)\n    except FileNotFoundError:\n        # If no config file and incomplete params, raise helpful error\n        if any(param is not None for param in required_params):\n            # Some params provided but not all\n            missing = []\n            if data_directory is None:\n                missing.append('data_directory')\n            if filetype is None:\n                missing.append('filetype') \n            if timezone is None:\n                missing.append('timezone')\n            raise ValueError(\n                f\"Incomplete parameters provided. Missing: {missing}\\n\"\n                \"Please either:\\n\"\n                \"  1. Provide all required parameters (data_directory, filetype, timezone)\\n\"\n                \"  2. Create a config.json or config.yaml file\\n\"\n                \"  3. Provide a config_path parameter\"\n            )\n        else:\n            # No params and no config file - re-raise the original error\n            raise\n\n    # Override config values with any provided parameters\n    if data_directory is not None:\n        config['data_directory'] = data_directory\n        logger.debug(f\"Overriding data_directory: {data_directory}\")\n\n    if filetype is not None:\n        config['filetype'] = filetype\n        logger.debug(f\"Overriding filetype: {filetype}\")\n\n    if timezone is not None:\n        config['timezone'] = timezone\n        logger.debug(f\"Overriding timezone: {timezone}\")\n\n    if output_directory is not None:\n        config['output_directory'] = output_directory\n        logger.debug(f\"Overriding output_directory: {output_directory}\")\n\n    return config\n</code></pre>"},{"location":"api/utilities/#data-loading","title":"Data Loading","text":"<p>Core data loading functionality with timezone and filtering support.</p>"},{"location":"api/utilities/#clifpy.utils.io.load_data","title":"clifpy.utils.io.load_data","text":"<pre><code>load_data(\n    table_name,\n    table_path,\n    table_format_type,\n    sample_size=None,\n    columns=None,\n    filters=None,\n    site_tz=None,\n    verbose=False,\n)\n</code></pre> <p>Load data from a file in the specified directory with the option to select specific columns and apply filters.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to load.</p> required <code>table_path</code> <code>str</code> <p>Path to the directory containing the data file.</p> required <code>table_format_type</code> <code>str</code> <p>Format of the data file (e.g., 'csv', 'parquet').</p> required <code>sample_size</code> <code>int</code> <p>Number of rows to load.</p> <code>None</code> <code>columns</code> <code>list of str</code> <p>List of column names to load.</p> <code>None</code> <code>filters</code> <code>dict</code> <p>Dictionary of filters to apply.</p> <code>None</code> <code>site_tz</code> <code>str</code> <p>Timezone string for datetime conversion, e.g., \"America/New_York\".</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, show detailed loading messages. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the requested data.</p> Source code in <code>clifpy/utils/io.py</code> <pre><code>def load_data(table_name, table_path, table_format_type, sample_size=None, columns=None, filters=None, site_tz=None, verbose=False) -&gt; pd.DataFrame:\n    \"\"\"\n    Load data from a file in the specified directory with the option to select specific columns and apply filters.\n\n    Parameters\n    ----------\n    table_name : str\n        The name of the table to load.\n    table_path : str\n        Path to the directory containing the data file.\n    table_format_type : str\n        Format of the data file (e.g., 'csv', 'parquet').\n    sample_size : int, optional\n        Number of rows to load.\n    columns : list of str, optional\n        List of column names to load.\n    filters : dict, optional\n        Dictionary of filters to apply.\n    site_tz : str, optional\n        Timezone string for datetime conversion, e.g., \"America/New_York\".\n    verbose : bool, optional\n        If True, show detailed loading messages. Default is False.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the requested data.\n    \"\"\"\n    # Determine the file path based on the directory and filetype\n\n    file_path = os.path.join(table_path, 'clif_'+ table_name + '.' + table_format_type)\n\n    # Load the data based on filetype\n    if os.path.exists(file_path):\n        if  table_format_type == 'csv':\n            if verbose:\n                logger.info('Loading CSV file')\n            # For CSV, we can use DuckDB to read specific columns and apply filters efficiently\n            con = duckdb.connect()\n            # Build the SELECT clause\n            select_clause = \"*\" if not columns else \", \".join(columns)\n            # Start building the query\n            query = f\"SELECT {select_clause} FROM read_csv_auto('{file_path}')\"\n            # Apply filters\n            if filters:\n                filter_clauses = []\n                for column, values in filters.items():\n                    if isinstance(values, list):\n                        # Escape single quotes and wrap values in quotes\n                        values_list = ', '.join([\"'\" + str(value).replace(\"'\", \"''\") + \"'\" for value in values])\n                        filter_clauses.append(f\"{column} IN ({values_list})\")\n                    else:\n                        value = str(values).replace(\"'\", \"''\")\n                        filter_clauses.append(f\"{column} = '{value}'\")\n                if filter_clauses:\n                    query += \" WHERE \" + \" AND \".join(filter_clauses)\n            # Apply sample size limit\n            if sample_size:\n                query += f\" LIMIT {sample_size}\"\n            # Execute the query and fetch the data\n            df = con.execute(query).fetchdf()\n            con.close()\n        elif table_format_type == 'parquet':\n            df = load_parquet_with_tz(file_path, columns, filters, sample_size, verbose)\n        else:\n            raise ValueError(\"Unsupported filetype. Only 'csv' and 'parquet' are supported.\")\n        # Extract just the filename for cleaner output\n        filename = os.path.basename(file_path)\n        if verbose:\n            logger.info(f\"Data loaded successfully from {filename}\")\n        df = _cast_id_cols_to_string(df) # Cast id columns to string\n\n        # Convert datetime columns to site timezone if specified\n        if site_tz:\n            df = convert_datetime_columns_to_site_tz(df, site_tz, verbose)\n\n        return df\n    else:\n        raise FileNotFoundError(f\"The file {file_path} does not exist in the specified directory.\")\n</code></pre>"},{"location":"api/utilities/#simplified-import-paths","title":"Simplified Import Paths","text":"<p>As of version 0.0.1, commonly used utilities are available directly from the clifpy package:</p> <pre><code># Direct imports from clifpy\nimport clifpy\n\n# Encounter stitching\nhospitalization_stitched, adt_stitched, mapping = clifpy.stitch_encounters(\n    hospitalization_df, \n    adt_df,\n    time_interval=6\n)\n\n# Wide dataset creation\nwide_df = clifpy.create_wide_dataset(\n    clif_instance=orchestrator,\n    optional_tables=['vitals', 'labs'],\n    category_filters={'vitals': ['heart_rate', 'sbp']}\n)\n\n# Calculate comorbidity index\ncci_scores = clifpy.calculate_cci(\n    hospital_diagnosis_df,\n    hospitalization_df\n)\n\n# Apply outlier handling\nclifpy.apply_outlier_handling(table_object)\n</code></pre> <p>For backward compatibility, the original import paths (<code>clifpy.utils.module.function</code>) remain available.</p>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the CLIFpy User Guide. CLIFpy makes working with CLIF (Common Longitudinal ICU data Format) data straightforward and efficient for researchers, data scientists, and clinicians analyzing ICU outcomes and building predictive models.</p>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"Guide Description Installation Install CLIFpy and set up your environment Quickstart Learn core workflows: loading data with ClifOrchestrator, configuration, validation, and key features"},{"location":"user-guide/#core-features","title":"Core Features","text":"Guide Description Data Validation Understand how CLIFpy validates data against CLIF schemas Outlier Handling Detect and remove physiologically implausible values with configurable ranges Encounter Stitching Link related hospital encounters to create continuous patient care timelines Wide Dataset Creation Create comprehensive time-series datasets by joining multiple CLIF tables with automatic pivoting Working with Timezones Best practices for handling timezone-aware datetime data"},{"location":"user-guide/#advanced-features","title":"Advanced Features","text":"Guide Description Medication Unit Conversion Convert continuous medication doses to standardized units SOFA Score Computation Compute Sequential Organ Failure Assessment scores for sepsis identification Respiratory Support Waterfall Visualize patient trajectories and treatment timelines with customizable plots Comorbidity Index Computation Calculate Charlson and Elixhauser comorbidity indices from diagnosis data MDRO Flag Calculation Calculate MDR, XDR, PDR, and DTR flags for multi-drug resistant organisms from susceptibility data"},{"location":"user-guide/#api-reference","title":"API Reference","text":"<p>Complete API reference: API Documentation</p>"},{"location":"user-guide/#clif-tables-reference","title":"CLIF Tables Reference","text":"<p>This section provides a reference for all CLIF tables available in CLIFpy. For detailed field definitions, see the CLIF Data Dictionary.</p> Table Data Dictionary API Reference Orchestrator Methods Table Methods patient \ud83d\udcd6 \u2699\ufe0f - - hospitalization \ud83d\udcd6 \u2699\ufe0f - - adt \ud83d\udcd6 \u2699\ufe0f - - labs \ud83d\udcd6 \u2699\ufe0f - - vitals \ud83d\udcd6 \u2699\ufe0f - - medication_admin_continuous \ud83d\udcd6 \u2699\ufe0f <code>convert_dose_units_for_continuous_meds()</code> - medication_admin_intermittent \ud83d\udcd6 \u2699\ufe0f <code>convert_dose_units_for_intermittent_meds()</code> - patient_assessments \ud83d\udcd6 \u2699\ufe0f - - respiratory_support \ud83d\udcd6 \u2699\ufe0f - - position \ud83d\udcd6 \u2699\ufe0f - - hospital_diagnosis \ud83d\udcd6 \u2699\ufe0f - - microbiology_culture \ud83d\udcd6 \u2699\ufe0f - - crrt_therapy \ud83d\udcd6 \u2699\ufe0f - - patient_procedures \ud83d\udcd6 \u2699\ufe0f - - microbiology_susceptibility \ud83d\udcd6 \u2699\ufe0f - - ecmo_mcs \ud83d\udcd6 \u2699\ufe0f - - microbiology_nonculture \ud83d\udcd6 \u2699\ufe0f - - code_status \ud83d\udcd6 \u2699\ufe0f - - <p>Legend:</p> <ul> <li> <p>\ud83d\udcd6 Links to CLIF Data Dictionary for field specifications</p> </li> <li> <p>\u2699\ufe0f Links to CLIFpy API Reference for class documentation</p> </li> <li> <p>Orchestrator Methods: Table-specific methods callable from <code>ClifOrchestrator</code> instance</p> </li> <li> <p>Table Methods: Table-specific class methods</p> </li> </ul>"},{"location":"user-guide/comorbidity-index/","title":"Comorbidity Index Computation","text":"<p>CLIFpy provides comprehensive functionality for calculating comorbidity indices from hospital diagnosis data. These indices are essential tools in clinical research for quantifying patient complexity and adjusting for disease burden in outcomes studies.</p>"},{"location":"user-guide/comorbidity-index/#overview","title":"Overview","text":"<p>Comorbidity indices are standardized scoring systems that summarize the burden of concurrent diseases in hospitalized patients. CLIFpy implements two widely-used indices:</p> <ul> <li>Charlson Comorbidity Index (CCI) - 17 conditions with differential weighting</li> <li>Elixhauser Comorbidity Index - 31 conditions with van Walraven weights</li> </ul> <p>Both indices use ICD-10-CM diagnosis codes and implement hierarchy logic to prevent double-counting of related conditions.</p> <p>!!! warning \"Important Usage Note\" Hospital diagnosis codes are finalized billing diagnosis codes for reimbursement. They are appropriate for calculating comorbidity scores but should not be used as input features for prediction models of inpatient events.</p>"},{"location":"user-guide/comorbidity-index/#charlson-comorbidity-index-cci","title":"Charlson Comorbidity Index (CCI)","text":"<p>The Charlson Comorbidity Index predicts 10-year mortality risk based on 17 comorbid conditions. CLIFpy implements the Quan et al. (2011) adaptation for ICD-10-CM codes.</p>"},{"location":"user-guide/comorbidity-index/#cci-conditions-and-weights","title":"CCI Conditions and Weights","text":"<p>Note: These weights are based on the Quan et al. (2011) updated version for ICD-10-CM codes, which differ from the original Charlson (1987) weights.</p> Condition Weight Example ICD-10-CM Codes Myocardial Infarction 0 I21, I22, I252 Congestive Heart Failure 2 I50, I099, I110 Peripheral Vascular Disease 0 I70, I71, I731 Cerebrovascular Disease 0 I60-I69, G45, G46 Dementia 2 F00-F03, G30, G311 Chronic Pulmonary Disease 1 J40-J47, J60-J67 Connective Tissue Disease 1 M05, M06, M32-M34 Peptic Ulcer Disease 0 K25-K28 Mild Liver Disease 2 K70-K77 Diabetes (uncomplicated) 0 E10-E14 Hemiplegia 2 G81, G82 Renal Disease 1 N18-N19, N052-N057 Diabetes with Complications 1 E10-E14 with complications Cancer 2 C00-C26, C30-C34, C37-C41 Moderate/Severe Liver Disease 4 I85, I864, I982, K704 Metastatic Solid Tumor 6 C77-C80 AIDS 4 B20-B22, B24"},{"location":"user-guide/comorbidity-index/#basic-cci-usage","title":"Basic CCI Usage","text":"<pre><code>from clifpy.tables.hospital_diagnosis import HospitalDiagnosis\nfrom clifpy.utils.comorbidity import calculate_cci\n\n# Load hospital diagnosis data\nhosp_dx = HospitalDiagnosis(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Eastern'\n)\n\n# Calculate CCI scores\ncci_results = calculate_cci(hosp_dx, hierarchy=True)\n\n# View results\nprint(cci_results.head())\nprint(f\"CCI score range: {cci_results['cci_score'].min()} - {cci_results['cci_score'].max()}\")\n</code></pre>"},{"location":"user-guide/comorbidity-index/#cci-output-format","title":"CCI Output Format","text":"<p>The function returns a pandas DataFrame with:</p> <ul> <li><code>hospitalization_id</code> (index) - Unique hospitalization identifier</li> <li>17 binary condition columns (0/1) - One for each CCI condition</li> <li><code>cci_score</code> - Weighted sum of present conditions</li> </ul> <pre><code># Example output structure\nhospitalization_id  myocardial_infarction  congestive_heart_failure  ...  cci_score\nHOSP_001           1                      0                         ...  3\nHOSP_002           0                      1                         ...  1\nHOSP_003           0                      0                         ...  0\n</code></pre>"},{"location":"user-guide/comorbidity-index/#elixhauser-comorbidity-index","title":"Elixhauser Comorbidity Index","text":"<p>The Elixhauser Comorbidity Index, originally developed by Elixhauser et al. (1998), captures a broader range of comorbidities compared to CCI. The original index identified 30 conditions as binary indicators (present/absent) without weighting. CLIFpy implements an enhanced version that combines:</p> <ul> <li>ICD-10-CM code mappings from Quan et al. (2005) adaptation, which expanded to 31 conditions</li> <li>van Walraven weighted scoring system (2009) for mortality prediction</li> </ul>"},{"location":"user-guide/comorbidity-index/#elixhauser-conditions-and-van-walraven-weights","title":"Elixhauser Conditions and van Walraven Weights","text":"<p>Note: These weights are based on van Walraven et al. (2009), which converted the original binary Elixhauser (1998) measures into a weighted point system for mortality prediction. ICD-10-CM code mappings are from Quan et al. (2005). Negative weights indicate protective effects.</p> <p>All 31 conditions with their van Walraven weights for mortality prediction:</p> Condition Weight Description Congestive Heart Failure 7 Heart failure, cardiomyopathy Cardiac Arrhythmias 5 Atrial fibrillation, other arrhythmias Valvular Disease -1 Heart valve disorders Pulmonary Circulation Disorders 4 Pulmonary embolism, pulmonary hypertension Peripheral Vascular Disorders 2 Peripheral artery disease Hypertension (uncomplicated) 0 Essential hypertension Hypertension (complicated) 0 Hypertensive complications Paralysis 7 Paraplegia, hemiplegia Other Neurological Disorders 6 Parkinson's, epilepsy, other Chronic Pulmonary Disease 3 COPD, asthma Diabetes (uncomplicated) 0 Diabetes without complications Diabetes (complicated) 0 Diabetes with complications Hypothyroidism 0 Thyroid disorders Renal Failure 5 Chronic kidney disease Liver Disease 11 Chronic liver disease Peptic Ulcer Disease (excluding bleeding) 0 Peptic ulcers without bleeding AIDS/HIV 0 HIV/AIDS Lymphoma 9 Lymphomas Metastatic Cancer 12 Metastatic solid tumors Solid Tumor (without metastasis) 4 Non-metastatic cancer Rheumatoid Arthritis/Collagen Vascular Disease 0 RA, lupus, connective tissue diseases Coagulopathy 3 Bleeding disorders Obesity -4 Obesity (protective weight) Weight Loss 6 Cachexia, malnutrition Fluid and Electrolyte Disorders 5 Electrolyte imbalances Blood Loss Anemia -2 Anemia from blood loss Deficiency Anemia -2 Iron, B12, folate deficiency Alcohol Abuse 0 Alcohol use disorder Drug Abuse -7 Substance use disorder (protective weight) Psychoses 0 Schizophrenia, psychotic disorders Depression -3 Major depressive disorder"},{"location":"user-guide/comorbidity-index/#basic-elixhauser-usage","title":"Basic Elixhauser Usage","text":"<pre><code>from clifpy.utils.comorbidity import calculate_elix\n\n# Calculate Elixhauser scores\nelix_results = calculate_elix(hosp_dx, hierarchy=True)\n\n# View results\nprint(elix_results.head())\nprint(f\"Elixhauser score range: {elix_results['elix_score'].min()} - {elix_results['elix_score'].max()}\")\n\n# Check condition prevalence\ncondition_prevalence = elix_results.iloc[:, :-1].mean().sort_values(ascending=False)\nprint(\"Most common conditions:\")\nprint(condition_prevalence.head(10))\n</code></pre>"},{"location":"user-guide/comorbidity-index/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/comorbidity-index/#working-with-different-input-types","title":"Working with Different Input Types","text":"<p>CLIFpy's comorbidity functions accept multiple input formats:</p> <pre><code>import pandas as pd\n\n# Option 1: HospitalDiagnosis object (recommended)\ncci_scores = calculate_cci(hosp_dx_table)\n\n# Option 2: pandas DataFrame\ndf = pd.DataFrame({\n    'hospitalization_id': ['H001', 'H001', 'H002'],\n    'diagnosis_code': ['I21.45', 'E10.1', 'K25.5'],\n    'diagnosis_code_format': ['ICD10CM', 'ICD10CM', 'ICD10CM']\n})\ncci_scores = calculate_cci(df)\n\n# Option 3: polars DataFrame\nimport polars as pl\npl_df = pl.from_pandas(df)\ncci_scores = calculate_cci(pl_df)\n</code></pre>"},{"location":"user-guide/comorbidity-index/#hierarchy-logic","title":"Hierarchy Logic","text":"<p>Both indices implement hierarchy logic (assign0) to prevent double-counting of related conditions:</p> <pre><code># With hierarchy (default, recommended)\ncci_with_hierarchy = calculate_cci(hosp_dx, hierarchy=True)\n\n# Without hierarchy (for research comparison)\ncci_without_hierarchy = calculate_cci(hosp_dx, hierarchy=False)\n\n# Compare the difference\nhierarchy_impact = cci_with_hierarchy['cci_score'] - cci_without_hierarchy['cci_score']\nprint(f\"Hierarchy reduces scores by: {hierarchy_impact.mean():.2f} points on average\")\n</code></pre>"},{"location":"user-guide/comorbidity-index/#hierarchy-rules","title":"Hierarchy Rules","text":"<p>CCI Hierarchies: - Severe liver disease supersedes mild liver disease - Diabetes with complications supersedes uncomplicated diabetes - Metastatic cancer supersedes local cancer</p> <p>Elixhauser Hierarchies: - Complicated hypertension supersedes uncomplicated hypertension - Complicated diabetes supersedes uncomplicated diabetes - Metastatic cancer supersedes solid tumor without metastasis</p>"},{"location":"user-guide/comorbidity-index/#data-requirements","title":"Data Requirements","text":""},{"location":"user-guide/comorbidity-index/#input-data-format","title":"Input Data Format","text":"<p>Your hospital diagnosis data must include these columns:</p> <pre><code>required_columns = [\n    'hospitalization_id',      # Unique hospitalization identifier\n    'diagnosis_code',          # ICD diagnosis code (e.g., \"I21.45\")\n    'diagnosis_code_format'    # Code format (must be \"ICD10CM\")\n]\n</code></pre>"},{"location":"user-guide/comorbidity-index/#icd-code-processing","title":"ICD Code Processing","text":"<ul> <li>Decimal handling: Codes like \"I21.45\" are automatically truncated to \"I21\" for mapping</li> <li>Format filtering: Only ICD10CM codes are processed; other formats are ignored</li> <li>Prefix matching: Uses prefix matching (e.g., \"I21\" matches all I21.x codes)</li> </ul>"},{"location":"user-guide/comorbidity-index/#clinical-interpretation","title":"Clinical Interpretation","text":""},{"location":"user-guide/comorbidity-index/#cci-score-interpretation","title":"CCI Score Interpretation","text":"Score Range Mortality Risk Typical Patient Population 0 Low Healthy patients, minor procedures 1-2 Moderate Single comorbidity, routine surgery 3-4 High Multiple comorbidities, complex cases \u22655 Very High Severely ill, high-risk procedures"},{"location":"user-guide/comorbidity-index/#elixhauser-score-interpretation","title":"Elixhauser Score Interpretation","text":"<p>Elixhauser scores can be negative due to protective conditions (negative weights). Typical ranges:</p> <ul> <li>\u22640: Low complexity, some protective factors</li> <li>1-4: Moderate complexity</li> <li>5-15: High complexity</li> <li>&gt;15: Very high complexity, multiple severe conditions</li> </ul>"},{"location":"user-guide/comorbidity-index/#research-applications","title":"Research Applications","text":"<pre><code># Mortality risk stratification\ndef risk_category(score, index_type='cci'):\n    if index_type == 'cci':\n        if score == 0:\n            return 'Low'\n        elif score &lt;= 2:\n            return 'Moderate'\n        elif score &lt;= 4:\n            return 'High'\n        else:\n            return 'Very High'\n    # Add Elixhauser categorization as needed\n\n# Apply risk stratification\nresults['risk_category'] = results['cci_score'].apply(\n    lambda x: risk_category(x, 'cci')\n)\n\n# Analyze by risk category\nrisk_summary = results.groupby('risk_category').agg({\n    'cci_score': ['count', 'mean', 'std']\n})\n</code></pre>"},{"location":"user-guide/comorbidity-index/#configuration-and-customization","title":"Configuration and Customization","text":""},{"location":"user-guide/comorbidity-index/#yaml-configuration-files","title":"YAML Configuration Files","text":"<p>Comorbidity mappings are stored in YAML files:</p> <ul> <li><code>clifpy/data/comorbidity/cci.yaml</code> - CCI mappings and weights</li> <li><code>clifpy/data/comorbidity/elixhauser.yaml</code> - Elixhauser mappings and weights</li> </ul>"},{"location":"user-guide/comorbidity-index/#configuration-structure","title":"Configuration Structure","text":"<pre><code># Example CCI configuration structure\nname: \"Charlson Comorbidity Index\"\nversion: \"quan\"\nsupported_formats:\n  - ICD10CM\n\ndiagnosis_code_mappings:\n  ICD10CM:\n    myocardial_infarction:\n      codes: [\"I21\", \"I22\", \"I252\"]\n      description: \"History of definite or probable MI\"\n    # ... other conditions\n\nweights:\n  myocardial_infarction: 1\n  # ... other weights\n\nhierarchies:\n  - higher: \"diabetes_with_complications\"\n    lower: \"diabetes_uncomplicated\"\n  # ... other hierarchies\n</code></pre>"},{"location":"user-guide/comorbidity-index/#custom-mappings","title":"Custom Mappings","text":"<p>For research requiring custom mappings, you can modify the YAML files or create custom versions. Ensure proper validation and testing when using custom configurations.</p>"},{"location":"user-guide/comorbidity-index/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/comorbidity-index/#common-issues","title":"Common Issues","text":"<p>No ICD10CM codes found:</p> <pre><code># Check your data format\nprint(hosp_dx.df['diagnosis_code_format'].value_counts())\n# Ensure codes are marked as 'ICD10CM'\n</code></pre> <p>Unexpected zero scores:</p> <pre><code># Check for missing hospitalization_id\nmissing_ids = hosp_dx.df['hospitalization_id'].isna().sum()\nprint(f\"Missing hospitalization IDs: {missing_ids}\")\n\n# Verify diagnosis codes are properly formatted\nprint(hosp_dx.df['diagnosis_code'].head())\n</code></pre> <p>Memory errors with large datasets:</p> <pre><code># Monitor memory usage\nimport psutil\nprint(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Consider processing in smaller chunks\n</code></pre>"},{"location":"user-guide/comorbidity-index/#validation","title":"Validation","text":"<p>Always validate your results:</p> <pre><code># Basic validation checks\nassert not cci_results['cci_score'].isna().any(), \"CCI scores contain NaN\"\nassert (cci_results['cci_score'] &gt;= 0).all(), \"CCI scores should be non-negative\"\nassert cci_results['hospitalization_id'].is_unique, \"Hospitalization IDs should be unique\"\n\n# Clinical validation\nmax_score = cci_results['cci_score'].max()\nprint(f\"Maximum CCI score: {max_score}\")\nif max_score &gt; 20:\n    print(\"Warning: Unusually high CCI scores detected\")\n</code></pre>"},{"location":"user-guide/comorbidity-index/#references","title":"References","text":"<ul> <li>Charlson ME, Pompei P, Ales KL, MacKenzie CR. A new method of classifying prognostic comorbidity in longitudinal studies: development and validation. J Chronic Dis. 1987;40(5):373-383.</li> <li>Elixhauser A, Steiner C, Harris DR, Coffey RM. Comorbidity measures for use with administrative data. Med Care. 1998;36(1):8-27.</li> <li>Quan H, Sundararajan V, Halfon P, et al. Coding algorithms for defining comorbidities in ICD-9-CM and ICD-10 administrative data. Med Care. 2005;43(11):1130-1139.</li> <li>van Walraven C, Austin PC, Jennings A, Quan H, Forster AJ. A modification of the Elixhauser comorbidity measures into a point system for hospital death using administrative data. Med Care. 2009;47(6):626-633.</li> <li>Quan H, Li B, Couris CM, Fushimi K, Graham P, Hider P, Januel JM, Sundararajan V. Updating and Validating the Charlson Comorbidity Index and Score for Risk Adjustment in Hospital Discharge Abstracts Using Data From 6 Countries. Am J Epidemiol. 2011;173(6):676-682. doi:10.1093/aje/kwq433.</li> </ul>"},{"location":"user-guide/encounter-stitching/","title":"Encounter Stitching","text":"<p>The encounter stitching functionality identifies and groups hospitalizations that occur within a specified time window of each other, treating them as a single continuous encounter. This is particularly useful for handling cases where patients are discharged and quickly readmitted, such as transfers between the emergency department and inpatient units.</p>"},{"location":"user-guide/encounter-stitching/#overview","title":"Overview","text":"<p>In clinical data, what appears as separate hospitalizations may actually represent a single continuous episode of care. Common scenarios include:</p> <ul> <li>ED to inpatient transfers - Patient admitted through ED, then formally admitted to hospital</li> <li>Inter-facility transfers - Patient moved between hospitals within a health system</li> <li>Brief discharges - Patient discharged and readmitted within hours (e.g., for procedures)</li> <li>Administrative separations - Billing or administrative reasons create multiple records</li> </ul> <p>The encounter stitching algorithm links these related hospitalizations using a configurable time window (default: 6 hours) between discharge and subsequent admission.</p>"},{"location":"user-guide/encounter-stitching/#how-it-works","title":"How It Works","text":"<p>The stitching algorithm:</p> <ol> <li>Sorts hospitalizations by patient and admission time</li> <li>Calculates gaps between discharge and next admission for each patient</li> <li>Links encounters when the gap is less than the specified time window</li> <li>Assigns encounter blocks - a unique identifier grouping linked hospitalizations</li> <li>Updates tables in-place - adds <code>encounter_block</code> column to both hospitalization and ADT tables</li> </ol>"},{"location":"user-guide/encounter-stitching/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/encounter-stitching/#quick-start-with-automatic-stitching","title":"Quick Start with Automatic Stitching","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize orchestrator with stitching enabled\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,          # Enable automatic stitching\n    stitch_time_interval=6          # 6-hour window (default)\n)\n\n# Load tables - stitching happens automatically\nclif.initialize(['hospitalization', 'adt'])\n\n# Access the encounter mapping\nmapping = clif.get_encounter_mapping()\nprint(f\"Created {mapping['encounter_block'].nunique()} encounter blocks\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#custom-time-windows","title":"Custom Time Windows","text":"<pre><code># Use a 12-hour window for linking encounters\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,\n    stitch_time_interval=12  # 12-hour window\n)\n\n# Use a 2-hour window for stricter linking\nclif_strict = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,\n    stitch_time_interval=2   # 2-hour window\n)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#direct-function-usage","title":"Direct Function Usage","text":"<p>You can also use the stitching function directly without the orchestrator:</p> <pre><code>from clifpy.utils.stitching_encounters import stitch_encounters\n\n# Load your dataframes\nhospitalization_df = pd.read_parquet('hospitalization.parquet')\nadt_df = pd.read_parquet('adt.parquet')\n\n# Perform stitching\nhosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n    hospitalization=hospitalization_df,\n    adt=adt_df,\n    time_interval=12  # 12-hour window\n)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#parameters","title":"Parameters","text":""},{"location":"user-guide/encounter-stitching/#cliforchestrator-parameters","title":"ClifOrchestrator Parameters","text":"Parameter Type Default Description <code>stitch_encounter</code> bool False Enable automatic encounter stitching during initialization <code>stitch_time_interval</code> int 6 Hours between discharge and next admission to consider encounters linked"},{"location":"user-guide/encounter-stitching/#direct-function-parameters","title":"Direct Function Parameters","text":"Parameter Type Default Description <code>hospitalization</code> pd.DataFrame Required Hospitalization table with required columns <code>adt</code> pd.DataFrame Required ADT table with required columns <code>time_interval</code> int 6 Hours between discharge and next admission to consider encounters linked"},{"location":"user-guide/encounter-stitching/#required-data-columns","title":"Required Data Columns","text":""},{"location":"user-guide/encounter-stitching/#hospitalization-table","title":"Hospitalization Table","text":"<ul> <li><code>patient_id</code></li> <li><code>hospitalization_id</code></li> <li><code>admission_dttm</code></li> <li><code>discharge_dttm</code></li> <li><code>age_at_admission</code></li> <li><code>admission_type_category</code></li> <li><code>discharge_category</code></li> </ul>"},{"location":"user-guide/encounter-stitching/#adt-table","title":"ADT Table","text":"<ul> <li><code>hospitalization_id</code></li> <li><code>in_dttm</code></li> <li><code>out_dttm</code></li> <li><code>location_category</code></li> <li><code>hospital_id</code></li> </ul>"},{"location":"user-guide/encounter-stitching/#output","title":"Output","text":"<p>When stitching is enabled, the process:</p> <ol> <li>Updates hospitalization table - Adds <code>encounter_block</code> column</li> <li>Updates ADT table - Adds <code>encounter_block</code> column</li> <li>Creates encounter mapping - Available via <code>clif.get_encounter_mapping()</code>:    - <code>hospitalization_id</code>: Original hospitalization identifier    - <code>encounter_block</code>: Assigned encounter block number</li> </ol>"},{"location":"user-guide/encounter-stitching/#understanding-encounter-blocks","title":"Understanding Encounter Blocks","text":"<p>Each encounter block represents a continuous episode of care:</p> <pre><code># Access the mapping after initialization\nmapping = clif.get_encounter_mapping()\n\n# Find multi-hospitalization encounters\nmulti_hosp = mapping.groupby('encounter_block').size()\nmulti_hosp_encounters = multi_hosp[multi_hosp &gt; 1]\n\nprint(f\"Encounters with multiple hospitalizations: {len(multi_hosp_encounters)}\")\n\n# Get details for a specific encounter block\nblock_1_hosps = mapping[mapping['encounter_block'] == 1]\nprint(f\"Hospitalizations in encounter block 1: {block_1_hosps['hospitalization_id'].tolist()}\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#practical-examples","title":"Practical Examples","text":""},{"location":"user-guide/encounter-stitching/#calculate-true-length-of-stay","title":"Calculate True Length of Stay","text":"<p>When encounters are stitched, you can calculate the true length of stay across linked hospitalizations:</p> <pre><code># Access stitched hospitalization data\nstitched_df = clif.hospitalization.df\n\n# Calculate encounter-level statistics\nencounter_stats = stitched_df.groupby('encounter_block').agg({\n    'admission_dttm': 'min',  # First admission\n    'discharge_dttm': 'max',  # Last discharge\n    'hospitalization_id': 'count',  # Number of linked hospitalizations\n    'patient_id': 'first'\n})\n\n# Calculate total length of stay\nencounter_stats['total_los_days'] = (\n    (encounter_stats['discharge_dttm'] - encounter_stats['admission_dttm'])\n    .dt.total_seconds() / 86400\n)\n\nprint(encounter_stats[['patient_id', 'hospitalization_id', 'total_los_days']].head())\n</code></pre>"},{"location":"user-guide/encounter-stitching/#analyze-icu-stays-across-encounters","title":"Analyze ICU Stays Across Encounters","text":"<pre><code># Access stitched ADT data\nadt_stitched_df = clif.adt.df\n\n# Find ICU stays by encounter\nicu_by_encounter = adt_stitched_df[\n    adt_stitched_df['location_category'] == 'icu'\n].groupby('encounter_block').agg({\n    'in_dttm': 'min',\n    'out_dttm': 'max',\n    'hospitalization_id': 'nunique'\n})\n\nprint(\"ICU stays by encounter block:\")\nprint(icu_by_encounter.head())\n</code></pre>"},{"location":"user-guide/encounter-stitching/#filter-data-by-encounter-properties","title":"Filter Data by Encounter Properties","text":"<pre><code># Find encounters with ED to inpatient transfers\ned_admits = clif.adt.df[\n    clif.adt.df['location_category'] == 'ed'\n]['encounter_block'].unique()\n\ninpatient_admits = clif.adt.df[\n    clif.adt.df['location_category'].isin(['icu', 'ward'])\n]['encounter_block'].unique()\n\ned_to_inpatient = set(ed_admits) &amp; set(inpatient_admits)\nprint(f\"Encounters with ED to inpatient transfer: {len(ed_to_inpatient)}\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#compare-different-time-windows","title":"Compare Different Time Windows","text":"<pre><code># Test effect of different time windows\nwindows = [3, 6, 12, 24]\nresults = []\n\nfor window in windows:\n    clif_test = ClifOrchestrator(\n        data_directory='/path/to/data',\n        filetype='parquet',\n        timezone='UTC',\n        stitch_encounter=True,\n        stitch_time_interval=window\n    )\n    clif_test.initialize(['hospitalization', 'adt'])\n\n    mapping = clif_test.get_encounter_mapping()\n    results.append({\n        'window_hours': window,\n        'total_encounters': mapping['encounter_block'].nunique(),\n        'multi_hosp_encounters': (mapping.groupby('encounter_block').size() &gt; 1).sum()\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"user-guide/encounter-stitching/#wide-dataset-creation","title":"Wide Dataset Creation","text":"<p>Stitched encounters are automatically used when creating wide datasets:</p> <pre><code># Initialize with stitching\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True\n)\n\n# Load tables (stitching happens automatically)\nclif.initialize(['hospitalization', 'adt', 'labs', 'vitals'])\n\n# Create wide dataset using stitched encounters\nwide_df = clif.create_wide_dataset(\n    start_time='admission_dttm',\n    end_time='discharge_dttm',\n    time_col='charttime'\n)\n</code></pre>"},{"location":"user-guide/encounter-stitching/#validation","title":"Validation","text":"<p>The stitched tables maintain compatibility with validation methods:</p> <pre><code># Validate all loaded tables (including stitched ones)\nvalidation_results = clif.validate_all()\n\n# Check specific tables\nclif.hospitalization.validate()\nclif.adt.validate()\n</code></pre>"},{"location":"user-guide/encounter-stitching/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose appropriate time windows:    - 2-4 hours: Strict linking for direct transfers only    - 6 hours (default): Balances capturing related encounters while avoiding over-grouping    - 12-24 hours: Liberal definition, captures day surgery readmissions</p> </li> <li> <p>Validate stitching results:    <pre><code># Check for suspiciously large encounter blocks\nmapping = clif.get_encounter_mapping()\nhosp_counts = mapping.groupby('encounter_block').size()\nsuspicious = hosp_counts[hosp_counts &gt; 5]\nif len(suspicious) &gt; 0:\n    print(f\"Review encounters with &gt;5 hospitalizations: {suspicious.index.tolist()}\")\n</code></pre></p> </li> <li> <p>Consider your analysis goals:    - Outcome studies: Use stitched encounters to avoid counting transfers as readmissions    - Resource utilization: May want to keep encounters separate for accurate billing    - Quality metrics: Check if measure specifications require episode-based analysis</p> </li> <li> <p>Document your choices:    <pre><code># Save stitching parameters for reproducibility\nif clif.encounter_mapping is not None:\n    stitching_info = {\n        'time_interval_hours': clif.stitch_time_interval,\n        'timestamp': pd.Timestamp.now(),\n        'num_encounters_created': clif.encounter_mapping['encounter_block'].nunique(),\n        'num_multi_hosp_encounters': (\n            clif.encounter_mapping.groupby('encounter_block').size().gt(1).sum()\n        )\n    }\n    # Save to file or include in analysis metadata\n</code></pre></p> </li> </ol>"},{"location":"user-guide/encounter-stitching/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/encounter-stitching/#algorithm-implementation","title":"Algorithm Implementation","text":"<p>The stitching algorithm:</p> <ol> <li>Filters required columns from hospitalization and ADT tables</li> <li>Joins hospitalization and ADT data</li> <li>Sorts by patient_id and admission_dttm</li> <li>Calculates hours between discharge and next admission</li> <li>Creates linked flag for gaps &lt; time_interval</li> <li>Iteratively propagates encounter_block IDs through linked chains</li> <li>Updates original dataframes with encounter_block column</li> </ol>"},{"location":"user-guide/encounter-stitching/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Stitching is performed in-memory using pandas operations</li> <li>Performance scales linearly with number of hospitalizations</li> <li>For datasets with &gt;1M hospitalizations, ensure adequate RAM (8GB+ recommended)</li> <li>Processing time is typically seconds to minutes depending on data size</li> </ul>"},{"location":"user-guide/encounter-stitching/#error-handling","title":"Error Handling","text":"<p>The orchestrator handles common issues:</p> <ul> <li>Missing tables: Warns if hospitalization or ADT tables are not loaded</li> <li>Missing columns: Raises ValueError with specific missing columns listed</li> <li>Processing errors: Catches exceptions and reports them without failing initialization</li> </ul>"},{"location":"user-guide/encounter-stitching/#limitations","title":"Limitations","text":"<ul> <li>Currently only links hospitalizations for the same patient</li> <li>Does not consider clinical criteria (purely time-based)</li> <li>Requires both hospitalization and ADT tables to be present</li> <li>Does not link across different hospital systems (requires same patient_id)</li> </ul>"},{"location":"user-guide/encounter-stitching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/encounter-stitching/#common-issues","title":"Common Issues","text":"<p>Issue: \"Encounter stitching requires both hospitalization and ADT tables to be loaded\" - Solution: Include both 'hospitalization' and 'adt' in your <code>initialize()</code> call</p> <p>Issue: \"Missing required columns in hospitalization DataFrame\" - Solution: Ensure your data contains all required columns listed above - Check: Use <code>clif.hospitalization.df.columns</code> to see available columns</p> <p>Issue: No encounters are being stitched despite close admissions - Check: Verify datetime columns are properly parsed and in the same timezone - Check: Ensure discharge_dttm is not null for hospitalizations you expect to link - Try: Increase the time window to see if encounters get linked</p>"},{"location":"user-guide/encounter-stitching/#debugging","title":"Debugging","text":"<pre><code># Enable detailed output during initialization\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC',\n    stitch_encounter=True,\n    stitch_time_interval=6\n)\n\n# Check if stitching was attempted\nclif.initialize(['hospitalization', 'adt'])\n\n# Verify encounter_block was added\nprint(\"Hospitalization columns:\", clif.hospitalization.df.columns.tolist())\nprint(\"Has encounter_block:\", 'encounter_block' in clif.hospitalization.df.columns)\n\n# Check mapping\nif clif.encounter_mapping is not None:\n    print(f\"Mapping shape: {clif.encounter_mapping.shape}\")\nelse:\n    print(\"No encounter mapping created\")\n</code></pre>"},{"location":"user-guide/encounter-stitching/#see-also","title":"See Also","text":"<ul> <li>ClifOrchestrator - Main interface for CLIF data operations</li> <li>Hospitalization Table - Structure of hospitalization data</li> <li>ADT Table - Structure of ADT data</li> <li>Wide Dataset Creation - Creating analysis-ready datasets</li> <li>Examples Notebook - Interactive examples</li> </ul>"},{"location":"user-guide/installation/","title":"Installation","text":""},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<ul> <li> <p>Python 3.9 or higher</p> </li> <li> <p>pip or uv (package installer)</p> </li> </ul>"},{"location":"user-guide/installation/#installation-for-users","title":"Installation for Users","text":"<p>Using pip:</p> <pre><code>pip install clifpy\n</code></pre> <p>Using uv:</p> <pre><code>uv pip install clifpy\n</code></pre> <p>Optionally, if you want to build the documentation locally:</p> <pre><code>pip install clifpy[docs]\n# or with uv\nuv pip install clifpy[docs]\n</code></pre>"},{"location":"user-guide/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that CLIFpy is properly installed:</p> <pre><code>import clifpy\nprint(clifpy.__version__)\n</code></pre> <p>You should see the version number (e.g., <code>0.0.8</code>).</p>"},{"location":"user-guide/installation/#installation-for-contributors","title":"Installation for Contributors","text":"<p>If you want to contribute to CLIFpy or test the latest development version:</p> <p><pre><code># Clone the repository\ngit clone https://github.com/Common-Longitudinal-ICU-data-Format/CLIFpy.git\ncd CLIFpy\n\n# Create virtual environment (if using uv, it handles this automatically)\nuv venv  # or: python -m venv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate (or venv\\Scripts\\activate)\n\n# Install in editable mode with dev dependencies\nuv pip install -e .\n# or with pip\npip install -e .\n</code></pre> For testing and documentation development, install the dev dependency group:</p> <pre><code># Using uv (reads from pyproject.toml [dependency-groups])\nuv pip install -e . --group dev\n\n# Or manually install dev dependencies\npip install marimo mkdocs mkdocs-material mkdocstrings[python] nbformat pytest-doctestplus sybil\n</code></pre> <p>Dev dependencies include:</p> <ul> <li> <p>sybil: For markdown documentation testing</p> </li> <li> <p>mkdocs + mkdocs-material: For building documentation</p> </li> <li> <p>mkdocstrings: For API reference generation</p> </li> <li> <p>pytest-doctestplus: For enhanced docstring testing</p> </li> <li> <p>marimo: For interactive notebook development</p> </li> </ul>"},{"location":"user-guide/installation/#platform-support","title":"Platform Support","text":"<p>CLIFpy is tested on:</p> <ul> <li> <p>Linux (Ubuntu 20.04+)</p> </li> <li> <p>macOS (10.15+)</p> </li> <li> <p>Windows (10+)</p> </li> </ul>"},{"location":"user-guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using the correct Python environment:</p> <pre><code>which python\npython --version\n</code></pre>"},{"location":"user-guide/installation/#permission-errors","title":"Permission Errors","text":"<p>On some systems, you may need to use <code>pip install --user</code>:</p> <pre><code>pip install --user clifpy\n</code></pre>"},{"location":"user-guide/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency conflicts, use a fresh virtual environment:</p> <pre><code># With uv\nuv venv --python 3.9\nsource .venv/bin/activate\n\n# With venv\npython -m venv fresh-env\nsource fresh-env/bin/activate\n\n# Then install\nuv pip install clifpy  # or: pip install clifpy\n</code></pre>"},{"location":"user-guide/mdro-flags/","title":"MDRO Flag Calculation","text":"<p>The MDRO (Multi-Drug Resistant Organism) flag calculation utility identifies and classifies organisms based on antimicrobial susceptibility testing results. It automatically calculates MDR, XDR, PDR, and DTR flags according to standardized clinical criteria.</p>"},{"location":"user-guide/mdro-flags/#overview","title":"Overview","text":"<p>Multi-drug resistant organisms pose significant challenges in healthcare settings. Proper identification and classification of resistance patterns is crucial for:</p> <ul> <li>Infection control - Implementing appropriate isolation and prevention measures</li> <li>Treatment decisions - Guiding antibiotic selection and escalation</li> <li>Surveillance - Tracking resistance trends over time</li> <li>Research - Studying outcomes and risk factors for resistant infections</li> </ul> <p>The MDRO flag calculator automates the classification of organisms into standardized resistance categories based on antimicrobial susceptibility testing (AST) results.</p>"},{"location":"user-guide/mdro-flags/#supported-classifications","title":"Supported Classifications","text":"<ul> <li>MDR (Multi-Drug Resistant) - Non-susceptible to \u22651 agent in \u22653 antimicrobial categories</li> <li>XDR (Extensively Drug Resistant) - Non-susceptible to \u22651 agent in all but \u22642 categories</li> <li>PDR (Pandrug Resistant) - Non-susceptible to all antimicrobial agents tested</li> <li>DTR (Difficult to Treat Resistance) - Non-susceptible to specific key antimicrobials (organism-specific)</li> </ul>"},{"location":"user-guide/mdro-flags/#currently-supported-organisms","title":"Currently Supported Organisms","text":"<ul> <li>Pseudomonas aeruginosa - Common healthcare-associated pathogen with 8 antimicrobial groups</li> </ul> <p>The system is designed to be extensible - additional organisms can be easily added through the YAML configuration file.</p>"},{"location":"user-guide/mdro-flags/#how-it-works","title":"How It Works","text":"<p>The MDRO flag calculation follows a systematic workflow:</p> <pre><code>flowchart TD\n    A[Load Culture Table] --&gt; B[Filter by Organism Name]\n    B --&gt; C{Apply Filters?}\n    C --&gt;|Cohort| D[Filter by Date Range]\n    C --&gt;|Hospitalization IDs| E[Filter by IDs]\n    C --&gt;|None| F[Use All Matching Organisms]\n    D --&gt; G[LEFT JOIN with Susceptibility]\n    E --&gt; G\n    F --&gt; G\n    G --&gt; H[Map Antimicrobials to Groups]\n    H --&gt; I[Identify Resistant Results]\n    I --&gt; J[Calculate MDR/XDR/PDR/DTR Flags]\n    J --&gt; K[Create Antimicrobial Columns]\n    K --&gt; L[Create Group Columns]\n    L --&gt; M[Merge into Wide Format]\n    M --&gt; N[Return Results DataFrame]</code></pre>"},{"location":"user-guide/mdro-flags/#key-processing-steps","title":"Key Processing Steps","text":"<ol> <li>Culture Filtering - Filters the culture table to the specified organism (e.g., Pseudomonas aeruginosa)</li> <li>Optional Filtering - Applies cohort date ranges or specific hospitalization IDs if provided</li> <li>Susceptibility Join - LEFT JOIN preserves all organism cultures, even without susceptibility data</li> <li>Group Mapping - Maps individual antimicrobials (e.g., ciprofloxacin) to groups (e.g., fluoroquinolones)</li> <li>Resistance Classification - Identifies resistant results (both 'intermediate' and 'non_susceptible')</li> <li>Flag Calculation - Applies organism-specific criteria to determine MDR/XDR/PDR/DTR status</li> <li>Wide Format Creation - Pivots results to include individual antimicrobial and group columns for verification</li> </ol>"},{"location":"user-guide/mdro-flags/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/mdro-flags/#quick-start","title":"Quick Start","text":"<pre><code>from clifpy.tables import MicrobiologyCulture, MicrobiologySusceptibility\nfrom clifpy.utils.mdro_flags import calculate_mdro_flags\n\n# Load your microbiology data\nculture = MicrobiologyCulture(\n    data_directory='/path/to/data',\n    filetype='parquet'\n)\n\nsusceptibility = MicrobiologySusceptibility(\n    data_directory='/path/to/data',\n    filetype='parquet'\n)\n\n# Calculate MDRO flags for all P. aeruginosa\nmdro_flags = calculate_mdro_flags(\n    culture=culture,\n    susceptibility=susceptibility,\n    organism_name='pseudomonas_aeruginosa'\n)\n\n# View results\nprint(mdro_flags.head())\n</code></pre>"},{"location":"user-guide/mdro-flags/#with-specific-hospitalizations","title":"With Specific Hospitalizations","text":"<pre><code># Calculate flags for specific encounters only\ntarget_hospitalizations = ['H001', 'H002', 'H003']\n\nmdro_flags = calculate_mdro_flags(\n    culture=culture,\n    susceptibility=susceptibility,\n    organism_name='pseudomonas_aeruginosa',\n    hospitalization_ids=target_hospitalizations\n)\n</code></pre>"},{"location":"user-guide/mdro-flags/#with-cohort-date-filtering","title":"With Cohort Date Filtering","text":"<pre><code>import pandas as pd\n\n# Define a cohort with date ranges\ncohort_df = pd.DataFrame({\n    'hospitalization_id': ['H001', 'H002', 'H003'],\n    'start_dttm': ['2023-01-01', '2023-01-15', '2023-02-01'],\n    'end_dttm': ['2023-01-10', '2023-01-25', '2023-02-15']\n})\n\n# Only include organism cultures within cohort time windows\nmdro_flags = calculate_mdro_flags(\n    culture=culture,\n    susceptibility=susceptibility,\n    organism_name='pseudomonas_aeruginosa',\n    cohort=cohort_df\n)\n</code></pre>"},{"location":"user-guide/mdro-flags/#parameters","title":"Parameters","text":""},{"location":"user-guide/mdro-flags/#function-parameters","title":"Function Parameters","text":"Parameter Type Required Default Description <code>culture</code> MicrobiologyCulture Yes - Culture table object containing organism data <code>susceptibility</code> MicrobiologySusceptibility Yes - Susceptibility table object with AST results <code>organism_name</code> str Yes - Organism to classify (must match <code>organism_category</code> value) <code>cohort</code> pd.DataFrame No None Cohort with hospitalization_id, start_dttm, end_dttm for date filtering <code>hospitalization_ids</code> List[str] No None Specific hospitalization IDs to include <code>config_path</code> str No None Path to custom MDRO configuration YAML (uses default if not specified)"},{"location":"user-guide/mdro-flags/#required-data-columns","title":"Required Data Columns","text":"<p>Culture Table: - <code>patient_id</code> - <code>hospitalization_id</code> - <code>organism_id</code> - Unique identifier for each organism culture - <code>organism_category</code> - Standardized organism name (e.g., 'pseudomonas_aeruginosa') - <code>result_dttm</code> - Result datetime (for cohort filtering)</p> <p>Susceptibility Table: - <code>organism_id</code> - Links to culture table - <code>antimicrobial_category</code> - Standardized antimicrobial name (e.g., 'ciprofloxacin') - <code>susceptibility_category</code> - Result: 'susceptible', 'intermediate', 'non_susceptible', or 'NA'</p>"},{"location":"user-guide/mdro-flags/#understanding-the-output","title":"Understanding the Output","text":"<p>The function returns a wide-format DataFrame designed for easy verification and analysis.</p>"},{"location":"user-guide/mdro-flags/#output-structure","title":"Output Structure","text":"<pre><code># Example output columns\nhospitalization_id | organism_id | amikacin_agent | ciprofloxacin_agent | ... | aminoglycosides_group | fluoroquinolones_group | ... | MDR | XDR\n</code></pre>"},{"location":"user-guide/mdro-flags/#column-types","title":"Column Types","text":"<p>1. Identifier Columns - <code>hospitalization_id</code> - Hospital encounter identifier - <code>organism_id</code> - Unique organism culture identifier</p> <p>2. Individual Antimicrobial Columns - One column per antimicrobial agent tested (with <code>_agent</code> suffix) - Values: 'susceptible', 'intermediate', 'non_susceptible', 'NA', or None (not tested) - Example columns: <code>amikacin_agent</code>, <code>ciprofloxacin_agent</code>, <code>ceftazidime_agent</code>, <code>meropenem_agent</code></p> <p>3. Antimicrobial Group Columns - One column per antimicrobial group (with <code>_group</code> suffix) - Binary values: 1 (resistant to \u22651 agent in group), 0 (susceptible to all tested agents) - Allows quick identification of which drug classes are affected - Example columns: <code>aminoglycosides_group</code>, <code>carbapenems_group</code>, <code>fluoroquinolones_group</code></p> <p>4. MDRO Flag Columns - <code>MDR</code> - Multi-Drug Resistant flag (0/1) - <code>XDR</code> - Extensively Drug Resistant flag (0/1) - <code>PDR</code> - Pandrug Resistant flag (0/1) - <code>DTR</code> - Difficult to Treat Resistance flag (0/1)</p>"},{"location":"user-guide/mdro-flags/#interpreting-results","title":"Interpreting Results","text":"<pre><code># Load results\nmdro_flags = calculate_mdro_flags(culture, susceptibility, 'pseudomonas_aeruginosa')\n\n# Count resistance patterns\nmdr_count = mdro_flags['MDR'].sum()\nxdr_count = mdro_flags['XDR'].sum()\npdr_count = mdro_flags['PDR'].sum()\ndtr_count = mdro_flags['DTR'].sum()\n\nprint(f\"MDR organisms: {mdr_count}\")\nprint(f\"XDR organisms: {xdr_count}\")\nprint(f\"PDR organisms: {pdr_count}\")\nprint(f\"DTR organisms: {dtr_count}\")\n\n# View a specific organism's results\norganism_detail = mdro_flags[mdro_flags['organism_id'] == 'ORG123']\nprint(organism_detail)\n</code></pre>"},{"location":"user-guide/mdro-flags/#handling-duplicates","title":"Handling Duplicates","text":"<p>If an organism has multiple tests for the same antimicrobial, the most resistant result is automatically selected:</p> <ul> <li>Priority: <code>non_susceptible</code> &gt; <code>intermediate</code> &gt; <code>susceptible</code> &gt; <code>NA</code></li> </ul>"},{"location":"user-guide/mdro-flags/#practical-examples","title":"Practical Examples","text":""},{"location":"user-guide/mdro-flags/#example-1-surveillance-report","title":"Example 1: Surveillance Report","text":"<pre><code>from clifpy.tables import MicrobiologyCulture, MicrobiologySusceptibility\nfrom clifpy.utils.mdro_flags import calculate_mdro_flags\nimport pandas as pd\n\n# Load data\nculture = MicrobiologyCulture(data_directory='./data', filetype='parquet')\nsusceptibility = MicrobiologySusceptibility(data_directory='./data', filetype='parquet')\n\n# Calculate flags\nmdro = calculate_mdro_flags(culture, susceptibility, 'pseudomonas_aeruginosa')\n\n# Create surveillance summary\nsummary = pd.DataFrame({\n    'Total Organisms': [len(mdro)],\n    'MDR (%)': [f\"{mdro['MDR'].sum()} ({mdro['MDR'].mean()*100:.1f}%)\"],\n    'XDR (%)': [f\"{mdro['XDR'].sum()} ({mdro['XDR'].mean()*100:.1f}%)\"],\n    'PDR (%)': [f\"{mdro['PDR'].sum()} ({mdro['PDR'].mean()*100:.1f}%)\"],\n    'DTR (%)': [f\"{mdro['DTR'].sum()} ({mdro['DTR'].mean()*100:.1f}%)\"]\n})\n\nprint(\"P. aeruginosa Resistance Summary\")\nprint(summary.to_string(index=False))\n</code></pre>"},{"location":"user-guide/mdro-flags/#example-2-identify-high-risk-patients","title":"Example 2: Identify High-Risk Patients","text":"<pre><code># Find hospitalizations with XDR or PDR organisms\nhigh_risk = mdro[\n    (mdro['XDR'] == 1) | (mdro['PDR'] == 1)\n][['hospitalization_id', 'organism_id', 'XDR', 'PDR']]\n\nprint(f\"High-risk encounters: {high_risk['hospitalization_id'].nunique()}\")\nprint(high_risk)\n</code></pre>"},{"location":"user-guide/mdro-flags/#example-3-analyze-resistance-patterns","title":"Example 3: Analyze Resistance Patterns","text":"<pre><code># Check which antimicrobial groups are most commonly resistant\ngroup_cols = [col for col in mdro.columns if col.endswith('_group')]\n\nresistance_by_group = mdro[group_cols].sum().sort_values(ascending=False)\nprint(\"Organisms resistant to each antimicrobial group:\")\nprint(resistance_by_group)\n</code></pre>"},{"location":"user-guide/mdro-flags/#example-4-verify-classification","title":"Example 4: Verify Classification","text":"<pre><code># Deep dive into why an organism was flagged as MDR\norganism_id = 'ORG123'\norg_data = mdro[mdro['organism_id'] == organism_id]\n\n# Get all antimicrobial columns (individual agents with _agent suffix)\nantimicrobial_cols = [col for col in org_data.columns if col.endswith('_agent')]\n\n# Show only tested antimicrobials\ntested = org_data[antimicrobial_cols].iloc[0]\ntested = tested[tested.notna()]\n\n# Filter to resistant results\nresistant = tested[tested.isin(['intermediate', 'non_susceptible'])]\n\nprint(f\"Organism {organism_id}:\")\nprint(f\"MDR Status: {org_data['MDR'].iloc[0]}\")\nprint(f\"\\nResistant to {len(resistant)} antimicrobials:\")\nprint(resistant)\n</code></pre>"},{"location":"user-guide/mdro-flags/#example-5-monthly-trend-analysis","title":"Example 5: Monthly Trend Analysis","text":"<pre><code># Add culture result date\nculture_dates = culture.df[['organism_id', 'result_dttm']].copy()\nmdro_with_dates = mdro.merge(culture_dates, on='organism_id', how='left')\n\n# Extract month\nmdro_with_dates['year_month'] = pd.to_datetime(\n    mdro_with_dates['result_dttm']\n).dt.to_period('M')\n\n# Calculate monthly MDR rates\nmonthly_trends = mdro_with_dates.groupby('year_month').agg({\n    'organism_id': 'count',\n    'MDR': 'sum',\n    'XDR': 'sum',\n    'PDR': 'sum'\n}).rename(columns={'organism_id': 'total_organisms'})\n\nmonthly_trends['mdr_rate'] = (\n    monthly_trends['MDR'] / monthly_trends['total_organisms'] * 100\n)\n\nprint(\"Monthly MDR Trends:\")\nprint(monthly_trends)\n</code></pre>"},{"location":"user-guide/mdro-flags/#mdro-classification-criteria","title":"MDRO Classification Criteria","text":""},{"location":"user-guide/mdro-flags/#pseudomonas-aeruginosa","title":"Pseudomonas aeruginosa","text":"<p>The function uses 8 antimicrobial groups for P. aeruginosa classification:</p> <ol> <li>Aminoglycosides - gentamicin, tobramycin, amikacin, netilmicin</li> <li>Antipseudomonal Carbapenems - imipenem, meropenem, doripenem</li> <li>Antipseudomonal Cephalosporins - ceftazidime, cefepime</li> <li>Antipseudomonal Fluoroquinolones - ciprofloxacin, levofloxacin</li> <li>Antipseudomonal Penicillins + \u03b2-lactamase Inhibitors - piperacillin-tazobactam, ticarcillin-clavulanate</li> <li>Monobactams - aztreonam</li> <li>Phosphonic Acids - fosfomycin</li> <li>Polymyxins - colistin, polymyxin B</li> </ol>"},{"location":"user-guide/mdro-flags/#classification-definitions","title":"Classification Definitions","text":"<p>MDR (Multi-Drug Resistant) - Non-susceptible to \u22651 agent in \u22653 antimicrobial groups - Example: Resistant to ciprofloxacin (fluoroquinolones), ceftazidime (cephalosporins), and gentamicin (aminoglycosides)</p> <p>XDR (Extensively Drug Resistant) - Non-susceptible to \u22651 agent in all but \u22642 antimicrobial groups - Example: Resistant to 6 out of 8 groups (susceptible to only polymyxins and fosfomycin)</p> <p>PDR (Pandrug Resistant) - Non-susceptible to ALL antimicrobial agents tested - Most severe classification</p> <p>DTR (Difficult to Treat Resistance) - ALL 8 specific agents must be tested AND all must be non-susceptible:   - Piperacillin-tazobactam   - Ceftazidime   - Cefepime   - Aztreonam   - Meropenem   - Imipenem   - Ciprofloxacin   - Levofloxacin</p>"},{"location":"user-guide/mdro-flags/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/mdro-flags/#1-data-quality-checks","title":"1. Data Quality Checks","text":"<pre><code># Before running MDRO calculation, verify data quality\n\n# Check for missing organism IDs\nmissing_organism_id = culture.df['organism_id'].isna().sum()\nif missing_organism_id &gt; 0:\n    print(f\"Warning: {missing_organism_id} cultures missing organism_id\")\n\n# Check susceptibility linkage\nlinked = susceptibility.df['organism_id'].isin(culture.df['organism_id']).sum()\ntotal_susc = len(susceptibility.df)\nprint(f\"Susceptibility tests linked to culture: {linked}/{total_susc}\")\n\n# Verify organism names\nunique_organisms = culture.df['organism_category'].value_counts()\nprint(\"Top organisms in culture data:\")\nprint(unique_organisms.head(10))\n</code></pre>"},{"location":"user-guide/mdro-flags/#2-interpret-in-clinical-context","title":"2. Interpret in Clinical Context","text":"<ul> <li>Consider testing completeness - Not all antimicrobials may be tested for every organism</li> <li>Review intermediate results - Both 'intermediate' and 'non_susceptible' count as resistant</li> <li>Check local epidemiology - Resistance patterns vary by geography and institution</li> <li>Validate flagged organisms - Spot-check results against clinical microbiology reports</li> </ul>"},{"location":"user-guide/mdro-flags/#3-document-your-analysis","title":"3. Document Your Analysis","text":"<pre><code># Save analysis metadata\nmetadata = {\n    'analysis_date': pd.Timestamp.now(),\n    'organism': 'pseudomonas_aeruginosa',\n    'total_cultures': len(culture.df),\n    'total_psar': (culture.df['organism_category'] == 'pseudomonas_aeruginosa').sum(),\n    'psar_with_susceptibility': len(mdro),\n    'mdr_count': mdro['MDR'].sum(),\n    'xdr_count': mdro['XDR'].sum(),\n    'config_used': 'clifpy/data/mdro.yaml'\n}\n\n# Save for reproducibility\nimport json\nwith open('mdro_analysis_metadata.json', 'w') as f:\n    json.dump({k: str(v) for k, v in metadata.items()}, f, indent=2)\n</code></pre>"},{"location":"user-guide/mdro-flags/#4-filter-appropriately","title":"4. Filter Appropriately","text":"<pre><code># For ICU-specific analysis, use cohort filtering\nicu_cohort = pd.DataFrame({\n    'hospitalization_id': icu_patient_ids,\n    'start_dttm': icu_admission_times,\n    'end_dttm': icu_discharge_times\n})\n\nicu_mdro = calculate_mdro_flags(\n    culture, susceptibility, 'pseudomonas_aeruginosa',\n    cohort=icu_cohort\n)\n</code></pre>"},{"location":"user-guide/mdro-flags/#integration-with-cliforchestrator","title":"Integration with ClifOrchestrator","text":"<p>If using the ClifOrchestrator, you can streamline the workflow:</p> <pre><code>from clifpy import ClifOrchestrator\nfrom clifpy.utils.mdro_flags import calculate_mdro_flags\n\n# Initialize orchestrator\nclif = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='UTC'\n)\n\n# Load required tables\nclif.initialize([\n    'hospitalization',\n    'microbiology_culture',\n    'microbiology_susceptibility'\n])\n\n# Calculate MDRO flags\nmdro_flags = calculate_mdro_flags(\n    culture=clif.microbiology_culture,\n    susceptibility=clif.microbiology_susceptibility,\n    organism_name='pseudomonas_aeruginosa'\n)\n\n# Merge with hospitalization data for further analysis\nhosps_with_mdro = clif.hospitalization.df.merge(\n    mdro_flags,\n    on='hospitalization_id',\n    how='left'\n)\n</code></pre>"},{"location":"user-guide/mdro-flags/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/mdro-flags/#common-issues","title":"Common Issues","text":"<p>Issue: \"Organism 'xxx' not found in configuration\" - Cause: The organism name doesn't match any configured organisms - Solution: Check <code>clifpy/data/mdro.yaml</code> for available organisms. Use exact organism_category values (e.g., 'pseudomonas_aeruginosa')</p> <p>Issue: \"Missing required columns in culture table\" - Cause: Culture data missing required columns - Solution: Ensure culture table has: <code>organism_id</code>, <code>hospitalization_id</code>, <code>organism_category</code> - Check: <code>print(culture.df.columns.tolist())</code></p> <p>Issue: \"No organisms with susceptibility data found\" - Cause: No matching organism_id between culture and susceptibility tables - Solution: Verify organism_id values match between tables - Check:   <pre><code>culture_ids = set(culture.df['organism_id'])\nsusc_ids = set(susceptibility.df['organism_id'])\nprint(f\"Linked: {len(culture_ids &amp; susc_ids)} organisms\")\n</code></pre></p> <p>Issue: All MDRO flags are 0 - Cause: Organisms are susceptible OR insufficient testing - Solution: Verify susceptibility data is present and check antimicrobial testing coverage - Check:   <pre><code># See what was tested\ntested_drugs = susceptibility.df['antimicrobial_category'].value_counts()\nprint(\"Most tested antimicrobials:\")\nprint(tested_drugs.head(10))\n</code></pre></p>"},{"location":"user-guide/mdro-flags/#debugging","title":"Debugging","text":"<pre><code># Enable logging to see detailed processing\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Run with logging enabled\nmdro_flags = calculate_mdro_flags(culture, susceptibility, 'pseudomonas_aeruginosa')\n\n# Check intermediate results\n# The logs will show:\n# - Number of cultures filtered\n# - Organisms without susceptibility data\n# - Number of antimicrobial columns created\n# - Number of group columns created\n</code></pre>"},{"location":"user-guide/mdro-flags/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Processing time scales linearly with number of organisms</li> <li>Memory usage depends on number of unique antimicrobials tested (wide format)</li> <li>For large datasets (&gt;100K organisms), consider:</li> <li>Filtering to specific time periods</li> <li>Processing by batch (monthly/quarterly)</li> <li>Using hospitalization_ids parameter to limit scope</li> </ul>"},{"location":"user-guide/mdro-flags/#see-also","title":"See Also","text":"<ul> <li>Microbiology Culture Table - Culture table structure</li> <li>Microbiology Susceptibility Table - Susceptibility table structure</li> <li>Utilities API - Technical API reference</li> <li>Configuration Guide - Adding new organisms and customizing criteria</li> <li>Example Notebook - Interactive Marimo demo</li> </ul>"},{"location":"user-guide/mdro-flags/#references","title":"References","text":"<ol> <li> <p>Magiorakos AP, Srinivasan A, Carey RB, et al. Multidrug-resistant, extensively drug-resistant and pandrug-resistant bacteria: an international expert proposal for interim standard definitions for acquired resistance. Clin Microbiol Infect. 2012;18(3):268-281.</p> </li> <li> <p>Kadri SS, Adjemian J, Lai YL, et al. Difficult-to-Treat Resistance in Gram-negative Bacteremia at 173 US Hospitals: Retrospective Cohort Analysis of Prevalence, Predictors, and Outcome of Resistance to All First-line Agents. Clin Infect Dis. 2018;67(12):1803-1814.</p> </li> <li> <p>CDC. Management of Multidrug-Resistant Organisms In Healthcare Settings, 2006. https://www.cdc.gov/infectioncontrol/guidelines/mdro/</p> </li> <li> <p>CLSI. Performance Standards for Antimicrobial Susceptibility Testing. 33rd ed. CLSI supplement M100. Clinical and Laboratory Standards Institute; 2023.</p> </li> </ol>"},{"location":"user-guide/med-unit-conversion/","title":"Medication Unit Conversion","text":"<p>CLIFpy provides robust medication dose unit conversion functionality to standardize medication dosing across different unit systems. This is essential for clinical data analysis where medications may be recorded in various units across different systems.</p>"},{"location":"user-guide/med-unit-conversion/#standardize-dose-units-by-medication","title":"Standardize dose units by medication","text":"<p>In the most common use cases, we want to standardize dose units by medication and pattern of administration -- all propofol doses to be presented in mcg/kg/min in the continuous table and in mcg in the intermittent table, for example.</p> <p>To achieve this, simply call one of the two <code>convert_dose_units_*</code> functions (one for continuous and one for intermittent) from the CLIF orchestrator and provide a dictionary mapping of medication categories to their preferred units:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\nco = ClifOrchestrator(config_path=\"config/config.yaml\")\n\npreferred_units_cont = {\n    \"propofol\": \"mcg/min\",\n    \"fentanyl\": \"mcg/hr\",\n    \"insulin\": \"u/hr\",\n    \"midazolam\": \"mg/hr\",\n    \"heparin\": \"u/min\"\n}\n\nco.convert_dose_units_for_continuous_meds(preferred_units=preferred_units_cont)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#returns","title":"Returns","text":"<p>Under the hood, this function automatically loads and uses the medication and vitals tables to generate two dataframes that are saved to the corresponding medication table instance by default:</p> <ol> <li><code>co.medication_admin_continuous.df_converted</code> gives the updated medication table with the new columns appended:    - <code>weight_kg</code>: the most recent weight relative to the <code>admin_dttm</code> pulled from the <code>vitals</code> table.    - <code>_clean_unit</code>: cleaned source unit string where both 'U/h' and 'units / hour' would be standardized to 'u/hr', for example.    - <code>_unit_class</code>: distinguishes where the source unit is an amount (e.g. 'mcg'), a 'rate' (e.g. 'mcg/hr'), or 'unrecognized'.    - <code>_convert_status</code>: documents whether the conversion is a \"success\" or, in the case of failure, the reason for failure, e.g. 'cannot convert amount to rate' for rows of propofol in 'mcg' that the users want to convert to 'mcg/kg/min'.    - <code>med_dose_converted</code>, <code>med_dose_unit_converted</code>: the converted results if the <code>_convert_status</code> is 'success', or fall back to the original <code>med_dose</code> and <code>_clean_unit</code> if failure.</li> </ol> <p>Note: the following demo output omits some rows and columns for display purposes</p> <ol> <li><code>co.medication_admin_continuous.conversion_counts</code> shows an aggregated summary of which source units of which <code>med_category</code> are converted to which preferred units -- and their frequency counts. A useful quality check would be to filter for all the <code>_convert_status</code> that are not 'success.'</li> </ol> <p>To access the results directly instead of from the table instance, turn off the <code>save_to_table</code> argument:</p> <pre><code>cont_converted, cont_counts = co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    save_to_table=False\n)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#override-option","title":"Override option","text":"<p>The function automatically parses whether the provided <code>med_categories</code> and preferred units in the dictionary are acceptable and return errors or warnings when they are not. To override any code-breaking error such as an unidentified <code>med_category</code> or preferred unit string, turn on the arg <code>override=True</code>:</p> <pre><code>co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    override=True\n)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#acceptable-unit-formatting","title":"Acceptable unit formatting","text":"<p>The unit strings in <code>preferred_units</code> dictionary need to be formatted a certain way for them to be accepted. (The original source unit strings in <code>med_dose_unit</code> do not face such restrictions. Both 'mL' and 'milliliter' in <code>med_dose_unit</code> can be correctly parsed as 'ml', for example.)</p> <p>For a list of acceptable preferred units:</p> <ul> <li>amount:</li> <li>mass: <code>mcg</code>, <code>mg</code>, <code>ng</code>, <code>g</code></li> <li>volume: <code>ml</code>, <code>l</code></li> <li> <p>unit: <code>mu</code>, <code>u</code></p> </li> <li> <p>weight: <code>/kg</code>, <code>/lb</code></p> </li> <li> <p>time: <code>/hr</code>, <code>/min</code></p> </li> <li> <p>rate: a combination of amount, weight, and time, e.g. 'mcg/kg/min', 'u/hr'.</p> </li> <li>the unit can be either weight-adjusted or not -- that is, both 'mcg/kg/min' and 'mcg/min' are acceptable. When no weight is available from the <code>vitals</code> table to enable conversion between weight-adjusted and weight-less units, an error will be returned.</li> </ul> <p>All strings should be in lower case with no whitespaces in between.</p>"},{"location":"user-guide/med-unit-conversion/#standardize-to-base-units-across-medications","title":"Standardize to base units across medications","text":"<p>In rarer cases, one might prefer all applicable units of the same class be collapsed onto the same scale across medications, e.g. both 'mcg/kg/min' and 'mg/hour' would be converted to the same 'mcg/min' -- referred to here as the \"base unit\" -- across all medications applicable.</p> <p>To enable this, turn on the <code>show_intermediate=True</code> argument:</p> <pre><code>cont_converted_detailed, _ = co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    save_to_table=False,\n    show_intermediate=True\n)\n</code></pre> <p>This would append a series of additional columns that were the intermediate results generated during the conversion, including the <code>_base_dose</code> and <code>_base_unit</code>.</p> <p>The set of base units are:</p> <ul> <li>amount: <code>mcg</code>, <code>ml</code>, <code>u</code></li> <li>time: <code>/min</code></li> <li>rate: a combination of amount and time, e.g. <code>mcg/min</code>, <code>u/min</code>.</li> <li>Note that all base units would be weight-less.</li> </ul>"},{"location":"user-guide/med-unit-conversion/#unit-classification-system","title":"Unit Classification System","text":""},{"location":"user-guide/med-unit-conversion/#unit-classes","title":"Unit Classes","text":"<ul> <li><code>rate</code>: Dose per time units (e.g., mcg/min, ml/hr, u/kg/hr)</li> <li><code>amount</code>: Total dose units (e.g., mcg, ml, u)</li> <li><code>unrecognized</code>: Units that cannot be parsed or converted</li> </ul>"},{"location":"user-guide/med-unit-conversion/#unit-subclasses","title":"Unit Subclasses","text":"<ul> <li><code>mass</code>: Weight-based units (mcg, mg, ng, g)</li> <li><code>volume</code>: Volume-based units (ml, l)</li> <li><code>unit</code>: Unit-based dosing (u, mu)</li> <li><code>unrecognized</code>: Units that don't fit standard categories</li> </ul> <p>Unit class and subclass compatibility determines whether conversions are allowed. For example:</p> <ul> <li> <p>\u2705 <code>rate</code> \u2192 <code>rate</code> (same class)</p> </li> <li> <p>\u2705 <code>mass</code> \u2192 <code>mass</code> (same subclass)</p> </li> <li> <p>\u274c <code>rate</code> \u2192 <code>amount</code> (different class)</p> </li> <li> <p>\u274c <code>mass</code> \u2192 <code>volume</code> (different subclass)</p> </li> </ul>"},{"location":"user-guide/med-unit-conversion/#reference-table","title":"Reference Table","text":"unit class unit subclass _clean_unit acceptable source <code>med_dose_unit</code> examples _base_unit Amount Units amount mass mcg MCG, \u00b5g, \u03bcg, ug mcg amount mass mg MG, milligram mcg amount mass ng NG, nanogram mcg amount mass g G, gram, grams mcg amount volume ml mL, milliliter, milliliters ml amount volume l L, liter, liters, litre, litres ml amount unit u U, unit, units u amount unit mu MU, milliunit, milliunits, milli-unit, milli-units u Rate Units rate mass mcg/min MCG/MIN, \u00b5g/min, \u03bcg/min, mcg/minute, micrograms/minute mcg/min rate mass mcg/hr MCG/HR, \u00b5g/hr, \u03bcg/hr, mcg/hour, micrograms/hour mcg/min rate mass mcg/kg/min MCG/KG/MIN, \u00b5g/kg/min, mcg/kg/minute mcg/min rate mass mcg/kg/hr MCG/KG/HR, \u00b5g/kg/hr, mcg/kg/hour mcg/min rate mass mcg/lb/min MCG/LB/MIN, \u00b5g/lb/min, mcg/lb/minute mcg/min rate mass mcg/lb/hr MCG/LB/HR, \u00b5g/lb/hr, mcg/lb/hour mcg/min rate volume ml/min mL/min, ml/m, milliliter/minute ml/min rate volume ml/hr mL/hr, ml/h, milliliter/hour, milliliters/hour, millilitres/hour ml/min rate volume ml/kg/min mL/kg/min, milliliter/kg/minute ml/min rate volume ml/kg/hr mL/kg/hr, milliliter/kg/hour ml/min rate volume ml/lb/min mL/lb/min, milliliter/lb/minute ml/min rate volume ml/lb/hr mL/lb/hr, milliliter/lb/hour ml/min rate unit u/min U/min, units/minute, unit/minute u/min rate unit u/hr U/hr, units/hour, unit/hour u/min rate unit u/kg/min U/kg/min, units/kg/minute u/min rate unit u/kg/hr U/kg/hr, u/kg/h, units/kg/hour u/min rate unit u/lb/min U/lb/min, units/lb/minute u/min rate unit u/lb/hr U/lb/hr, units/lb/hour, unit/lb/hr u/min"},{"location":"user-guide/med-unit-conversion/#important-notes","title":"Important Notes","text":"<ul> <li>_clean_unit: The exact format you must use when specifying preferred units</li> <li>Acceptable Variations: Raw <code>med_dose_unit</code> strings in your original DataFrame that the converter can detect and clean (these are NOT acceptable formats for preferred units)</li> <li>_base_unit: The standardized unit all conversions target (mcg/min, ml/min, u/min for rates; mcg, ml, u for amounts)</li> </ul>"},{"location":"user-guide/med-unit-conversion/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/med-unit-conversion/#the-_convert_status-column","title":"The <code>_convert_status</code> Column","text":"<p>After conversion, each record includes a <code>_convert_status</code> field indicating the outcome:</p> <ul> <li><code>success</code>: Conversion completed successfully</li> <li><code>original unit is missing</code>: No unit provided in source data</li> <li><code>original unit [unit] is not recognized</code>: Input unit cannot be parsed</li> <li><code>user-preferred unit [unit] is not recognized</code>: Target unit is invalid</li> <li><code>cannot convert [class1] to [class2]</code>: Incompatible unit classes (e.g., rate \u2192 amount)</li> <li><code>cannot convert [subclass1] to [subclass2]</code>: Incompatible unit subclasses (e.g., mass \u2192 volume)</li> <li><code>cannot convert to a weighted unit if weight_kg is missing</code>: Weight-based conversion attempted without patient weight</li> </ul>"},{"location":"user-guide/med-unit-conversion/#failure-handling","title":"Failure Handling","text":"<p>When conversion fails: - <code>med_dose_converted</code> = original <code>_base_dose</code> (or original dose if base conversion failed) - <code>med_dose_unit_converted</code> = <code>_clean_unit</code> (or original unit if cleaning failed)</p>"},{"location":"user-guide/med-unit-conversion/#alternative-direct-unit-converter-usage","title":"Alternative: Direct Unit Converter Usage","text":"<p>For advanced users who need more control or want to use the unit converter directly without the ClifOrchestrator:</p>"},{"location":"user-guide/med-unit-conversion/#primary-function-convert_dose_units_by_med_category","title":"Primary Function: <code>convert_dose_units_by_med_category()</code>","text":"<pre><code>from clifpy.utils.unit_converter import convert_dose_units_by_med_category\nimport pandas as pd\n\n# Load your medication data\nmed_df = pd.read_parquet('clifpy/data/clif_demo/clif_medication_admin_continuous.parquet')\n\n# Define preferred units for each medication\npreferred_units = {\n    'propofol': 'mcg/kg/min',\n    'fentanyl': 'mcg/hr',\n    'insulin': 'u/hr',\n    'midazolam': 'mg/hr'\n}\n\n# Convert units\nconverted_df, summary_df = convert_dose_units_by_med_category(\n    med_df=med_df,\n    preferred_units=preferred_units,\n    override=False\n)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#secondary-function-standardize_dose_to_base_units","title":"Secondary Function: <code>standardize_dose_to_base_units()</code>","text":"<p>This function is for advanced users who need to standardize all units to a base set without medication-specific preferences.</p> <pre><code>from clifpy.utils.unit_converter import standardize_dose_to_base_units\n\n# Standardize to base units only\nbase_df, counts_df = standardize_dose_to_base_units(med_df)\n</code></pre>"},{"location":"user-guide/med-unit-conversion/#best-practices","title":"Best Practices","text":"<ol> <li>Check conversion status after processing to identify failed conversions</li> <li>Use exact _clean_unit formats when specifying preferred units</li> <li>Review the conversion counts summary DataFrame to understand conversion patterns and identify data quality issues</li> <li>Test with override=True first to see all potential issues before requiring strict validation</li> <li>Validate your preferred_units dictionary against the acceptable units table above</li> </ol>"},{"location":"user-guide/med-unit-conversion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/med-unit-conversion/#common-issues","title":"Common Issues","text":"<p>Issue: \"Cannot convert rate to amount\"     - Solution: Ensure unit classes match (rate\u2192rate, amount\u2192amount)</p> <p>Issue: \"Cannot convert mass to volume\"     - Solution: Ensure unit subclasses match (mass\u2192mass, volume\u2192volume)</p> <p>Issue: \"User-preferred unit [unit] is not recognized\"     - Solution: Use exact <code>_clean_unit</code> format from the reference table above</p> <p>Issue: Weight-based conversions failing     - Solution: Ensure <code>weight_kg</code> column exists in your DataFrame or is available in vitals data</p> <p>Issue: \"Cannot convert to a weighted unit if weight_kg is missing\"     - Solution: Provide patient weights in the vitals table or med_df</p>"},{"location":"user-guide/med-unit-conversion/#getting-help","title":"Getting Help","text":"<p>If you encounter units not in the reference table or unexpected conversion failures:</p> <ol> <li>Check the <code>_convert_status</code> column for specific error messages</li> <li>Review the summary DataFrame for patterns in failed conversions</li> <li>Use <code>override=True</code> to see warnings instead of stopping on errors</li> <li>Consult the API reference for detailed function documentation</li> </ol>"},{"location":"user-guide/med-unit-conversion/#example-analysis-workflow","title":"Example Analysis Workflow","text":"<pre><code># 1. Basic conversion\nconverted_df, summary_df = co.convert_dose_units_for_continuous_meds(\n    preferred_units=preferred_units_cont,\n    save_to_table=False\n)\n\n# 2. Check conversion success\nprint(f\"Total records: {len(converted_df)}\")\nprint(f\"Successful conversions: {(converted_df['_convert_status'] == 'success').sum()}\")\n\n# 3. Analyze conversion patterns\nsummary_analysis = summary_df.groupby(['med_category', '_convert_status'])['count'].sum()\nprint(\"Conversion summary by medication:\")\nprint(summary_analysis)\n\n# 4. Check for problematic units\nproblematic_units = summary_df[summary_df['_convert_status'] != 'success']\nprint(\"\\\\nUnits requiring attention:\")\nprint(problematic_units[['med_dose_unit', '_convert_status', 'count']])\n</code></pre>"},{"location":"user-guide/outlier-handling/","title":"Outlier Handling","text":"<p>The outlier handling functionality in CLIFpy automatically identifies and removes physiologically implausible values from clinical data. This data cleaning process converts outlier values to NaN while preserving the data structure, ensuring that downstream analysis operates on clinically reasonable values.</p>"},{"location":"user-guide/outlier-handling/#overview","title":"Overview","text":"<p>Outlier handling provides:</p> <ul> <li>Automated detection of values outside clinically reasonable ranges</li> <li>Category-specific ranges for different vital signs, lab tests, medications, and assessments</li> <li>Unit-aware validation for medication dosing based on category and unit combinations</li> <li>Configurable ranges using either CLIF standard ranges or custom configurations</li> <li>Detailed statistics showing the impact of outlier removal</li> <li>Non-destructive preview capability to assess outliers before removal</li> </ul>"},{"location":"user-guide/outlier-handling/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/outlier-handling/#apply_outlier_handling","title":"<code>apply_outlier_handling()</code>","text":"<p>Applies outlier handling by converting out-of-range values to NaN:</p> <pre><code>from clifpy.utils import apply_outlier_handling\n\n# Modify data in-place using CLIF standard ranges\napply_outlier_handling(vitals_table)\n\n# Or use custom configuration\napply_outlier_handling(vitals_table, outlier_config_path=\"/path/to/custom_config.yaml\")\n</code></pre> <p>Parameters: - <code>table_obj</code>: A CLIFpy table object with <code>.df</code> and <code>.table_name</code> attributes - <code>outlier_config_path</code> (optional): Path to custom YAML configuration file</p> <p>Returns: None (modifies table data in-place)</p>"},{"location":"user-guide/outlier-handling/#get_outlier_summary","title":"<code>get_outlier_summary()</code>","text":"<p>Provides a preview of outliers without modifying data:</p> <pre><code>from clifpy.utils import get_outlier_summary\n\n# Get summary without modifying data\nsummary = get_outlier_summary(vitals_table)\nprint(f\"Total rows: {summary['total_rows']}\")\nprint(f\"Config source: {summary['config_source']}\")\n</code></pre> <p>Parameters: - <code>table_obj</code>: A CLIFpy table object with <code>.df</code> and <code>.table_name</code> attributes - <code>outlier_config_path</code> (optional): Path to custom YAML configuration file</p> <p>Returns: Dictionary with outlier analysis summary</p>"},{"location":"user-guide/outlier-handling/#configuration-types","title":"Configuration Types","text":""},{"location":"user-guide/outlier-handling/#internal-clif-standard-configuration","title":"Internal CLIF Standard Configuration","text":"<p>By default, CLIFpy uses internal clinically-validated ranges:</p> <pre><code>from clifpy.utils import apply_outlier_handling\n\n# Uses internal CLIF standard ranges automatically\napply_outlier_handling(vitals_table)\n# Output: \"Using CLIF standard outlier ranges\"\n</code></pre> <p>The internal configuration includes ranges for: - Vitals: Heart rate (0-300), blood pressure (0-300/0-200), temperature (32-44\u00b0C), etc. - Labs: Hemoglobin (2-25), sodium (90-210), glucose (0-2000), lactate (0-30), etc. - Medications: Unit-specific dosing ranges (e.g., norepinephrine 0-3 mcg/kg/min) - Assessments: Scale-specific ranges (e.g., GCS 3-15, RASS -5 to +4)</p>"},{"location":"user-guide/outlier-handling/#custom-yaml-configuration","title":"Custom YAML Configuration","text":"<p>Create custom configurations for specific research needs:</p> <pre><code># Apply custom ranges\napply_outlier_handling(vitals_table, outlier_config_path=\"/path/to/custom_ranges.yaml\")\n# Output: \"Using custom outlier ranges from: /path/to/custom_ranges.yaml\"\n</code></pre>"},{"location":"user-guide/outlier-handling/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/outlier-handling/#example-1-basic-usage-with-standard-ranges","title":"Example 1: Basic Usage with Standard Ranges","text":"<pre><code>from clifpy import Vitals\nfrom clifpy.utils import apply_outlier_handling\n\n# Load vitals data\nvitals = Vitals.from_file('/data', 'parquet', 'UTC')\n\nprint(f\"Before: {vitals.df['vital_value'].notna().sum()} non-null values\")\n\n# Apply outlier handling with CLIF standard ranges\napply_outlier_handling(vitals)\n\nprint(f\"After: {vitals.df['vital_value'].notna().sum()} non-null values\")\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-2-preview-outliers-before-removal","title":"Example 2: Preview Outliers Before Removal","text":"<pre><code>from clifpy.utils import get_outlier_summary, apply_outlier_handling\n\n# Get summary without modifying data\nsummary = get_outlier_summary(vitals)\nprint(\"Outlier Analysis Summary:\")\nprint(f\"- Table: {summary['table_name']}\")\nprint(f\"- Total rows: {summary['total_rows']}\")\nprint(f\"- Configuration: {summary['config_source']}\")\n\n# Apply outlier handling after review\napply_outlier_handling(vitals)\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-3-custom-configuration-for-research","title":"Example 3: Custom Configuration for Research","text":"<pre><code># Create custom configuration file\ncustom_config = \"\"\"\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 40    # More restrictive than standard (0)\n        max: 180   # More restrictive than standard (300)\n      temp_c:\n        min: 35.0  # More restrictive than standard (32)\n        max: 42.0  # More restrictive than standard (44)\n\"\"\"\n\nwith open('research_config.yaml', 'w') as f:\n    f.write(custom_config)\n\n# Apply custom ranges\napply_outlier_handling(vitals, outlier_config_path='research_config.yaml')\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-4-multiple-tables-with-different-configurations","title":"Example 4: Multiple Tables with Different Configurations","text":"<pre><code>from clifpy import Labs, Vitals, MedicationAdminContinuous\nfrom clifpy.utils import apply_outlier_handling\n\n# Load tables\nvitals = Vitals.from_file('/data', 'parquet', 'UTC')\nlabs = Labs.from_file('/data', 'parquet', 'UTC')\nmeds = MedicationAdminContinuous.from_file('/data', 'parquet', 'UTC')\n\n# Apply outlier handling to each table\nfor table in [vitals, labs, meds]:\n    print(f\"\\n=== Processing {table.table_name} ===\")\n    apply_outlier_handling(table)\n</code></pre>"},{"location":"user-guide/outlier-handling/#table-specific-handling","title":"Table-Specific Handling","text":""},{"location":"user-guide/outlier-handling/#simple-range-columns","title":"Simple Range Columns","text":"<p>For columns with straightforward min/max ranges:</p> <pre><code># Example: Age at admission (0-120 years)\n# Configuration:\nhospitalization:\n  age_at_admission:\n    min: 0\n    max: 120\n\n# Output statistics:\n# age_at_admission              :   5432 values \u2192     23 nullified ( 0.4%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#category-dependent-columns","title":"Category-Dependent Columns","text":"<p>For vitals, labs, and assessments where ranges depend on the category:</p> <pre><code># Example: Vital signs with different ranges per category\n# Configuration:\nvitals:\n  vital_value:\n    heart_rate:\n      min: 0\n      max: 300\n    temp_c:\n      min: 32\n      max: 44\n\n# Output statistics:\n# Vitals Table - Category Statistics:\n#   heart_rate        :  15234 values \u2192    156 nullified ( 1.0%)\n#   temp_c           :   8765 values \u2192     23 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#unit-dependent-medication-dosing","title":"Unit-Dependent Medication Dosing","text":"<p>For medications where ranges depend on both category and unit:</p> <pre><code># Example: Norepinephrine dosing with different units\n# Configuration:\nmedication_admin_continuous:\n  med_dose:\n    norepinephrine:\n      \"mcg/kg/min\":\n        min: 0.0\n        max: 3.0\n      \"mcg/min\":\n        min: 0.0\n        max: 200.0\n\n# Output statistics:\n# Medication Table - Category/Unit Statistics:\n#   norepinephrine (mcg/kg/min)  :   2341 values \u2192     12 nullified ( 0.5%)\n#   norepinephrine (mcg/min)     :    876 values \u2192      4 nullified ( 0.5%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#custom-yaml-configuration-examples","title":"Custom YAML Configuration Examples","text":""},{"location":"user-guide/outlier-handling/#example-1-research-specific-vitals-ranges","title":"Example 1: Research-Specific Vitals Ranges","text":"<pre><code># custom_vitals_config.yaml\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 50     # More restrictive for adults\n        max: 150    # Exclude extreme tachycardia\n      sbp:\n        min: 70     # Focus on hypotension\n        max: 200    # Exclude severe hypertension\n      temp_c:\n        min: 36.0   # Normothermic range\n        max: 39.0   # Exclude extreme hyperthermia\n      spo2:\n        min: 88     # Allow mild hypoxemia\n        max: 100    # Standard upper bound\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-2-pediatric-specific-ranges","title":"Example 2: Pediatric-Specific Ranges","text":"<pre><code># pediatric_config.yaml\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 60     # Pediatric range\n        max: 200    # Higher for children\n      sbp:\n        min: 60     # Lower for pediatrics\n        max: 140\n\n  hospitalization:\n    age_at_admission:\n      min: 0\n      max: 18     # Pediatric patients only\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-3-icu-specific-lab-ranges","title":"Example 3: ICU-Specific Lab Ranges","text":"<pre><code># icu_lab_config.yaml\ntables:\n  labs:\n    lab_value_numeric:\n      lactate:\n        min: 0.5    # Minimum detectable\n        max: 20.0   # ICU-relevant range\n      hemoglobin:\n        min: 4.0    # Severe anemia threshold\n        max: 20.0   # Exclude transfusion artifacts\n      creatinine:\n        min: 0.3    # Physiologic minimum\n        max: 15.0   # Include severe AKI\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-4-complete-custom-configuration-template","title":"Example 4: Complete Custom Configuration Template","text":"<pre><code># complete_custom_config.yaml\ntables:\n  # Simple range columns\n  hospitalization:\n    age_at_admission:\n      min: 18      # Adult patients only\n      max: 100     # Exclude very elderly\n\n  respiratory_support:\n    fio2_set:\n      min: 0.21    # Room air minimum\n      max: 1.0     # 100% oxygen maximum\n    peep_set:\n      min: 0       # No PEEP\n      max: 25      # High PEEP limit\n\n  # Category-dependent columns\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 40\n        max: 200\n      sbp:\n        min: 60\n        max: 250\n      temp_c:\n        min: 35.0\n        max: 42.0\n\n  labs:\n    lab_value_numeric:\n      hemoglobin:\n        min: 5.0\n        max: 18.0\n      sodium:\n        min: 120\n        max: 160\n\n  # Unit-dependent medication dosing\n  medication_admin_continuous:\n    med_dose:\n      norepinephrine:\n        \"mcg/kg/min\":\n          min: 0.01\n          max: 2.0\n        \"mcg/min\":\n          min: 1.0\n          max: 150.0\n      propofol:\n        \"mg/hr\":\n          min: 1.0\n          max: 300.0\n\n  # Assessment-specific ranges\n  patient_assessments:\n    numerical_value:\n      gcs_total:\n        min: 3\n        max: 15\n      RASS:\n        min: -5\n        max: 4\n</code></pre>"},{"location":"user-guide/outlier-handling/#understanding-output-statistics","title":"Understanding Output Statistics","text":"<p>The outlier handling provides detailed statistics showing the impact of data cleaning:</p>"},{"location":"user-guide/outlier-handling/#category-dependent-statistics","title":"Category-Dependent Statistics","text":"<pre><code>Vitals Table - Category Statistics:\n  heart_rate        :  15234 values \u2192    156 nullified ( 1.0%)\n  sbp              :  12876 values \u2192     45 nullified ( 0.3%)\n  temp_c           :   8765 values \u2192     23 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#medication-unit-dependent-statistics","title":"Medication Unit-Dependent Statistics","text":"<pre><code>Medication Table - Category/Unit Statistics:\n  norepinephrine (mcg/kg/min)  :   2341 values \u2192     12 nullified ( 0.5%)\n  propofol (mg/hr)            :   1876 values \u2192      8 nullified ( 0.4%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#simple-range-statistics","title":"Simple Range Statistics","text":"<pre><code>age_at_admission              :   5432 values \u2192     23 nullified ( 0.4%)\nfio2_set                     :   3456 values \u2192     12 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#integration-with-cliforchestrator","title":"Integration with ClifOrchestrator","text":"<p>The outlier handling integrates seamlessly with the ClifOrchestrator workflow:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\nfrom clifpy.utils import apply_outlier_handling\n\n# Initialize orchestrator and load tables\nco = ClifOrchestrator('/data', 'parquet', 'UTC')\nco.initialize(['vitals', 'labs', 'medication_admin_continuous'])\n\n# Apply outlier handling to all loaded tables\nfor table_name in co.get_loaded_tables():\n    table_obj = getattr(co, table_name)\n    print(f\"\\n=== Cleaning {table_name} ===\")\n    apply_outlier_handling(table_obj)\n\n# Validate after outlier handling\nco.validate_all()\n\n# Create wide dataset with clean data\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin', 'sodium']\n    }\n)\n</code></pre>"},{"location":"user-guide/outlier-handling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/outlier-handling/#1-preview-before-application","title":"1. Preview Before Application","text":"<pre><code># Always preview outliers first\nsummary = get_outlier_summary(table)\nprint(f\"Will affect {summary['total_rows']} rows\")\n\n# Apply after review\napply_outlier_handling(table)\n</code></pre>"},{"location":"user-guide/outlier-handling/#2-keep-original-data","title":"2. Keep Original Data","text":"<pre><code># Make backup before outlier handling\noriginal_df = vitals.df.copy()\n\n# Apply outlier handling\napply_outlier_handling(vitals)\n\n# Compare results\nprint(f\"Original: {original_df['vital_value'].notna().sum()} values\")\nprint(f\"Cleaned:  {vitals.df['vital_value'].notna().sum()} values\")\nprint(f\"Removed:  {original_df['vital_value'].notna().sum() - vitals.df['vital_value'].notna().sum()} values\")\n</code></pre>"},{"location":"user-guide/outlier-handling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/outlier-handling/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>No Configuration Found <pre><code># Error: \"No outlier configuration found for table: custom_table\"\n# Solution: Add table configuration to your custom YAML\n\ntables:\n  custom_table:\n    numeric_column:\n      min: 0\n      max: 100\n</code></pre></p> <p>Missing Columns <pre><code># Warning: Configuration references columns not in data\n# Solution: Check column names in your data vs. configuration\nprint(vitals.df.columns.tolist())  # Check actual column names\n</code></pre></p> <p>No Outliers Detected <pre><code># All values are within range - this is normal for clean data\n# The statistics will show \"0 nullified\" for all categories\n</code></pre></p>"},{"location":"user-guide/outlier-handling/#internal-clif-standard-ranges","title":"Internal CLIF Standard Ranges","text":"<p>The internal configuration includes clinically-validated ranges for:</p>"},{"location":"user-guide/outlier-handling/#vitals","title":"Vitals","text":"<ul> <li>Heart rate: 0-300 bpm</li> <li>Blood pressure: SBP 0-300, DBP 0-200, MAP 0-250 mmHg  </li> <li>Temperature: 32-44\u00b0C</li> <li>SpO2: 50-100%</li> <li>Respiratory rate: 0-60/min</li> <li>Height: 76-255 cm, Weight: 30-1100 kg</li> </ul>"},{"location":"user-guide/outlier-handling/#common-labs","title":"Common Labs","text":"<ul> <li>Hemoglobin: 2.0-25.0 g/dL</li> <li>Sodium: 90-210 mEq/L</li> <li>Potassium: 0-15 mEq/L</li> <li>Creatinine: 0-20 mg/dL</li> <li>Glucose: 0-2000 mg/dL</li> <li>Lactate: 0-30 mmol/L</li> </ul>"},{"location":"user-guide/outlier-handling/#medication-dosing-examples","title":"Medication Dosing (examples)","text":"<ul> <li>Norepinephrine: 0-3 mcg/kg/min, 0-200 mcg/min</li> <li>Propofol: 0-400 mg/hr, 0-200 mcg/kg/min</li> <li>Fentanyl: 0-500 mcg/hr, 0-10 mcg/kg/hr</li> </ul>"},{"location":"user-guide/outlier-handling/#clinical-assessments","title":"Clinical Assessments","text":"<ul> <li>GCS Total: 3-15</li> <li>RASS: -5 to +4</li> <li>Richmond Agitation Sedation Scale: 1-7</li> <li>Braden Total: 6-23</li> </ul>"},{"location":"user-guide/outlier-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about data validation to ensure data quality after outlier removal</li> <li>Explore wide dataset creation with cleaned data</li> <li>Review individual table guides for table-specific considerations</li> <li>See the orchestrator guide for workflow integration</li> </ul>"},{"location":"user-guide/quickstart/","title":"Quickstart","text":"<p>This tutorial will get you up to speed with the core functionalities of <code>CLIFpy</code>.</p>  The built-in demo data  Here and throughout the user guide, we use the built-in CLIF-MIMIC demo data, but any interfaces presented would be exactly the same as when you use your own site's data."},{"location":"user-guide/quickstart/#core-workflow","title":"Core workflow","text":"<p>The easiest way to use <code>CLIFpy</code> is via the CLIF orchestrator, which allows you to seamlessly load and process multiple CLIF tables at the same time through an unified interface. </p>"},{"location":"user-guide/quickstart/#initialize-a-clif-orchestrator","title":"Initialize a CLIF orchestrator","text":"<p>To initialize a clif orchestrator, share with it your usual CLIF configurations, either via a <code>.yaml</code> config file (details below) or by directly populating the parameters: </p> <pre><code>from clifpy import ClifOrchestrator\n\n# approach 1 (recommended) - provide the path to your CLIF config file\nco = ClifOrchestrator(config_path=\"config/demo_data_config.yaml\")\n\n# approach 2 - populate all the parameters\nco = ClifOrchestrator(\n    data_directory='clifpy/data/clif_demo',\n    filetype='parquet',\n    timezone='US/Eastern',\n    output_directory='output/demo'\n)\n</code></pre> <p>The config file, conventionally stored at <code>&lt;PROJECT_ROOT&gt;/config/config.yaml</code>, should look like: <pre><code>data_directory: /path/to/where/you/store/your/clif/tables\nfiletype: parquet\ntimezone: US/Central\noutput_directory: /path/to/where/you/want/to/store/any/clifpy/outputs\n</code></pre></p> <p>The parameters always override config file settings if both are provided:</p> <pre><code># approach 3 - use the config file but override specific settings\nco = ClifOrchestrator(\n    config_path=\"config/demo_data_config.yaml\",\n    timezone='UTC',  # Override the timezone setting of 'US/Central' in the config file\n)\n</code></pre>"},{"location":"user-guide/quickstart/#load-data","title":"Load data","text":"<p>To load data from multiple CLIF tables at once, call the <code>.initialize</code> method from the orchestrator instance. To be memory-efficient, specify and only load the columns and rows you need:</p> <pre><code>required_labs_columns = ['hospitalization_id', 'lab_result_dttm', 'lab_value', 'lab_category']\nrequired_vitals_columns = ['hospitalization_id', 'recorded_dttm', 'vital_value', 'vital_category']\nrequired_labs_filters = {'lab_category': ['hemoglobin', 'sodium', 'creatinine']}\nrequired_vitals_filters = {'vital_category': ['heart_rate', 'sbp', 'spo2']}\n\nco.initialize(\n    tables=['patient', 'labs', 'vitals'],\n    # sample_size=1000, \n    columns={\n        'labs': required_labs_columns,\n        'vitals': required_vitals_columns\n    },\n    filters={\n        'labs': required_labs_filters,\n        'vitals': required_vitals_filters\n    }\n)\n</code></pre> <pre><code>&gt;&gt;&gt; co.get_loaded_tables() # list the names of loaded tables\n['patient', 'labs', 'vitals']\n</code></pre> <p>If no <code>columns</code> or <code>filters</code> are specified, the entire tables would be loaded.</p>  Loading without the orchestrator  Alternatively (and more verbosely), you can load data individually from their table-specific classes:  <pre><code>from clifpy import Patient, Labs, Vitals\n\npatient_table = Patient.from_file(config_path='config/demo_data_config.yaml')\n\nlabs_table = Labs.from_file(\n    config_path='config/demo_data_config.yaml',\n    columns=required_labs_columns,\n    filters=required_labs_filters\n)\n\nvitals_table = Vitals.from_file(\n    config_path='config/demo_data_config.yaml',\n    columns=required_vitals_columns,\n    filters=required_vitals_filters\n)\n</code></pre>  In most cases, using the orchestrator is recommended since it provides an unified interface where the individually loaded `patient_table` object, e.g., can be accessed from the orchestrator at `co.patients`:  <pre><code>&gt;&gt;&gt; isinstance(co.patient, Patient)\nTrue\n\n&gt;&gt;&gt; isinstance(co.labs, Labs)\nTrue\n</code></pre> <p>Once loaded, the data would be available as <code>pd.DataFrame</code> at the <code>.df</code> attribute of their corresponding table object:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; isinstance(co.patient.df, pd.DataFrame)\nTrue\n&gt;&gt;&gt; isinstance(co.labs.df, pd.DataFrame)\nTrue\n</code></pre>"},{"location":"user-guide/quickstart/#validate-data-quality","title":"Validate data quality","text":"<p>Through the orchestrator, you can batch validate if your CLIF tables are in line with the schema:</p> <pre><code>&gt;&gt;&gt; co.validate_all()\nValidating 3 table(s)...\n\nValidating patient...\nValidation completed with 5 error(s). See `errors` attribute.\n\nValidating labs...\nValidation completed with 3 error(s). See `errors` attribute.\n\nValidating vitals...\nValidation completed with 3 error(s). See `errors` attribute.\n</code></pre>  Validating tables individually  This is equivalent to running each of the following individually:    <pre><code>&gt;&gt;&gt; co.patient.validate()\nValidation completed with 5 error(s). See `errors` attribute.\n\n&gt;&gt;&gt; co.labs.validate()\nValidation completed with 3 error(s). See `errors` attribute.\n\n&gt;&gt;&gt; co.vitals.validate()\nValidation completed with 3 error(s). See `errors` attribute.\n</code></pre> <p>The validation failures for the <code>labs</code> table, for example, can be viewed at:</p> <pre><code>&gt;&gt;&gt; co.labs.errors\n[\n  {\n    \"type\": \"datetime_timezone\",\n    \"column\": \"lab_result_dttm\",\n    \"timezone\": \"US/Eastern\",\n    \"expected\": \"UTC\",\n    \"status\": \"warning\",\n    \"message\": \"Column 'lab_result_dttm' has timezone 'US/Eastern' but expected 'UTC'\"\n  },\n  {\n    \"type\": \"missing_categorical_values\",\n    \"column\": \"lab_category\",\n    \"missing_values\": [\n      \"basophils_percent\",\n      \"chloride\",\n      \"so2_central_venous\",\n    ],\n    \"total_missing\": 3,\n    \"message\": \"Column 'lab_category' is missing 3 expected category values\"\n  }\n]\n</code></pre>"},{"location":"user-guide/quickstart/#features-tour","title":"Features Tour","text":""},{"location":"user-guide/quickstart/#wide-datasets","title":"Wide Datasets","text":"<p>Transform time-series data into wide format. See Wide Dataset Guide for details.</p> <pre><code>co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['hemoglobin', 'sodium', 'creatinine']\n    },\n    sample=True,  # 20 random patients\n    # batch_size=500,\n    # memory_limit='8GB',\n    show_progress=True\n)\n</code></pre>"},{"location":"user-guide/quickstart/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Use YAML configuration for consistent settings across projects</li> <li>Load with filters to reduce memory load</li> <li>Use sample_size when prototyping (e.g., 1000 rows)</li> <li>Validate early after loading to catch data issues</li> </ul>"},{"location":"user-guide/sofa/","title":"SOFA Score Computation","text":"<p>Compute Sequential Organ Failure Assessment (SOFA) scores from CLIF data.</p>"},{"location":"user-guide/sofa/#quick-start","title":"Quick Start","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\nco = ClifOrchestrator(config_path='config/config.yaml')\nsofa_scores = co.compute_sofa_scores()\n</code></pre>"},{"location":"user-guide/sofa/#parameters","title":"Parameters","text":"<ul> <li><code>wide_df</code>: Optional pre-computed wide dataset</li> <li><code>cohort_df</code>: Optional time windows for filtering</li> <li><code>id_name</code>: Grouping column (default: 'encounter_block')</li> <li><code>extremal_type</code>: 'worst' (default) or 'latest' (future)</li> <li><code>fill_na_scores_with_zero</code>: Handle missing data (default: True)</li> </ul>"},{"location":"user-guide/sofa/#encounter-block-vs-hospitalization-id","title":"Encounter Block vs Hospitalization ID","text":"<p>By default, SOFA scores are computed per <code>encounter_block</code>, which groups related hospitalizations:</p> <pre><code># Initialize with encounter stitching\nco = ClifOrchestrator(\n    config_path='config/config.yaml',\n    stitch_encounter=True,\n    stitch_time_interval=6  # hours between admissions\n)\n\n# Default: scores per encounter block (may span multiple hospitalizations)\nsofa_by_encounter = co.compute_sofa_scores()  # uses encounter_block\n\n# Alternative: scores per individual hospitalization\nsofa_by_hosp = co.compute_sofa_scores(id_name='hospitalization_id')\n</code></pre> <p>What happens when using encounter_block:</p> <ul> <li>If encounter mapping doesn't exist, it's created automatically via <code>run_stitch_encounters()</code></li> <li>Multiple hospitalizations within the time interval are grouped as one encounter</li> <li>SOFA score represents the worst values across the entire encounter</li> <li>Result has one row per encounter_block instead of per hospitalization</li> </ul> <p>Example encounter mapping: <pre><code>hospitalization_id | encounter_block\n-------------------|----------------\n12345             | E001\n12346             | E001  # Same encounter (readmit &lt; 6 hours)\n12347             | E002  # Different encounter\n</code></pre></p>"},{"location":"user-guide/sofa/#required-data","title":"Required Data","text":"<p>SOFA requires these variables:</p> <ul> <li>Labs: creatinine, platelet_count, po2_arterial, bilirubin_total</li> <li>Vitals: map, spo2</li> <li>Assessments: gcs_total</li> <li>Medications: norepinephrine, epinephrine, dopamine, dobutamine (pre-converted to mcg/kg/min)</li> <li>Respiratory: device_category, fio2_set</li> </ul>"},{"location":"user-guide/sofa/#missing-data","title":"Missing Data","text":"<ul> <li>Missing values default to score of 0</li> <li>P/F ratio uses PaO2 or imputed from SpO2</li> <li>Medications must be pre-converted to standard units</li> </ul>"},{"location":"user-guide/sofa/#example-with-time-filtering","title":"Example with Time Filtering","text":"<pre><code>import pandas as pd\n\n# Define cohort with time windows\ncohort_df = pd.DataFrame({\n    'encounter_block': ['E001', 'E002'],  # or 'hospitalization_id'\n    'start_time': pd.to_datetime(['2024-01-01', '2024-01-02']),\n    'end_time': pd.to_datetime(['2024-01-03', '2024-01-04'])\n})\n\nsofa_scores = co.compute_sofa_scores(\n    cohort_df=cohort_df,\n    id_name='encounter_block'  # must match cohort_df column\n)\n</code></pre>"},{"location":"user-guide/sofa/#output","title":"Output","text":"<p>Returns DataFrame with:</p> <ul> <li>One row per <code>id_name</code> (encounter_block or hospitalization_id)</li> <li>Individual component scores (sofa_cv_97, sofa_coag, sofa_liver, sofa_resp, sofa_cns, sofa_renal)</li> <li>Total SOFA score (sofa_total)</li> <li>Intermediate calculations (p_f, p_f_imputed)</li> </ul>"},{"location":"user-guide/sofa/#sofa-components","title":"SOFA Components","text":"Component Based on Score Range Cardiovascular Vasopressor doses, MAP 0-4 Coagulation Platelet count 0-4 Liver Bilirubin levels 0-4 Respiratory P/F ratio, respiratory support 0-4 CNS GCS score 0-4 Renal Creatinine levels 0-4 <p>Higher scores indicate worse organ dysfunction. Total score ranges from 0-24.</p>"},{"location":"user-guide/sofa/#notes","title":"Notes","text":"<ul> <li>Medication units: Ensure medications are pre-converted to mcg/kg/min using the unit converter</li> <li>PaO2 imputation: When PaO2 is missing but SpO2 &lt; 97%, PaO2 is estimated using the Severinghaus equation</li> <li>Missing data philosophy: Absence of monitoring data suggests the organ wasn't failing enough to warrant close observation (score = 0)</li> </ul>"},{"location":"user-guide/sofa/#high-performance-sofa-with-polars","title":"High-Performance SOFA with Polars","text":"<p>For large datasets or performance-critical applications, CLIFpy provides <code>compute_sofa_polars()</code>, an optimized implementation using Polars that loads data directly from files.</p>"},{"location":"user-guide/sofa/#quick-start-polars","title":"Quick Start (Polars)","text":"<pre><code>import polars as pl\nfrom datetime import datetime\nfrom clifpy import compute_sofa_polars\n\n# Define cohort with time windows\ncohort_df = pl.DataFrame({\n    'hospitalization_id': ['H001', 'H002', 'H003'],\n    'start_dttm': [datetime(2024, 1, 1), datetime(2024, 1, 2), datetime(2024, 1, 3)],\n    'end_dttm': [datetime(2024, 1, 2), datetime(2024, 1, 3), datetime(2024, 1, 4)]\n})\n\n# Compute SOFA scores\nsofa_scores = compute_sofa_polars(\n    data_directory='/path/to/clif/data',\n    cohort_df=cohort_df,\n    filetype='parquet',\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"user-guide/sofa/#parameters-polars","title":"Parameters (Polars)","text":"Parameter Type Default Description <code>data_directory</code> str required Path to directory containing CLIF data files <code>cohort_df</code> pl.DataFrame required Cohort with hospitalization_id, start_dttm, end_dttm <code>filetype</code> str 'parquet' File format ('parquet' or 'csv') <code>id_name</code> str 'hospitalization_id' Column name for grouping scores <code>extremal_type</code> str 'worst' Aggregation type ('worst' for min/max) <code>fill_na_scores_with_zero</code> bool True Fill missing component scores with 0 <code>remove_outliers</code> bool True Remove physiologically implausible values <code>timezone</code> str None Target timezone (e.g., 'US/Central')"},{"location":"user-guide/sofa/#with-encounter-blocks","title":"With Encounter Blocks","text":"<pre><code>import polars as pl\nfrom datetime import datetime\nfrom clifpy import compute_sofa_polars\n\n# Cohort with encounter blocks\ncohort_df = pl.DataFrame({\n    'hospitalization_id': ['H001', 'H002', 'H003'],\n    'encounter_block': [1, 1, 2],  # H001 and H002 are same encounter\n    'start_dttm': [datetime(2024, 1, 1), datetime(2024, 1, 2), datetime(2024, 1, 5)],\n    'end_dttm': [datetime(2024, 1, 2), datetime(2024, 1, 3), datetime(2024, 1, 6)]\n})\n\n# Group by encounter_block instead of hospitalization_id\nsofa_scores = compute_sofa_polars(\n    data_directory='/path/to/clif/data',\n    cohort_df=cohort_df,\n    filetype='parquet',\n    id_name='encounter_block',\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"user-guide/sofa/#integration-with-pandas-workflow","title":"Integration with Pandas Workflow","text":"<p>If you have a pandas cohort DataFrame, convert it to Polars:</p> <pre><code>import pandas as pd\nimport polars as pl\nfrom clifpy import compute_sofa_polars\n\n# Pandas cohort\ncohort_pd = pd.DataFrame({\n    'hospitalization_id': ['H001', 'H002'],\n    'start_dttm': pd.to_datetime(['2024-01-01', '2024-01-02']),\n    'end_dttm': pd.to_datetime(['2024-01-02', '2024-01-03'])\n})\n\n# Convert to Polars\ncohort_pl = pl.from_pandas(cohort_pd)\n\n# Compute SOFA\nsofa_scores_pl = compute_sofa_polars(\n    data_directory='/path/to/clif/data',\n    cohort_df=cohort_pl,\n    timezone='US/Central'\n)\n\n# Convert result back to pandas if needed\nsofa_scores_pd = sofa_scores_pl.to_pandas()\n</code></pre>"},{"location":"user-guide/sofa/#performance-benefits","title":"Performance Benefits","text":"<p>The Polars implementation offers significant performance improvements:</p> <ul> <li>Lazy evaluation: Uses <code>scan_parquet()</code> for memory-efficient loading</li> <li>Predicate pushdown: Filters are applied at the file level</li> <li>Parallel execution: Polars automatically parallelizes operations</li> <li>Memory efficiency: Processes data in chunks, avoiding memory exhaustion</li> </ul> <p>Recommended for: - Large cohorts (&gt;10,000 hospitalizations) - Memory-constrained environments - Production pipelines requiring fast execution</p>"},{"location":"user-guide/sofa/#polars-vs-orchestrator-comparison","title":"Polars vs Orchestrator Comparison","text":"Feature <code>ClifOrchestrator.compute_sofa_scores()</code> <code>compute_sofa_polars()</code> Backend Pandas + DuckDB Polars Data loading Requires pre-loaded tables Loads directly from files Memory usage Higher (full tables in memory) Lower (lazy evaluation) Speed Good Faster for large datasets Integration Works with orchestrator workflow Standalone function Output pandas DataFrame polars DataFrame"},{"location":"user-guide/sofa/#additional-polars-utilities","title":"Additional Polars Utilities","text":"<p>CLIFpy also exports Polars-based utilities for loading and datetime handling:</p> <pre><code>from clifpy import (\n    load_data_polars,\n    load_clif_table_polars,\n    standardize_datetime_columns_polars,\n)\n\n# Load any CLIF table as Polars LazyFrame\nlabs = load_data_polars(\n    table_name='labs',\n    table_path='/path/to/clif/data',\n    table_format_type='parquet',\n    site_tz='US/Central',\n    lazy=True  # Returns LazyFrame for deferred execution\n)\n\n# Convenience function with filtering\nvitals = load_clif_table_polars(\n    data_directory='/path/to/clif/data',\n    table_name='vitals',\n    hospitalization_ids=['H001', 'H002'],\n    site_tz='US/Central'\n)\n</code></pre>"},{"location":"user-guide/timezones/","title":"Working with Timezones","text":"<p>Proper timezone handling is critical when working with ICU data from multiple sources. This guide explains how CLIFpy manages timezones and best practices for your data.</p>"},{"location":"user-guide/timezones/#overview","title":"Overview","text":"<p>CLIFpy ensures all datetime columns are timezone-aware to: - Prevent ambiguity in timestamp interpretation - Enable accurate time-based calculations - Support data from multiple time zones - Maintain consistency across tables</p>"},{"location":"user-guide/timezones/#timezone-specification","title":"Timezone Specification","text":""},{"location":"user-guide/timezones/#when-loading-data","title":"When Loading Data","text":"<p>Always specify the timezone when loading data:</p> <pre><code># Specify source data timezone\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'  # Source data timezone\n)\n\n# Common US timezones\n# 'US/Eastern', 'US/Central', 'US/Mountain', 'US/Pacific'\n# 'America/New_York', 'America/Chicago', 'America/Denver', 'America/Los_Angeles'\n</code></pre>"},{"location":"user-guide/timezones/#using-orchestrator","title":"Using Orchestrator","text":"<p>The orchestrator ensures consistent timezone across all tables:</p> <pre><code>orchestrator = ClifOrchestrator(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'  # Applied to all tables\n)\n</code></pre>"},{"location":"user-guide/timezones/#timezone-conversion","title":"Timezone Conversion","text":""},{"location":"user-guide/timezones/#during-loading","title":"During Loading","text":"<p>CLIFpy automatically converts datetime columns to the specified timezone:</p> <pre><code># Original data in UTC\ntable = TableClass.from_file('/data', 'parquet', timezone='UTC')\n\n# Convert to Central time during loading\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#after-loading","title":"After Loading","text":"<p>Convert between timezones using pandas:</p> <pre><code># Convert to different timezone\ndf = table.df.copy()\ndf['lab_datetime'] = df['lab_datetime'].dt.tz_convert('US/Eastern')\n\n# Localize timezone-naive data (not recommended)\n# df['datetime'] = df['datetime'].dt.tz_localize('US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#common-timezone-issues","title":"Common Timezone Issues","text":""},{"location":"user-guide/timezones/#issue-1-timezone-naive-data","title":"Issue 1: Timezone-Naive Data","text":"<p>Problem: Source data lacks timezone information</p> <pre><code># This will fail validation\ntable.validate()\n# Error: \"Datetime column 'admission_date' is not timezone-aware\"\n</code></pre> <p>Solution: Specify timezone during loading</p> <pre><code># CLIFpy will localize to specified timezone\ntable = TableClass.from_file(\n    '/data', \n    'parquet', \n    timezone='US/Central'  # Assumes data is in Central time\n)\n</code></pre>"},{"location":"user-guide/timezones/#issue-2-mixed-timezones","title":"Issue 2: Mixed Timezones","text":"<p>Problem: Different tables from different timezones</p> <pre><code># Hospital A in Eastern time\nlabs_a = Labs.from_file('/hospital_a/data', 'parquet', timezone='US/Eastern')\n\n# Hospital B in Pacific time  \nlabs_b = Labs.from_file('/hospital_b/data', 'parquet', timezone='US/Pacific')\n</code></pre> <p>Solution: Convert to common timezone</p> <pre><code># Convert both to UTC for analysis\nlabs_a.df['lab_datetime'] = labs_a.df['lab_datetime'].dt.tz_convert('UTC')\nlabs_b.df['lab_datetime'] = labs_b.df['lab_datetime'].dt.tz_convert('UTC')\n\n# Combine datasets\ncombined_labs = pd.concat([labs_a.df, labs_b.df])\n</code></pre>"},{"location":"user-guide/timezones/#issue-3-daylight-saving-time","title":"Issue 3: Daylight Saving Time","text":"<p>Problem: Ambiguous times during DST transitions</p> <pre><code># Fall back: 2:00 AM occurs twice\n# Spring forward: 2:00 AM doesn't exist\n</code></pre> <p>Solution: Use pytz-aware timezone names</p> <pre><code># Good - handles DST automatically\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n\n# Avoid - doesn't handle DST\n# table = TableClass.from_file('/data', 'parquet', timezone='CST6CDT')\n</code></pre>"},{"location":"user-guide/timezones/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/timezones/#1-know-your-source-timezone","title":"1. Know Your Source Timezone","text":"<pre><code># Document source timezone\n\"\"\"\nData extracted from Hospital EHR\nTimezone: US/Central (America/Chicago)\nIncludes DST adjustments\n\"\"\"\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#2-use-consistent-timezones","title":"2. Use Consistent Timezones","text":"<pre><code># Use orchestrator for consistency\norchestrator = ClifOrchestrator('/data', 'parquet', timezone='US/Central')\norchestrator.initialize(tables=['labs', 'vitals', 'medications'])\n\n# All tables now use same timezone\n</code></pre>"},{"location":"user-guide/timezones/#3-validate-timezone-handling","title":"3. Validate Timezone Handling","text":"<pre><code># Check timezone after loading\nprint(f\"Lab datetime timezone: {table.df['lab_datetime'].dt.tz}\")\n\n# Verify reasonable time ranges\nprint(f\"Earliest: {table.df['lab_datetime'].min()}\")\nprint(f\"Latest: {table.df['lab_datetime'].max()}\")\n</code></pre>"},{"location":"user-guide/timezones/#4-document-timezone-conversions","title":"4. Document Timezone Conversions","text":"<pre><code># Keep audit trail of conversions\nmetadata = {\n    'original_timezone': 'US/Eastern',\n    'converted_timezone': 'UTC',\n    'conversion_date': datetime.now(),\n    'conversion_method': 'pandas dt.tz_convert'\n}\n</code></pre>"},{"location":"user-guide/timezones/#time-based-calculations","title":"Time-based Calculations","text":""},{"location":"user-guide/timezones/#duration-calculations","title":"Duration Calculations","text":"<p>Timezone-aware datetimes ensure accurate duration calculations:</p> <pre><code># Calculate length of stay\nlos = adt.df['out_dttm'] - adt.df['in_dttm']\nlos_hours = los.dt.total_seconds() / 3600\n\n# Time since admission\ncurrent_time = pd.Timestamp.now(tz='US/Central')\ntime_since = current_time - hosp.df['admission_dttm']\n</code></pre>"},{"location":"user-guide/timezones/#filtering-by-time","title":"Filtering by Time","text":"<pre><code># Get data from last 24 hours\ncutoff = pd.Timestamp.now(tz='US/Central') - pd.Timedelta(hours=24)\nrecent = table.df[table.df['datetime_column'] &gt;= cutoff]\n\n# Filter to specific date (timezone-aware)\ndate = pd.Timestamp('2023-01-01', tz='US/Central')\nday_data = table.df[table.df['datetime_column'].dt.date == date.date()]\n</code></pre>"},{"location":"user-guide/timezones/#aggregating-by-time","title":"Aggregating by Time","text":"<pre><code># Hourly aggregation\nhourly = table.df.set_index('datetime_column').resample('H').mean()\n\n# Daily aggregation (timezone affects day boundaries!)\ndaily = table.df.set_index('datetime_column').resample('D').count()\n</code></pre>"},{"location":"user-guide/timezones/#multi-site-considerations","title":"Multi-site Considerations","text":"<p>When combining data from multiple sites:</p> <pre><code># Strategy 1: Convert all to UTC\nsites = ['site_a', 'site_b', 'site_c']\nsite_timezones = {\n    'site_a': 'US/Eastern',\n    'site_b': 'US/Central', \n    'site_c': 'US/Pacific'\n}\n\nall_data = []\nfor site in sites:\n    table = Labs.from_file(f'/data/{site}', 'parquet', \n                          timezone=site_timezones[site])\n    # Convert to UTC\n    table.df['lab_datetime'] = table.df['lab_datetime'].dt.tz_convert('UTC')\n    table.df['site'] = site\n    all_data.append(table.df)\n\ncombined = pd.concat(all_data)\n</code></pre> <pre><code># Strategy 2: Use site's local time with site column\n# Keep original timezone but track source\nfor site in sites:\n    table = Labs.from_file(f'/data/{site}', 'parquet',\n                          timezone=site_timezones[site])\n    table.df['site'] = site\n    table.df['source_timezone'] = site_timezones[site]\n</code></pre>"},{"location":"user-guide/timezones/#timezone-reference","title":"Timezone Reference","text":"<p>Common medical facility timezones:</p> <pre><code>US_TIMEZONES = {\n    'Eastern': 'US/Eastern',     # NYC, Boston, Atlanta\n    'Central': 'US/Central',     # Chicago, Houston, Dallas\n    'Mountain': 'US/Mountain',   # Denver, Phoenix\n    'Pacific': 'US/Pacific',     # LA, Seattle, San Francisco\n    'Toronto': 'America/Toronto' # Toronto, Canada\n}\n</code></pre>"},{"location":"user-guide/timezones/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/timezones/#check-current-timezone","title":"Check Current Timezone","text":"<pre><code># For a datetime column\nprint(table.df['datetime_column'].dt.tz)\n\n# For a single timestamp\nprint(table.df['datetime_column'].iloc[0].tzinfo)\n</code></pre>"},{"location":"user-guide/timezones/#fix-timezone-issues","title":"Fix Timezone Issues","text":"<pre><code># If validation fails due to timezone\nif not table.isvalid():\n    tz_errors = [e for e in table.errors if 'timezone' in str(e)]\n    if tz_errors:\n        # Reload with proper timezone\n        table = TableClass.from_file('/data', 'parquet', \n                                   timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#next-steps","title":"Next Steps","text":"<ul> <li>Review validation guide for timezone validation</li> <li>See examples of timezone handling</li> <li>Learn about multi-site analysis</li> </ul>"},{"location":"user-guide/validation/","title":"Data Validation","text":"<p>CLIFpy provides comprehensive validation to ensure your data conforms to CLIF standards. This guide explains the validation process and how to interpret results.</p>"},{"location":"user-guide/validation/#overview","title":"Overview","text":"<p>Validation in CLIFpy operates at multiple levels:</p> <ol> <li>Schema Validation - Ensures required columns exist with correct data types</li> <li>Category Validation - Verifies values match standardized categories</li> <li>Range Validation - Checks values fall within clinically reasonable ranges</li> <li>Timezone Validation - Ensures datetime columns are timezone-aware</li> <li>Duplicate Detection - Identifies duplicate records based on composite keys</li> <li>Completeness Checks - Analyzes missing data patterns</li> </ol>"},{"location":"user-guide/validation/#running-validation","title":"Running Validation","text":""},{"location":"user-guide/validation/#basic-validation","title":"Basic Validation","text":"<pre><code># Load and validate a table\ntable = TableClass.from_file('/data', 'parquet')\ntable.validate()\n\n# Check if valid\nif table.isvalid():\n    print(\"Validation passed!\")\nelse:\n    print(f\"Found {len(table.errors)} validation errors\")\n</code></pre>"},{"location":"user-guide/validation/#bulk-validation-with-orchestrator","title":"Bulk Validation with Orchestrator","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\norchestrator = ClifOrchestrator('/data', 'parquet')\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Validate all tables\norchestrator.validate_all()\n</code></pre>"},{"location":"user-guide/validation/#understanding-validation-results","title":"Understanding Validation Results","text":""},{"location":"user-guide/validation/#error-types","title":"Error Types","text":"<p>Validation errors are stored in the <code>errors</code> attribute:</p> <pre><code># Review errors\nfor error in table.errors[:10]:  # First 10 errors\n    print(f\"Type: {error['type']}\")\n    print(f\"Message: {error['message']}\")\n    print(f\"Details: {error.get('details', 'N/A')}\")\n    print(\"-\" * 50)\n</code></pre> <p>Common error types: - <code>missing_column</code> - Required column not found - <code>invalid_category</code> - Value not in permissible list - <code>out_of_range</code> - Value outside acceptable range - <code>invalid_timezone</code> - Datetime column not timezone-aware - <code>duplicate_rows</code> - Duplicate records found</p>"},{"location":"user-guide/validation/#validation-reports","title":"Validation Reports","text":"<p>Validation results are automatically saved to the output directory:</p> <pre><code># Set custom output directory\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    output_directory='/path/to/reports'\n)\n\n# After validation, these files are created:\n# - validation_log_[table_name].log\n# - validation_errors_[table_name].csv\n# - missing_data_stats_[table_name].csv\n</code></pre>"},{"location":"user-guide/validation/#schema-validation","title":"Schema Validation","text":"<p>Each table has a YAML schema defining its structure:</p> <pre><code># Example from patient_schema.yaml\ncolumns:\n  - name: patient_id\n    data_type: VARCHAR\n    required: true\n    is_category_column: false\n  - name: sex_category\n    data_type: VARCHAR\n    required: true\n    is_category_column: true\n    permissible_values:\n      - Male\n      - Female\n      - Unknown\n</code></pre>"},{"location":"user-guide/validation/#required-columns","title":"Required Columns","text":"<pre><code># Check which required columns are missing\nif not table.isvalid():\n    missing_cols = [e for e in table.errors if e['type'] == 'missing_column']\n    for error in missing_cols:\n        print(f\"Missing required column: {error['column']}\")\n</code></pre>"},{"location":"user-guide/validation/#data-types","title":"Data Types","text":"<p>CLIFpy validates that columns have appropriate data types: - <code>VARCHAR</code> - String/text data - <code>DATETIME</code> - Timezone-aware datetime - <code>NUMERIC</code> - Numeric values (int or float)</p>"},{"location":"user-guide/validation/#category-validation","title":"Category Validation","text":"<p>Standardized categories ensure consistency across institutions:</p> <pre><code># Example: Validating location categories in ADT\nvalid_locations = ['ed', 'ward', 'stepdown', 'icu', 'procedural', \n                   'l&amp;d', 'hospice', 'psych', 'rehab', 'radiology', \n                   'dialysis', 'other']\n\n# Check for invalid categories\ncategory_errors = [e for e in table.errors \n                   if e['type'] == 'invalid_category']\n</code></pre>"},{"location":"user-guide/validation/#range-validation","title":"Range Validation","text":"<p>Clinical values are checked against reasonable ranges:</p> <pre><code># Example: Vital signs ranges\nranges = {\n    'heart_rate': (0, 300),\n    'sbp': (0, 300),\n    'dbp': (0, 200),\n    'temp_c': (25, 44),\n    'spo2': (50, 100)\n}\n\n# Identify out-of-range values\nrange_errors = [e for e in table.errors \n                if e['type'] == 'out_of_range']\n</code></pre>"},{"location":"user-guide/validation/#timezone-validation","title":"Timezone Validation","text":"<p>All datetime columns must be timezone-aware:</p> <pre><code># Check timezone issues\ntz_errors = [e for e in table.errors \n             if 'timezone' in e.get('message', '').lower()]\n\nif tz_errors:\n    print(\"Datetime columns must be timezone-aware\")\n    print(\"Consider reloading with explicit timezone:\")\n    print(\"table = TableClass.from_file('/data', 'parquet', timezone='US/Central')\")\n</code></pre>"},{"location":"user-guide/validation/#duplicate-detection","title":"Duplicate Detection","text":"<p>Duplicates are identified based on composite keys:</p> <pre><code># Check for duplicates\nduplicate_errors = [e for e in table.errors \n                    if e['type'] == 'duplicate_rows']\n\nif duplicate_errors:\n    for error in duplicate_errors:\n        print(f\"Found {error['count']} duplicate rows\")\n        print(f\"Composite keys: {error['keys']}\")\n</code></pre>"},{"location":"user-guide/validation/#missing-data-analysis","title":"Missing Data Analysis","text":"<p>CLIFpy analyzes missing data patterns:</p> <pre><code># Get missing data statistics\nsummary = table.get_summary()\nif 'missing_data' in summary:\n    print(\"Columns with missing data:\")\n    for col, count in summary['missing_data'].items():\n        pct = (count / summary['num_rows']) * 100\n        print(f\"  {col}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"user-guide/validation/#custom-validation","title":"Custom Validation","text":"<p>Tables may include specific validation logic:</p> <pre><code># Example: Labs table validates reference ranges\n# Example: Medications validates dose units match drug\n# Example: Respiratory support validates device/mode combinations\n</code></pre>"},{"location":"user-guide/validation/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate after loading - Catch issues early</li> <li>Review all error types - Don't just check if valid</li> <li>Save validation reports - Keep audit trail</li> <li>Fix data at source - Update extraction/ETL process</li> <li>Document exceptions - Some errors may be acceptable</li> </ol>"},{"location":"user-guide/validation/#handling-validation-errors","title":"Handling Validation Errors","text":""},{"location":"user-guide/validation/#option-1-fix-and-reload","title":"Option 1: Fix and Reload","text":"<pre><code># Identify issues\ntable.validate()\nerrors_df = pd.DataFrame(table.errors)\nerrors_df.to_csv('validation_errors.csv', index=False)\n\n# Fix source data based on errors\n# Then reload\ntable = TableClass.from_file('/fixed_data', 'parquet')\ntable.validate()\n</code></pre>"},{"location":"user-guide/validation/#option-2-filter-invalid-records","title":"Option 2: Filter Invalid Records","text":"<pre><code># Remove records with invalid categories\nvalid_categories = ['Male', 'Female', 'Unknown']\ncleaned_df = table.df[table.df['sex_category'].isin(valid_categories)]\n\n# Create new table instance with cleaned data\ntable = TableClass(data=cleaned_df, timezone='US/Central')\n</code></pre>"},{"location":"user-guide/validation/#option-3-document-and-proceed","title":"Option 3: Document and Proceed","text":"<pre><code># For acceptable validation errors\nif not table.isvalid():\n    # Document why proceeding despite errors\n    with open('validation_notes.txt', 'w') as f:\n        f.write(f\"Proceeding with {len(table.errors)} known issues:\\n\")\n        f.write(\"- Missing optional columns\\n\")\n        f.write(\"- Historical data outside current ranges\\n\")\n</code></pre>"},{"location":"user-guide/validation/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about timezone handling</li> <li>Explore table-specific guides</li> <li>See practical examples</li> </ul>"},{"location":"user-guide/waterfall/","title":"Respiratory Support Waterfall Processing","text":"<p>The waterfall method provides a sophisticated data cleaning and imputation pipeline for respiratory support data, ensuring continuous and complete ventilator records for analysis.</p>"},{"location":"user-guide/waterfall/#overview","title":"Overview","text":"<p>The waterfall processing transforms raw, sparse respiratory support data into a dense, analysis-ready dataset by: - Creating hourly scaffolds for continuous timelines - Inferring missing device and mode information - Forward-filling numeric values within ventilation episodes - Applying clinical logic and heuristics</p> <p></p>"},{"location":"user-guide/waterfall/#usage","title":"Usage","text":""},{"location":"user-guide/waterfall/#basic-usage","title":"Basic Usage","text":"<pre><code>from clifpy.tables.respiratory_support import RespiratorySupport\n\n# Load your respiratory support data\nresp_support = RespiratorySupport.from_file(\n    data_directory=\"/path/to/data\",\n    filetype=\"parquet\"\n)\n\n# Apply waterfall processing\nprocessed = resp_support.waterfall()\n\n# The result is a new RespiratorySupport instance\nprocessed.validate()  # Validate the processed data\ndf = processed.df     # Access the DataFrame\n</code></pre>"},{"location":"user-guide/waterfall/#advanced-options","title":"Advanced Options","text":"<pre><code># Enable backward fill for numeric values\nprocessed = resp_support.waterfall(bfill=True)\n\n# Use a different ID column for grouping\nprocessed = resp_support.waterfall(id_col=\"patient_id\")\n\n# Get just the DataFrame\ndf = resp_support.waterfall(return_dataframe=True)\n\n# Silent mode (no progress messages)\nprocessed = resp_support.waterfall(verbose=False)\n</code></pre>"},{"location":"user-guide/waterfall/#timezone-handling","title":"Timezone Handling","text":"<p>The waterfall function expects data in UTC timezone. If your data is in a different timezone, it will be automatically converted:</p> <pre><code># Your data is in US/Eastern\nprocessed = resp_support.waterfall(verbose=True)\n# Output: \"Converting timezone from US/Eastern to UTC for waterfall processing\"\n</code></pre>"},{"location":"user-guide/waterfall/#processing-pipeline","title":"Processing Pipeline","text":"<p>The waterfall processing consists of five phases:</p>"},{"location":"user-guide/waterfall/#phase-0-initialize-build-hourly-scaffold","title":"Phase 0: Initialize &amp; Build Hourly Scaffold","text":"<ul> <li>Lower-case all text columns - Standardizes device and mode names for consistent matching</li> <li>Coerce numeric setters to floats - Ensures all numeric columns have proper data types</li> <li>Scale FiO\u2082 if needed - Corrects common documentation errors (e.g., 40 \u2192 0.40)</li> <li>Create hourly scaffold - Inserts synthetic rows at HH:59:59 for each hour</li> <li>Ensures every patient has a dense, regular timeline</li> <li>Vital for plots, per-hour metrics, and length-of-stay calculations</li> <li>These scaffold rows serve as \"landing spots\" for forward-fill operations</li> </ul>"},{"location":"user-guide/waterfall/#phase-1-devicemode-heuristics","title":"Phase 1: Device/Mode Heuristics","text":"<p>Applies intelligent rules to repair missing device and mode labels:</p> <ul> <li>IMV from mode strings - Infers invasive mechanical ventilation from mode categories</li> <li>Look-ahead/behind logic - Uses surrounding context to fill gaps</li> <li>Device-specific repairs:</li> <li>BiPAP device name standardization</li> <li>Nasal cannula PEEP guards</li> <li>Mode category inference</li> <li>Data cleaning:</li> <li>Removes rows with no usable information</li> <li>Handles timestamp duplicates</li> <li>Prioritizes non-NIPPV entries when duplicates exist</li> </ul>"},{"location":"user-guide/waterfall/#phase-2-hierarchical-ids","title":"Phase 2: Hierarchical IDs","text":"<p>Creates four nested run-length identifiers within each encounter:</p> <pre><code>device_cat_id \u2192 device_id \u2192 mode_cat_id \u2192 mode_name_id\n</code></pre> <ul> <li>Each ID increments when its label or parent ID changes</li> <li>Enables tracking of ventilation episodes and mode transitions</li> <li>Provides grouping keys for the numeric fill phase</li> </ul> <p>Example progression: <pre><code>Time  Device_Category  Device_Cat_ID  Mode_Category  Mode_Cat_ID\n10:00 IMV             1              AC/VC          1\n11:00 IMV             1              AC/VC          1\n12:00 IMV             1              SIMV           2  \u2190 mode change\n13:00 NIPPV           2              CPAP           3  \u2190 device change\n</code></pre></p>"},{"location":"user-guide/waterfall/#phase-3-numeric-waterfall","title":"Phase 3: Numeric Waterfall","text":"<p>Performs intelligent filling of numeric values within each <code>mode_name_id</code> block:</p> <ul> <li>Forward-fill by default - Carries last known settings forward</li> <li>Optional backward-fill - When <code>bfill=True</code>, also fills backwards</li> <li>Special handling:</li> <li>FiO\u2082 defaults to 0.21 for room air</li> <li>Tidal volume blanked for pressure support modes</li> <li>Trach collar acts as a \"breaker\" for fills</li> <li>Preserves clinical logic - Respects mode transitions and device changes</li> </ul>"},{"location":"user-guide/waterfall/#phase-4-final-tidy-up","title":"Phase 4: Final Tidy-up","text":"<ul> <li>De-duplicate rows - Ensures one row per timestamp</li> <li>Sort chronologically - Orders by encounter and time</li> <li>Forward-fill tracheostomy flag - Carries trach status through entire encounter</li> <li>Clean up helper columns - Removes temporary calculation fields</li> <li>Preserve scaffold indicator - <code>is_scaffold</code> column marks synthetic rows</li> </ul>"},{"location":"user-guide/waterfall/#output-format","title":"Output Format","text":"<p>The processed DataFrame includes:</p>"},{"location":"user-guide/waterfall/#original-columns-cleaned-and-filled","title":"Original Columns (cleaned and filled)","text":"<ul> <li>All original respiratory support columns</li> <li>Numeric values filled within appropriate contexts</li> <li>Categorical values standardized and inferred</li> </ul>"},{"location":"user-guide/waterfall/#new-columns","title":"New Columns","text":"<ul> <li><code>device_cat_id</code> - Device category episode ID</li> <li><code>device_id</code> - Device instance episode ID  </li> <li><code>mode_cat_id</code> - Mode category episode ID</li> <li><code>mode_name_id</code> - Mode instance episode ID</li> <li><code>is_scaffold</code> - Boolean flag for synthetic hourly rows</li> </ul>"},{"location":"user-guide/waterfall/#example","title":"Example","text":"<pre><code>import pandas as pd\nfrom clifpy.tables.respiratory_support import RespiratorySupport\n\n# Sample data with gaps\ndata = pd.DataFrame({\n    'hospitalization_id': ['H001', 'H001', 'H001'],\n    'recorded_dttm': pd.to_datetime([\n        '2023-01-01 10:30', \n        '2023-01-01 14:15',  # 4-hour gap\n        '2023-01-01 15:00'\n    ]).tz_localize('UTC'),\n    'device_category': ['imv', None, 'imv'],  # Missing value\n    'fio2_set': [0.5, None, 0.4],\n    'peep_set': [8, None, 10],\n    # ... other columns\n})\n\n# Create instance and process\nrs = RespiratorySupport(data=data)\nprocessed = rs.waterfall()\n\n# Result will have:\n# - Hourly rows at 11:59:59, 12:59:59, 13:59:59\n# - Device category filled for 14:15 row\n# - FiO\u2082 and PEEP carried forward through gaps\n# - Hierarchical IDs tracking ventilation episodes\n</code></pre>"},{"location":"user-guide/waterfall/#clinical-considerations","title":"Clinical Considerations","text":"<ol> <li>Scaffold rows are synthetic - Filter by <code>is_scaffold == False</code> for actual observations</li> <li>Fills respect clinical boundaries - Values don't cross mode/device transitions</li> <li>Room air defaults - FiO\u2082 set to 0.21 (21%) for room air observations</li> <li>Tracheostomy persistence - Once documented, carries through admission</li> </ol>"},{"location":"user-guide/waterfall/#performance-notes","title":"Performance Notes","text":"<ul> <li>Processing time scales with number of encounters and data density</li> <li>Memory usage increases due to hourly scaffold creation</li> <li>Consider processing in batches for very large datasets</li> </ul>"},{"location":"user-guide/waterfall/#see-also","title":"See Also","text":"<ul> <li>Respiratory Support Table - Table schema and validation</li> <li>Wide Dataset Creation - Creating analysis-ready datasets</li> <li>Data Validation - Understanding validation errors</li> <li>Timezone Handling - Working with different timezones</li> </ul>"},{"location":"user-guide/wide-dataset/","title":"Wide Dataset Creation","text":"<p>Transform your CLIF data into analysis-ready datasets with automatic table joining, pivoting, and temporal aggregation.</p>"},{"location":"user-guide/wide-dataset/#what-you-can-do","title":"What You Can Do","text":"<p>With wide dataset functionality, you can:</p> <ul> <li>Create event-level datasets - Join vitals, labs, medications, and assessments into a single timeline</li> <li>Aggregate to time windows - Convert irregular measurements into hourly, daily, or custom time buckets</li> <li>Prepare for machine learning - Generate consistent time-series features with configurable aggregation methods</li> </ul> <p>Key benefit: Turn complex, multi-table ICU data into analysis-ready DataFrames in just a few lines of code.</p>"},{"location":"user-guide/wide-dataset/#quick-start","title":"Quick Start","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize\nco = ClifOrchestrator(\n    data_directory='/path/to/clif/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Create wide dataset\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['hemoglobin', 'sodium', 'creatinine']\n    },\n    sample=True  # Start with 20 random patients\n)\n\n# Access the result\nwide_df = co.wide_df\nprint(f\"Created dataset: {wide_df.shape}\")\n</code></pre> <p>That's it! You now have a wide dataset with vitals and labs joined by patient and timestamp.</p>"},{"location":"user-guide/wide-dataset/#understanding-the-two-step-process","title":"Understanding the Two-Step Process","text":"<p>Wide dataset creation is a two-step process. You can use one or both steps depending on your needs:</p>"},{"location":"user-guide/wide-dataset/#step-1-create-wide-dataset-event-level-data","title":"Step 1: Create Wide Dataset (Event-Level Data)","text":"<p>This joins multiple tables into a single DataFrame with one row per measurement event.</p> <pre><code>co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin']\n    }\n)\n# Result stored in: co.wide_df\n</code></pre> <p>Output: Every measurement is a separate row - Patient has 10 heart rate measurements in hour 1 \u2192 10 rows - Each row has timestamp, patient info, and measurement values</p> <p>When to use: - Need every individual measurement - Analyzing measurement frequency or timing - Creating custom aggregations</p>"},{"location":"user-guide/wide-dataset/#step-2-aggregate-to-time-windows-optional","title":"Step 2: Aggregate to Time Windows (Optional)","text":"<p>Convert irregular measurements into consistent time windows with aggregation.</p> <pre><code>hourly_df = co.convert_wide_to_hourly(\n    aggregation_config={\n        'mean': ['heart_rate', 'sbp'],\n        'median': ['hemoglobin']\n    },\n    hourly_window=1  # 1-hour windows\n)\n</code></pre> <p>Output: One row per time window - Patient has 10 heart rate measurements in hour 1 \u2192 1 row with average - Consistent time intervals (hourly, 6-hour, daily, etc.)</p> <p>When to use: - Training machine learning models - Calculating clinical scores (SOFA, APACHE) - Analyzing trends over time - Need consistent time intervals</p>"},{"location":"user-guide/wide-dataset/#which-approach-do-i-need","title":"Which Approach Do I Need?","text":"Your Goal Use This Explore all individual measurements Wide dataset only (skip aggregation) Train ML models (LSTM, XGBoost, etc.) Wide dataset + hourly aggregation Calculate SOFA or other clinical scores Wide dataset + hourly aggregation Analyze daily trends Wide dataset + daily aggregation (24-hour windows) Custom analysis Wide dataset (then aggregate yourself)"},{"location":"user-guide/wide-dataset/#choosing-your-data","title":"Choosing Your Data","text":""},{"location":"user-guide/wide-dataset/#what-is-category_filters","title":"What is category_filters?","text":"<p><code>category_filters</code> tells the function which measurements you want from each table.</p> <p>Simple rule: List the measurement names you care about.</p> <pre><code>category_filters = {\n    'vitals': ['heart_rate', 'sbp', 'dbp', 'spo2', 'map'],\n    'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium', 'creatinine'],\n    'medication_admin_continuous': ['norepinephrine', 'propofol', 'fentanyl']\n}\n</code></pre>"},{"location":"user-guide/wide-dataset/#how-to-find-available-measurements","title":"How to Find Available Measurements","text":"<p>Not sure what's in your data? Check before creating the wide dataset:</p> <pre><code># Load tables first\nco.initialize(['vitals', 'labs', 'medication_admin_continuous'])\n\n# See what's available\nprint(\"Vitals:\", co.vitals.df['vital_category'].unique())\nprint(\"Labs:\", co.labs.df['lab_category'].unique())\nprint(\"Medications:\", co.medication_admin_continuous.df['med_category'].unique())\n\n# Then create wide dataset with what you found\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],  # Pick from the list above\n        'labs': ['hemoglobin', 'sodium']\n    }\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#special-case-respiratory-support-other-wide-tables","title":"Special Case: Respiratory Support &amp; Other wide tables","text":"<p><code>respiratory_support</code> is already in wide format, so you specify column names instead of categories:</p> <pre><code>category_filters = {\n    'vitals': ['heart_rate', 'spo2'],  # Category values (pivoted)\n    'respiratory_support': [            # Column names (already wide)\n        'device_category',\n        'fio2_set',\n        'peep_set',\n        'tidal_volume_set'\n    ]\n}\n</code></pre> <p>How to find respiratory support columns:</p> <pre><code>co.initialize(['respiratory_support'])\nprint(\"Available columns:\", co.respiratory_support.df.columns.tolist())\n</code></pre>"},{"location":"user-guide/wide-dataset/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/wide-dataset/#workflow-1-explore-vital-signs-for-sample-patients","title":"Workflow 1: Explore Vital Signs for Sample Patients","text":"<p>Goal: Understand vital sign patterns in a small sample</p> <pre><code>co.create_wide_dataset(\n    tables_to_load=['vitals'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'dbp', 'map', 'spo2', 'temp_c']\n    },\n    sample=True  # Just 20 random patients\n)\n\n# Explore the data\nwide_df = co.wide_df\nprint(f\"Patients: {wide_df['patient_id'].nunique()}\")\nprint(f\"Events: {len(wide_df)}\")\nprint(f\"Date range: {wide_df['event_time'].min()} to {wide_df['event_time'].max()}\")\n\n# Example analysis: measurements per patient\nmeasurements_per_patient = wide_df.groupby('patient_id').size()\nprint(f\"Average measurements per patient: {measurements_per_patient.mean():.0f}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#workflow-2-prepare-hourly-data-for-machine-learning","title":"Workflow 2: Prepare Hourly Data for Machine Learning","text":"<p>Goal: Create hourly aggregated features for predictive modeling</p> <pre><code># Step 1: Create wide dataset\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs', 'medication_admin_continuous'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'map', 'spo2', 'temp_c'],\n        'labs': ['hemoglobin', 'wbc', 'platelet', 'creatinine', 'bilirubin'],\n        'medication_admin_continuous': ['norepinephrine', 'vasopressin', 'propofol']\n    }\n)\n\n# Step 2: Aggregate to hourly windows\nhourly_df = co.convert_wide_to_hourly(\n    aggregation_config={\n        # Vital signs: mean for typical, max/min for extremes\n        'mean': ['heart_rate', 'sbp', 'map', 'temp_c'],\n        'max': ['heart_rate', 'sbp'],\n        'min': ['spo2', 'map'],\n\n        # Labs: max for values where peaks matter\n        'max': ['creatinine', 'bilirubin'],\n        'median': ['hemoglobin', 'wbc', 'platelet'],\n\n        # Medications: boolean for presence\n        'boolean': ['norepinephrine', 'vasopressin', 'propofol']\n    },\n    hourly_window=1,\n    fill_gaps=True  # Create complete time series for ML\n)\n\n# Now ready for sklearn, PyTorch, etc.\nprint(f\"ML-ready dataset: {hourly_df.shape}\")\nprint(f\"Features: {[c for c in hourly_df.columns if any(s in c for s in ['_mean', '_max', '_min', '_boolean'])]}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#workflow-3-calculate-sofa-scores","title":"Workflow 3: Calculate SOFA Scores","text":"<p>Goal: Compute hourly SOFA scores for patients</p> <pre><code># Create wide dataset with SOFA-required variables\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs', 'medication_admin_continuous', 'respiratory_support'],\n    category_filters={\n        'vitals': ['sbp', 'map'],\n        'labs': ['platelet', 'bilirubin', 'creatinine'],\n        'medication_admin_continuous': ['norepinephrine', 'dopamine', 'epinephrine'],\n        'respiratory_support': ['fio2_set', 'pao2']\n    }\n)\n\n# Aggregate to hourly windows (SOFA typically calculated hourly)\nhourly_df = co.convert_wide_to_hourly(\n    aggregation_config={\n        'min': ['sbp', 'map', 'platelet'],  # Worst values\n        'max': ['bilirubin', 'creatinine'],  # Worst values\n        'max': ['norepinephrine', 'dopamine', 'epinephrine'],  # Maximum doses\n        'mean': ['fio2_set', 'pao2']\n    },\n    hourly_window=1\n)\n\n# Now calculate SOFA scores using hourly aggregated data\n# (Use co.compute_sofa_scores() or custom SOFA calculation)\n</code></pre>"},{"location":"user-guide/wide-dataset/#workflow-4-analyze-daily-summaries-for-outcomes","title":"Workflow 4: Analyze Daily Summaries for Outcomes","text":"<p>Goal: Daily statistics for outcome analysis</p> <pre><code># Create wide dataset\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'temp_c'],\n        'labs': ['hemoglobin', 'wbc', 'creatinine']\n    }\n)\n\n# Aggregate to DAILY windows (24 hours)\ndaily_df = co.convert_wide_to_hourly(\n    aggregation_config={\n        'mean': ['heart_rate', 'sbp', 'temp_c'],\n        'max': ['heart_rate', 'temp_c'],\n        'min': ['sbp'],\n        'median': ['hemoglobin', 'wbc', 'creatinine'],\n        'first': ['hemoglobin'],  # Admission value\n        'last': ['creatinine']     # Discharge value\n    },\n    hourly_window=24  # 24-hour windows = daily\n)\n\nprint(f\"Daily summaries: {len(daily_df)} patient-days\")\nprint(f\"Average days per patient: {daily_df.groupby('hospitalization_id').size().mean():.1f}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#temporal-aggregation-deep-dive","title":"Temporal Aggregation Deep Dive","text":""},{"location":"user-guide/wide-dataset/#understanding-aggregation-methods","title":"Understanding Aggregation Methods","text":"<p>When you convert to time windows, you specify how to aggregate multiple measurements into one value:</p> Method Use For Example <code>mean</code> Typical vital signs, average doses Mean heart rate over the hour <code>max</code> Worst case, peaks Maximum creatinine (worst kidney function) <code>min</code> Best case, lows Minimum SpO2 (worst oxygenation) <code>median</code> Robust central tendency Median hemoglobin (less affected by outliers) <code>first</code> Initial/admission values First GCS score of the day <code>last</code> Final/discharge values Last assessment before discharge <code>boolean</code> Presence/absence Was norepinephrine given? (1=yes, 0=no) <code>one_hot_encode</code> Categorical variables Convert device types to binary columns"},{"location":"user-guide/wide-dataset/#choosing-window-sizes","title":"Choosing Window Sizes","text":"Window Size Use Case Output Density 1 hour High-resolution analysis, ML models \\~24 rows per day per patient 6 hours Shift-based analysis (morning/afternoon/evening/night) \\~4 rows per day per patient 12 hours Bi-daily patterns \\~2 rows per day per patient 24 hours (daily) Daily summaries, outcome studies \\~1 row per day per patient <pre><code># Hourly (high resolution)\nhourly = co.convert_wide_to_hourly(aggregation_config=config, hourly_window=1)\n\n# 6-hour windows (shift-based)\nshift_based = co.convert_wide_to_hourly(aggregation_config=config, hourly_window=6)\n\n# Daily summaries\ndaily = co.convert_wide_to_hourly(aggregation_config=config, hourly_window=24)\n</code></pre>"},{"location":"user-guide/wide-dataset/#gap-filling-for-machine-learning","title":"Gap Filling for Machine Learning","text":"<p>By default, only windows with data are created (sparse output). For ML models that need complete time series:</p> <pre><code># Dense output - ALL time windows created, gaps filled with NaN\nml_ready = co.convert_wide_to_hourly(\n    aggregation_config={\n        'mean': ['heart_rate', 'sbp'],\n        'boolean': ['norepinephrine']\n    },\n    hourly_window=1,\n    fill_gaps=True  # Creates rows for ALL hours, even without data\n)\n\n# Now every patient has hours 0, 1, 2, 3, ..., N\n# Missing data appears as NaN (ready for imputation)\n</code></pre> <p>When to use <code>fill_gaps</code>: - \u2705 Training LSTM, RNN, or time-series models - \u2705 Need regular time intervals - \u2705 Will apply imputation strategies</p> <p>When NOT to use <code>fill_gaps</code>: - \u274c Descriptive statistics (don't need empty hours) - \u274c Memory-constrained environments (denser data = more rows) - \u274c Will handle missing windows yourself</p>"},{"location":"user-guide/wide-dataset/#using-encounter-blocks","title":"Using Encounter Blocks","text":"<p>When encounter stitching is enabled, you can group by <code>encounter_block</code> instead of individual hospitalizations:</p> <pre><code># Enable encounter stitching\nco = ClifOrchestrator(\n    data_directory='/path/to/data',\n    stitch_encounter=True,\n    stitch_time_interval=6  # Link hospitalizations within 6 hours\n)\n\n# Create wide dataset\nco.create_wide_dataset(tables_to_load=['vitals'], category_filters={'vitals': ['heart_rate']})\n\n# Aggregate by ENCOUNTER BLOCK (treats linked hospitalizations as one continuous stay)\nhourly_encounter = co.convert_wide_to_hourly(\n    aggregation_config={'mean': ['heart_rate']},\n    id_name='encounter_block'  # Group by encounter, not hospitalization\n)\n\n# Compare: encounter blocks vs individual hospitalizations\nprint(f\"Encounter blocks: {hourly_encounter['encounter_block'].nunique()}\")\nprint(f\"Max hours in encounter: {hourly_encounter.groupby('encounter_block')['window_number'].max().max()}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#performance-optimization","title":"Performance &amp; Optimization","text":""},{"location":"user-guide/wide-dataset/#when-to-optimize","title":"When to Optimize","text":"<p>For most use cases (\\&lt; 1,000 hospitalizations, \\&lt; 1M rows), default settings work fine. Only optimize if: - Processing &gt; 10,000 hospitalizations - Experiencing memory errors - Processing takes &gt; 10 minutes</p>"},{"location":"user-guide/wide-dataset/#check-system-resources-first","title":"Check System Resources First","text":"<pre><code># Always start here for large datasets\nresources = co.get_sys_resource_info()\nprint(f\"Available RAM: {resources['memory_available_gb']:.1f} GB\")\nprint(f\"Recommended threads: {resources['max_recommended_threads']}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#optimization-strategies","title":"Optimization Strategies","text":"<p>1. Use Sampling for Development</p> <pre><code># Test with sample first\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    sample=True  # 20 random patients\n)\n</code></pre> <p>2. Adjust Batch Size for Large Datasets</p> <pre><code># For &gt; 10,000 hospitalizations\nco.create_wide_dataset(\n    tables_to_load=['vitals'],\n    category_filters={'vitals': ['heart_rate', 'sbp']},\n    batch_size=500,  # Smaller batches = less memory per batch\n    memory_limit='8GB'\n)\n</code></pre> <p>3. Configure Memory and Threads</p> <pre><code># Based on system resources\nresources = co.get_sys_resource_info(print_summary=False)\n\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    memory_limit=f\"{int(resources['memory_available_gb'] * 0.7)}GB\",  # Use 70% of available\n    threads=resources['max_recommended_threads']\n)\n</code></pre> <p>4. Save Large Datasets to Disk</p> <pre><code># Don't keep huge DataFrames in memory\nco.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    save_to_data_location=True,\n    output_format='parquet',  # Compressed format\n    output_filename='my_wide_dataset',\n    return_dataframe=False  # Don't keep in memory if not needed\n)\n# Later, load as needed\nimport pandas as pd\nwide_df = pd.read_parquet('/path/to/data/my_wide_dataset.parquet')\n</code></pre>"},{"location":"user-guide/wide-dataset/#performance-guidelines","title":"Performance Guidelines","text":"Hospitalizations Batch Size Memory Limit Expected Time \\&lt; 1,000 -1 (no batching) 4GB \\&lt; 1 minute 1,000 - 10,000 1000 8GB 2-10 minutes &gt; 10,000 500 16GB+ 10-60 minutes"},{"location":"user-guide/wide-dataset/#quick-reference","title":"Quick Reference","text":""},{"location":"user-guide/wide-dataset/#create_wide_dataset-parameters","title":"create_wide_dataset() Parameters","text":"Parameter Type Default Description <code>tables_to_load</code> List[str] None Tables to include: 'vitals', 'labs', 'medication_admin_continuous', 'patient_assessments', 'respiratory_support', 'crrt_therapy' <code>category_filters</code> Dict None Measurements to include from each table <code>sample</code> bool False Use 20 random patients for testing <code>hospitalization_ids</code> List[str] None Specific patient IDs to process <code>cohort_df</code> DataFrame None Time windows (columns: 'hospitalization_id', 'start_time', 'end_time') <code>batch_size</code> int 1000 Hospitalizations per batch (-1 = no batching) <code>memory_limit</code> str None DuckDB memory limit (e.g., '8GB') <code>threads</code> int None Processing threads (None = auto) <code>output_format</code> str 'dataframe' 'dataframe', 'csv', or 'parquet' <code>save_to_data_location</code> bool False Save to file <p>Access result: <code>co.wide_df</code></p>"},{"location":"user-guide/wide-dataset/#convert_wide_to_hourly-parameters","title":"convert_wide_to_hourly() Parameters","text":"Parameter Type Default Description <code>aggregation_config</code> Dict Required Maps methods to columns: <code>{'mean': ['hr'], 'max': ['sbp']}</code> <code>wide_df</code> DataFrame None Input data (uses <code>co.wide_df</code> if None) <code>id_name</code> str 'hospitalization_id' Grouping column: 'hospitalization_id' or 'encounter_block' <code>hourly_window</code> int 1 Window size in hours (1-72) <code>fill_gaps</code> bool False Create rows for all windows (True = dense, False = sparse) <code>memory_limit</code> str '4GB' DuckDB memory limit <code>batch_size</code> int Auto Batch size (auto-determined if None) <p>Available aggregation methods: <code>max</code>, <code>min</code>, <code>mean</code>, <code>median</code>, <code>first</code>, <code>last</code>, <code>boolean</code>, <code>one_hot_encode</code></p>"},{"location":"user-guide/wide-dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/wide-dataset/#memory-errors","title":"Memory Errors","text":"<p>Symptom: \"Memory limit exceeded\" or system crash</p> <p>Solutions:</p> <pre><code># 1. Check available memory first\nresources = co.get_sys_resource_info()\n\n# 2. Reduce batch size\nco.create_wide_dataset(\n    batch_size=250,  # Smaller batches\n    memory_limit='4GB'\n)\n\n# 3. Start with sample\nco.create_wide_dataset(sample=True)  # Test with 20 patients first\n\n# 4. Save to disk instead of memory\nco.create_wide_dataset(\n    save_to_data_location=True,\n    output_format='parquet',\n    return_dataframe=False\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#empty-or-no-results","title":"Empty or No Results","text":"<p>Symptom: <code>co.wide_df</code> is None or has no rows</p> <p>Solutions:</p> <pre><code># 1. Check if tables are loaded\nprint(\"Loaded tables:\", co.get_loaded_tables())\n\n# 2. Check if data exists\nco.initialize(['vitals'])\nprint(\"Vitals data:\", len(co.vitals.df))\nprint(\"Available categories:\", co.vitals.df['vital_category'].unique())\n\n# 3. Verify category names match exactly (case-sensitive!)\nco.create_wide_dataset(\n    tables_to_load=['vitals'],\n    category_filters={\n        'vitals': ['heart_rate']  # Not 'Heart_Rate' or 'heartrate'\n    }\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#slow-performance","title":"Slow Performance","text":"<p>Symptom: Takes very long to process</p> <p>Solutions:</p> <pre><code># 1. Use fewer tables/categories\nco.create_wide_dataset(\n    tables_to_load=['vitals'],  # Start with one table\n    category_filters={'vitals': ['heart_rate', 'sbp']}  # Just a few categories\n)\n\n# 2. Optimize threads\nresources = co.get_sys_resource_info(print_summary=False)\nco.create_wide_dataset(\n    tables_to_load=['vitals'],\n    category_filters={'vitals': ['heart_rate']},\n    threads=resources['max_recommended_threads']\n)\n\n# 3. Use sampling for testing\nco.create_wide_dataset(sample=True)  # Much faster\n</code></pre>"},{"location":"user-guide/wide-dataset/#wrong-column-names-in-output","title":"Wrong Column Names in Output","text":"<p>Symptom: Expected 'heart_rate' but got 'heart_rate_mean'</p> <p>Explanation: After temporal aggregation, columns get suffixes based on aggregation method: - <code>mean</code> \u2192 <code>_mean</code> - <code>max</code> \u2192 <code>_max</code> - <code>min</code> \u2192 <code>_min</code> - <code>boolean</code> \u2192 <code>_boolean</code></p> <pre><code># Wide dataset (no aggregation) - original column names\nco.create_wide_dataset(...)\nprint(co.wide_df.columns)  # ['event_time', 'heart_rate', 'sbp', ...]\n\n# After hourly aggregation - columns get suffixes\nhourly = co.convert_wide_to_hourly(aggregation_config={'mean': ['heart_rate', 'sbp']})\nprint(hourly.columns)  # ['window_start_dttm', 'heart_rate_mean', 'sbp_mean', ...]\n</code></pre>"},{"location":"user-guide/wide-dataset/#next-steps","title":"Next Steps","text":"<ul> <li>Data Validation - Ensure data quality</li> <li>Individual Table Guides - Detailed table documentation</li> <li>Orchestrator Guide - Advanced orchestrator features</li> <li>Timezone Handling - Multi-site data considerations</li> </ul>"}]}