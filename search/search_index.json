{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>CLIFpy is a Python package that implements the Common Longitudinal ICU data Format (CLIF) specification. It provides a standardized interface for working with critical care data in the CLIF format, enabling healthcare researchers and data scientists to analyze ICU data across different healthcare systems.</p> <ul> <li> <p>\ud83d\ude80 Getting Started</p> <p>Install with <code>pip</code> and get started with CLIFpy with these tutorials.</p> </li> <li> <p>\ud83d\udcd6 User Guide</p> <p>In depth explanation and discussion of the concepts and working of different features available in CLIFpy.</p> </li> <li> <p>\ud83d\udd28 How-to Guides</p> <p>Practical guides to help you achieve specific goals. Take a look at these guides to learn how to use CLIFpy to solve real-world problems.</p> </li> <li> <p>\ud83d\udcc4 API Reference</p> <p>Technical descriptions of how CLIFpy classes and methods work.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>CLIFpy is released under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"BaseTable/","title":"BaseTable class for pyCLIF tables.","text":"<p>This module provides the base class that all pyCLIF table classes inherit from. It handles common functionality including data loading, validation, and reporting.</p>"},{"location":"BaseTable/#overview","title":"Overview","text":"<p>The <code>BaseTable</code> class is the foundational component of all table classes in clifpy. It implements the common functionality that every CLIF table needs, following an inheritance pattern where specific tables (patient, hospitalization, adt, etc.) inherit from it.</p>"},{"location":"BaseTable/#design-pattern","title":"Design Pattern","text":"<p>BaseTable implements the Template Method Pattern where: - Common behavior is defined in the base class - Specific behavior is implemented in child classes - Extensibility is provided through method overriding</p>"},{"location":"BaseTable/#core-responsibilities","title":"Core Responsibilities","text":""},{"location":"BaseTable/#1-data-management","title":"1. Data Management","text":"<pre><code># Stores the actual data\nself.df: pd.DataFrame = None\n\n# Configuration\nself.data_directory: str    # Path to data files\nself.filetype: str         # File format (parquet, csv, etc.)\nself.timezone: str         # Timezone for datetime conversions\nself.output_directory: str # Where to save validation outputs\n</code></pre>"},{"location":"BaseTable/#2-schema-management","title":"2. Schema Management","text":"<pre><code># Automatically loads YAML schema based on table name\nself.schema: Dict = None  # Loaded from clifpy/schemas/{table_name}_schema.yaml\n</code></pre>"},{"location":"BaseTable/#3-validation-system","title":"3. Validation System","text":"<pre><code>self.errors: List[Dict] = []  # Stores validation errors and warnings\n</code></pre>"},{"location":"BaseTable/#4-logging-system","title":"4. Logging System","text":"<pre><code>self.logger: logging.Logger  # Per-table logger that writes to output directory\n</code></pre>"},{"location":"BaseTable/#key-methods","title":"Key Methods","text":""},{"location":"BaseTable/#constructor-__init__","title":"Constructor (<code>__init__</code>)","text":"<pre><code>def __init__(\n    self,\n    data_directory: str,\n    filetype: str, \n    timezone: str,\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n)\n</code></pre> <p>Parameters: - <code>data_directory</code>: Path to directory containing data files - <code>filetype</code>: File format (\"parquet\", \"csv\", etc.) - <code>timezone</code>: Timezone for datetime columns (default: \"UTC\")  - <code>output_directory</code>: Where to save validation outputs (optional) - <code>data</code>: Pre-loaded DataFrame (optional)</p> <p>What it does: 1. Stores configuration parameters 2. Sets up output directory (creates if doesn't exist) 3. Determines table name from class name 4. Sets up logging system 5. Loads YAML schema for the table 6. If data provided, automatically runs validation</p>"},{"location":"BaseTable/#class-method-from_file","title":"Class Method: <code>from_file()</code>","text":"<pre><code>@classmethod\ndef from_file(\n    cls,\n    data_directory: str,\n    filetype: str,\n    timezone: str = \"UTC\", \n    output_directory: Optional[str] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n)\n</code></pre> <p>Alternative constructor that loads data from files with additional options: - <code>sample_size</code>: Limit number of rows to load - <code>columns</code>: Only load specific columns - <code>filters</code>: Apply filters during loading</p> <p>Example: <pre><code>patient_table = patient.from_file(\n    data_directory=\"/path/to/data\",\n    filetype=\"parquet\",\n    timezone=\"US/Eastern\",\n    sample_size=1000  # Load only first 1000 rows\n)\n</code></pre></p>"},{"location":"BaseTable/#validation-methods","title":"Validation Methods","text":""},{"location":"BaseTable/#validate","title":"<code>validate()</code>","text":"<p>Runs comprehensive validation on the loaded data: - Schema validation (required columns, data types, categories) - Enhanced validation (missing data, duplicates, statistics) - Table-specific validation (can be overridden by child classes)</p>"},{"location":"BaseTable/#isvalid-bool","title":"<code>isvalid() -&gt; bool</code>","text":"<p>Returns <code>True</code> if no errors were found in the last validation run.</p> <pre><code>if table.isvalid():\n    print(\"\u2705 Data passed all validations!\")\nelse:\n    print(f\"\u274c Found {len(table.errors)} validation issues\")\n</code></pre>"},{"location":"BaseTable/#how-tables-inherit-from-basetable","title":"How Tables Inherit from BaseTable","text":""},{"location":"BaseTable/#basic-inheritance-pattern","title":"Basic Inheritance Pattern","text":"<pre><code>class patient(BaseTable):\n    \"\"\"Patient table with demographic information.\"\"\"\n\n    def __init__(self, data_directory: str = None, filetype: str = None, \n                 timezone: str = \"UTC\", output_directory: Optional[str] = None,\n                 data: Optional[pd.DataFrame] = None):\n        # Handle backward compatibility\n        if data_directory is None and filetype is None and data is not None:\n            data_directory = \".\"\n            filetype = \"parquet\" \n\n        # Call parent constructor\n        super().__init__(\n            data_directory=data_directory,\n            filetype=filetype, \n            timezone=timezone,\n            output_directory=output_directory,\n            data=data\n        )\n\n    # Add patient-specific methods\n    def get_demographics_summary(self):\n        \"\"\"Return demographic breakdown of patients.\"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"BaseTable/#table-specific-methods","title":"Table-Specific Methods","text":"<p>Child classes can add methods specific to their domain:</p> <pre><code># hospitalization.py\nclass hospitalization(BaseTable):\n    def get_mortality_rate(self) -&gt; float:\n        \"\"\"Calculate in-hospital mortality rate.\"\"\"\n        if 'discharge_category' not in self.df.columns:\n            return 0.0\n        total = len(self.df)\n        expired = len(self.df[self.df['discharge_category'] == 'Expired'])\n        return (expired / total) * 100 if total &gt; 0 else 0.0\n\n    def calculate_length_of_stay(self) -&gt; pd.DataFrame:\n        \"\"\"Calculate length of stay for each hospitalization.\"\"\"\n        # Implementation here\n        pass\n\n# adt.py  \nclass adt(BaseTable):\n    def get_location_categories(self) -&gt; List[str]:\n        \"\"\"Return unique location categories.\"\"\"\n        if 'location_category' not in self.df.columns:\n            return []\n        return self.df['location_category'].dropna().unique().tolist()\n\n    def filter_by_location_category(self, location: str) -&gt; pd.DataFrame:\n        \"\"\"Filter records by location category (e.g., 'icu', 'ward').\"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"BaseTable/#validation-flow","title":"Validation Flow","text":"<p>When you create a table instance, BaseTable automatically:</p> <ol> <li>Schema Loading: Reads <code>{table_name}_schema.yaml</code> from the schemas directory</li> <li>Logging Setup: Creates log files in the output directory</li> <li>Data Validation (if data provided):</li> <li>\u2705 Required columns - Ensures all mandatory columns are present</li> <li>\u2705 Data types - Validates column data types match schema</li> <li>\u2705 Categorical values - Checks values against permitted categories</li> <li>\u2705 Datetime timezones - Validates timezone-aware datetime columns</li> <li>\u2705 Missing data analysis - Calculates missing data statistics</li> <li>\u2705 Duplicate detection - Checks composite keys for uniqueness</li> <li>\u2705 Statistical analysis - Generates summaries and skewness analysis</li> <li>\u2705 Unit validation - For tables like vitals/labs, validates measurement units</li> <li>\u2705 Numeric ranges - Checks values fall within expected clinical ranges</li> </ol>"},{"location":"BaseTable/#output-files-generated","title":"Output Files Generated","text":"<p>BaseTable creates several files during validation in the output directory:</p> File Type Example Purpose Log files <code>validation_log_patient.log</code> Detailed validation logs with timestamps Missing data <code>missing_data_stats_patient.csv</code> Missing value counts and percentages Statistics <code>summary_statistics_patient.csv</code> Q1, Q3, median for numeric columns Skewness <code>skewness_analysis_patient.csv</code> Distribution analysis for numeric columns Validation errors <code>validation_errors_patient.csv</code> Summary of all validation issues"},{"location":"BaseTable/#usage-examples","title":"Usage Examples","text":""},{"location":"BaseTable/#method-1-direct-instantiation-with-data","title":"Method 1: Direct Instantiation with Data","text":"<pre><code># When you already have a DataFrame\npatient_table = patient(\n    data_directory=\"./data\",      # Required for schema/logging\n    filetype=\"parquet\",           # Required for metadata\n    timezone=\"UTC\",               # Timezone for datetime columns  \n    output_directory=\"./output\",  # Where to save validation files\n    data=my_dataframe            # Your pre-loaded DataFrame\n)\n</code></pre>"},{"location":"BaseTable/#method-2-load-from-file","title":"Method 2: Load from File","text":"<pre><code># Load data from files\npatient_table = patient.from_file(\n    data_directory=\"./data\",\n    filetype=\"parquet\", \n    timezone=\"US/Eastern\",\n    columns=['patient_id', 'age_at_admission', 'sex_category'],  # Only load specific columns\n    sample_size=5000  # Only load first 5000 rows\n)\n</code></pre>"},{"location":"BaseTable/#method-3-demo-data-recommended-for-learning","title":"Method 3: Demo Data (Recommended for Learning)","text":"<pre><code># Use built-in demo datasets\nfrom clifpy.data import load_demo_patient\n\npatient_table = load_demo_patient()  # Uses Method 1 internally\n</code></pre>"},{"location":"BaseTable/#all-methods-result-in-same-capabilities","title":"All Methods Result in Same Capabilities:","text":"<pre><code># Check validation status\nprint(f\"Valid: {patient_table.isvalid()}\")\nprint(f\"Errors: {len(patient_table.errors)}\")  \nprint(f\"Records: {len(patient_table.df)}\")\n\n# Access the data\ndf = patient_table.df\nprint(df.head())\n\n# Use table-specific methods (if implemented)\nif hasattr(patient_table, 'get_demographics_summary'):\n    demographics = patient_table.get_demographics_summary()\n</code></pre>"},{"location":"BaseTable/#benefits-of-this-design","title":"Benefits of This Design","text":""},{"location":"BaseTable/#1-code-reuse","title":"1. Code Reuse","text":"<ul> <li>All tables get validation, logging, and schema loading automatically</li> <li>No duplicate code across table implementations</li> <li>Consistent behavior across all table types</li> </ul>"},{"location":"BaseTable/#2-consistency","title":"2. Consistency","text":"<ul> <li>Same API across all table types: <code>validate()</code>, <code>isvalid()</code>, <code>from_file()</code></li> <li>Standardized output file formats and naming conventions</li> <li>Uniform error handling and logging</li> </ul>"},{"location":"BaseTable/#3-extensibility","title":"3. Extensibility","text":"<ul> <li>Easy to add new tables by inheriting from BaseTable</li> <li>Can override specific methods for table-specific behavior</li> <li>Template method pattern allows customization while preserving structure</li> </ul>"},{"location":"BaseTable/#4-separation-of-concerns","title":"4. Separation of Concerns","text":"<ul> <li>BaseTable: Infrastructure (validation, logging, I/O, schema management)</li> <li>Child classes: Domain-specific methods and business logic</li> <li>Validator module: Reusable validation functions</li> <li>Schema files: Data structure definitions</li> </ul>"},{"location":"BaseTable/#5-maintainability","title":"5. Maintainability","text":"<ul> <li>Changes to validation logic automatically apply to all tables</li> <li>Schema changes are managed in separate YAML files  </li> <li>Logging and error handling centralized in one place</li> </ul>"},{"location":"BaseTable/#advanced-features","title":"Advanced Features","text":""},{"location":"BaseTable/#custom-validation","title":"Custom Validation","text":"<p>Tables can override validation methods for specific requirements:</p> <pre><code>class vitals(BaseTable):\n    def _run_table_specific_validations(self):\n        \"\"\"Add vitals-specific validation rules.\"\"\"\n        super()._run_table_specific_validations()\n\n        # Custom validation for vital signs ranges\n        if 'vital_category' in self.df.columns and 'vital_value' in self.df.columns:\n            # Check for physiologically impossible values\n            extreme_values = self.df[\n                (self.df['vital_category'] == 'heart_rate') &amp; \n                ((self.df['vital_value'] &lt; 0) | (self.df['vital_value'] &gt; 300))\n            ]\n            if not extreme_values.empty:\n                self.errors.append({\n                    \"type\": \"extreme_vital_values\",\n                    \"message\": f\"Found {len(extreme_values)} extreme heart rate values\",\n                    \"count\": len(extreme_values)\n                })\n</code></pre>"},{"location":"BaseTable/#custom-output-methods","title":"Custom Output Methods","text":"<pre><code>class hospitalization(BaseTable):\n    def save_mortality_report(self, filename: str = None):\n        \"\"\"Save detailed mortality analysis to file.\"\"\"\n        if filename is None:\n            filename = os.path.join(self.output_directory, f'mortality_report_{self.table_name}.csv')\n\n        mortality_data = self.analyze_mortality_by_demographics()\n        mortality_data.to_csv(filename, index=False)\n        self.logger.info(f\"Saved mortality report to {filename}\")\n</code></pre>"},{"location":"BaseTable/#best-practices","title":"Best Practices","text":""},{"location":"BaseTable/#1-when-to-inherit-from-basetable","title":"1. When to Inherit from BaseTable","text":"<ul> <li>\u2705 Always for CLIF table implementations</li> <li>\u2705 When you need validation and schema management</li> <li>\u2705 For tables that will be used in production workflows</li> </ul>"},{"location":"BaseTable/#2-method-naming-conventions","title":"2. Method Naming Conventions","text":"<ul> <li>Use descriptive names: <code>get_mortality_rate()</code> not <code>mortality()</code></li> <li>Follow existing patterns: <code>filter_by_*()</code>, <code>get_*()</code>, <code>calculate_*()</code></li> <li>Return appropriate types: DataFrames for subsets, numbers for metrics</li> </ul>"},{"location":"BaseTable/#3-error-handling","title":"3. Error Handling","text":"<pre><code>def custom_method(self):\n    \"\"\"Custom analysis method with proper error handling.\"\"\"\n    try:\n        if self.df is None or self.df.empty:\n            self.logger.warning(\"No data available for analysis\")\n            return None\n\n        # Your analysis here\n        result = self.df.groupby('category').mean()\n\n        self.logger.info(f\"Analysis completed successfully with {len(result)} groups\")\n        return result\n\n    except Exception as e:\n        self.logger.error(f\"Analysis failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"BaseTable/#future-enhancements","title":"Future Enhancements","text":"<p>The BaseTable design supports future enhancements such as:</p> <ul> <li>Caching: Store validation results to avoid re-computation</li> <li>Streaming: Handle large datasets that don't fit in memory  </li> <li>Parallel processing: Run validation checks in parallel</li> <li>Custom validators: Plugin system for domain-specific validation rules</li> <li>Data lineage: Track data transformations and sources</li> <li>Version control: Schema versioning and migration support</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to CLIFpy will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive documentation with MkDocs</li> <li>Enhanced docstrings for all modules and classes</li> <li>API reference documentation</li> <li>User guide and examples</li> </ul>"},{"location":"changelog/#001-2024-01-xx","title":"0.0.1 - 2024-01-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release of CLIFpy</li> <li>Core implementation of CLIF 2.0.0 specification</li> <li>All 9 CLIF table implementations:</li> <li>Patient demographics</li> <li>ADT (Admission, Discharge, Transfer)</li> <li>Hospitalization</li> <li>Laboratory results</li> <li>Vital signs</li> <li>Respiratory support</li> <li>Continuous medication administration</li> <li>Patient assessments</li> <li>Patient positioning</li> <li>Data validation against mCIDE schemas</li> <li>Timezone handling and conversion</li> <li>ClifOrchestrator for multi-table management</li> <li>Comprehensive test suite</li> <li>Demo dataset based on MIMIC-IV</li> <li>Example notebooks</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Load data from CSV or Parquet files</li> <li>Schema-based validation</li> <li>Advanced filtering and querying</li> <li>Clinical calculations</li> <li>Summary statistics and reporting</li> <li>Memory-efficient data loading options</li> </ul>"},{"location":"contributing/","title":"Contributing to CLIFpy","text":"<p>We welcome contributions to CLIFpy! This guide will help you get started.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/CLIFpy.git\ncd CLIFpy\n</code></pre></li> <li>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></li> <li>Install in development mode with all dependencies:    <pre><code>pip install -e \".[docs]\"\n</code></pre></li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a new branch for your feature or fix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and ensure:</p> </li> <li>Code follows the existing style</li> <li>All tests pass</li> <li>New features include tests</li> <li> <p>Documentation is updated</p> </li> <li> <p>Run tests:    <pre><code>pytest tests/\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"feat: add new feature\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add type hints where appropriate</li> <li>Include docstrings for all public functions and classes</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update docstrings for any API changes</li> <li>Add examples to docstrings where helpful</li> <li>Update user guide if adding new features</li> <li>Build docs locally to verify:   <pre><code>mkdocs serve\n</code></pre></li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure all tests pass before submitting PR</li> <li>Aim for high test coverage</li> <li>Use pytest fixtures for common test data</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits format: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation changes - <code>test:</code> - Test additions or changes - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance tasks</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue for bugs or feature requests</li> <li>Join discussions in existing issues</li> <li>Reach out to maintainers if you need help</li> </ul> <p>Thank you for contributing to CLIFpy!</p>"},{"location":"prompts/","title":"Prompts","text":""},{"location":"prompts/#medication_admin_continuous","title":"<code>medication_admin_continuous</code>","text":"<p>Implement the following tests in tests/tables/test_medication_admin_continuous.py following instructions in their docstrings: - test_acceptable_dose_unit_patterns - test_standardize_dose_unit_pattern - test_convert_dose_to_same_units</p> <p>Use pytest features to make the tests more readable and less verbose. If helpful, follow the pattern of existing tests in the same file.</p> <p>the overall pattern looks good but please improve on the following issues: 1. the test data are not easily human readable and verificable. Can you create them as .csv's under  tests/fixtures? the csv would look like the output df which contains the original df and the newly generated appended columns. e.g. for test_convert_dose_to_same_units, the csv would have the following columns: ['med_dose', 'med_dose_unit_clean', 'weight_kg', 'med_dose_converted', 'med_dose_unit_converted'] 2. within the same test function, we are testing multiple scenarios. Does it make sense to split them up into separate test functions? I'm not sure about it either way. I think splitting them up seems like a good pattern but I worry about code duplication and overcomplicating things. Can you advise and implement what you think would be a preferred strategy, esp. taking into account the first issue above?</p> <p>it looks good overall! but would it make sense to consolidate related test data into the same csv's and  just add a new column like 'case' to differentiate between the different scenarios? i.e. I think i'd make sense for me to be able to view the valid and invalid unit cases at the same time in the same csv, and we can just load the csv as a whole and then filter by 'case'. Of course, for tests that are rather distinct and would require different columns, they should be still in different csv's.</p> <ol> <li>remove dose_unit_patterns.csv as that test does not necessarily need a tabular structure and the  test data can be easily understood as two lists written in the .py file directly.</li> <li>rename the csv to be aligned with the test names</li> <li>modify column names in the csv's to be exactly aligned with the dataframes, e.g. 'expected_clean' should be 'med_dose_unit_clean'; 'expected_dose_converted' should just be 'med_dose_converted'</li> <li>remove unnecessary columns in the csv's such as 'med_category' which does not impact unit conversion at all. </li> </ol> <p>Based on these two files: @tests/tables/test_medication_admin_continuous.py                                            \u2502 @clifpy/tables/medication_admin_continuous.py 1. Add a test(s) for when no med_df was ever provided (self.df was not even populated), triggering the ValueError(\"No data provided\") 2. Flesh out the doc strings for all functions</p>"},{"location":"wide_dataset/","title":"Wide Dataset Creation with pyCLIF","text":"<p>The pyCLIF library now includes powerful functionality for creating wide datasets by joining multiple CLIF tables with automatic pivoting of category-based data. This feature is designed to replicate and enhance the sophisticated wide dataset creation logic from the original notebook while making it reusable and configurable.</p>"},{"location":"wide_dataset/#overview","title":"Overview","text":"<p>The wide dataset functionality allows you to: - Automatically join multiple CLIF tables (patient, hospitalization, ADT, and optional tables) - Pivot category-based data from vitals, labs, medications, and assessments - Sample or filter hospitalizations for targeted analysis - Handle time-based alignment of events across different tables - Save results in multiple formats (DataFrame, CSV, Parquet)</p>"},{"location":"wide_dataset/#quick-start","title":"Quick Start","text":"<pre><code>from pyclif import CLIF\n\n# Initialize with your data\nclif = CLIF(\n    data_dir=\"/path/to/CLIF_data\",\n    filetype='parquet',\n    timezone=\"US/Eastern\"\n)\n\n# Create a wide dataset (tables auto-loaded as needed)\nwide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['map', 'heart_rate', 'spo2'],\n        'labs': ['hemoglobin', 'wbc', 'sodium']\n    },\n    sample=True  # 20 random hospitalizations\n)\n</code></pre>"},{"location":"wide_dataset/#core-functionality","title":"Core Functionality","text":""},{"location":"wide_dataset/#base-tables-always-included","title":"Base Tables (Always Included)","text":"<ul> <li>patient: Demographics and patient information</li> <li>hospitalization: Admission/discharge details</li> <li>adt: Admission, discharge, and transfer events</li> </ul>"},{"location":"wide_dataset/#optional-tables-user-specified","title":"Optional Tables (User-Specified)","text":"<ul> <li>vitals: Vital signs (pivoted by <code>vital_category</code>)</li> <li>labs: Laboratory results (pivoted by <code>lab_category</code>)</li> <li>medication_admin_continuous: Continuous medications (pivoted by <code>med_category</code>)</li> <li>patient_assessments: Clinical assessments (pivoted by <code>assessment_category</code>)</li> <li>respiratory_support: Respiratory support data</li> </ul>"},{"location":"wide_dataset/#parameters","title":"Parameters","text":""},{"location":"wide_dataset/#create_wide_dataset-parameters","title":"<code>create_wide_dataset()</code> Parameters","text":"Parameter Type Default Description <code>optional_tables</code> List[str] None List of optional tables to include <code>category_filters</code> Dict[str, List[str]] None Categories to pivot for each table <code>sample</code> bool False If True, randomly select 20 hospitalizations <code>hospitalization_ids</code> List[str] None Specific hospitalization IDs to include <code>output_format</code> str 'dataframe' Output format: 'dataframe', 'csv', 'parquet' <code>save_to_data_location</code> bool False Save output to data directory <code>output_filename</code> str None Custom filename (auto-generated if None) <code>auto_load</code> bool True Automatically load missing tables"},{"location":"wide_dataset/#usage-examples","title":"Usage Examples","text":""},{"location":"wide_dataset/#example-1-sample-mode","title":"Example 1: Sample Mode","text":"<p>Create a wide dataset with 20 random hospitalizations:</p> <pre><code>wide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['map', 'heart_rate', 'spo2', 'respiratory_rate'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium']\n    },\n    sample=True,\n    save_to_data_location=True,\n    output_format='parquet'\n)\n</code></pre>"},{"location":"wide_dataset/#example-2-specific-hospitalizations","title":"Example 2: Specific Hospitalizations","text":"<p>Target specific encounters for analysis:</p> <pre><code>target_ids = ['12345', '67890', '11111']\nwide_df = clif.create_wide_dataset(\n    hospitalization_ids=target_ids,\n    optional_tables=['medication_admin_continuous', 'patient_assessments'],\n    category_filters={\n        'medication_admin_continuous': ['norepinephrine', 'propofol', 'fentanyl'],\n        'patient_assessments': ['gcs_total', 'rass', 'sbt_delivery_pass_fail']\n    },\n    output_filename='targeted_encounters'\n)\n</code></pre>"},{"location":"wide_dataset/#example-3-comprehensive-dataset","title":"Example 3: Comprehensive Dataset","text":"<p>Create a full wide dataset with all optional tables:</p> <pre><code>wide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs', 'medication_admin_continuous', 'patient_assessments'],\n    category_filters={\n        'vitals': ['map', 'heart_rate', 'spo2', 'respiratory_rate', 'temp_c'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium', 'creatinine'],\n        'medication_admin_continuous': ['norepinephrine', 'epinephrine', 'propofol'],\n        'patient_assessments': ['gcs_total', 'rass', 'sbt_delivery_pass_fail']\n    },\n    save_to_data_location=True\n)\n</code></pre>"},{"location":"wide_dataset/#available-categories","title":"Available Categories","text":""},{"location":"wide_dataset/#vitals-categories","title":"Vitals Categories","text":"<p>Common vital sign categories include: - <code>map</code>, <code>heart_rate</code>, <code>sbp</code>, <code>dbp</code>, <code>spo2</code>, <code>respiratory_rate</code>, <code>temp_c</code>, <code>weight_kg</code>, <code>height_cm</code></p>"},{"location":"wide_dataset/#labs-categories","title":"Labs Categories","text":"<p>Common laboratory categories include: - <code>hemoglobin</code>, <code>wbc</code>, <code>sodium</code>, <code>potassium</code>, <code>creatinine</code>, <code>bun</code>, <code>glucose</code>, <code>lactate</code></p>"},{"location":"wide_dataset/#medication-categories","title":"Medication Categories","text":"<p>Common continuous medication categories include: - <code>norepinephrine</code>, <code>epinephrine</code>, <code>phenylephrine</code>, <code>vasopressin</code>, <code>dopamine</code> - <code>propofol</code>, <code>fentanyl</code>, <code>midazolam</code>, <code>lorazepam</code>, <code>morphine</code></p>"},{"location":"wide_dataset/#assessment-categories","title":"Assessment Categories","text":"<p>Common assessment categories include: - <code>gcs_total</code>, <code>rass</code>, <code>sbt_delivery_pass_fail</code>, <code>sat_delivery_pass_fail</code> - <code>sbt_screen_pass_fail</code>, <code>sat_screen_pass_fail</code></p>"},{"location":"wide_dataset/#output-structure","title":"Output Structure","text":"<p>The resulting wide dataset includes:</p>"},{"location":"wide_dataset/#core-columns","title":"Core Columns","text":"<ul> <li>Patient demographics (<code>patient_id</code>, <code>sex_category</code>, <code>race_category</code>, etc.)</li> <li>Hospitalization details (<code>hospitalization_id</code>, <code>admission_dttm</code>, <code>discharge_dttm</code>, etc.)</li> <li>Event timing (<code>event_time</code>, <code>day_number</code>, <code>hosp_id_day_key</code>)</li> <li>Location information (from ADT table)</li> </ul>"},{"location":"wide_dataset/#pivoted-columns","title":"Pivoted Columns","text":"<ul> <li>Individual columns for each specified category (e.g., <code>map</code>, <code>heart_rate</code>, <code>norepinephrine</code>)</li> <li>Values aligned by timestamp and hospitalization</li> </ul>"},{"location":"wide_dataset/#time-based-features","title":"Time-Based Features","text":"<ul> <li><code>day_number</code>: Sequential day number within each hospitalization</li> <li><code>hosp_id_day_key</code>: Unique identifier combining hospitalization and day</li> <li><code>event_time</code>: Timestamp for each record</li> </ul>"},{"location":"wide_dataset/#auto-loading-feature","title":"Auto-Loading Feature","text":"<p>The function automatically loads required tables if they haven't been loaded yet:</p> <pre><code># No need to manually load tables\nclif = CLIF(data_dir=\"/path/to/data\", filetype='parquet')\n\n# Tables will be auto-loaded as needed\nwide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs']  # These will be loaded automatically\n)\n</code></pre>"},{"location":"wide_dataset/#error-handling","title":"Error Handling","text":"<p>The function includes robust error handling:</p> <ul> <li>Missing tables: Warns and skips if optional tables aren't available</li> <li>Missing columns: Handles alternative timestamp column names</li> <li>Missing categories: Adds NaN columns for standard assessments/medications</li> <li>Empty data: Gracefully handles cases where no data remains after filtering</li> </ul>"},{"location":"wide_dataset/#performance-considerations","title":"Performance Considerations","text":""},{"location":"wide_dataset/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Use <code>sample=True</code> for testing and development</li> <li>Specify <code>hospitalization_ids</code> for targeted analysis</li> <li>Use <code>category_filters</code> to limit pivoted columns</li> </ul>"},{"location":"wide_dataset/#output-management","title":"Output Management","text":"<ul> <li>Use <code>save_to_data_location=True</code> for large datasets</li> <li>Choose <code>output_format='parquet'</code> for better compression</li> <li>Set <code>output_format='dataframe'</code> only for immediate analysis</li> </ul>"},{"location":"wide_dataset/#implementation-details","title":"Implementation Details","text":""},{"location":"wide_dataset/#temporal-alignment","title":"Temporal Alignment","text":"<p>The function creates a unified timeline by: 1. Collecting all unique timestamps from included tables 2. Creating a cartesian product of hospitalizations \u00d7 timestamps 3. Joining table-specific data based on matching timestamps</p>"},{"location":"wide_dataset/#pivoting-logic","title":"Pivoting Logic","text":"<p>Category-based tables are pivoted using DuckDB for performance: - Creates unique combination IDs (<code>hospitalization_id_YYYYMMDDHHMM</code>) - Pivots on category columns using <code>PIVOT</code> SQL operation - Handles missing values and duplicate timestamps</p>"},{"location":"wide_dataset/#data-integration","title":"Data Integration","text":"<p>Tables are joined using a combination of: - Hospitalization IDs for patient-level data - Timestamp-based combo IDs for time-series data - Left joins to preserve all timestamps</p>"},{"location":"wide_dataset/#best-practices","title":"Best Practices","text":"<ol> <li>Start Small: Use <code>sample=True</code> for initial testing</li> <li>Filter Categories: Specify only needed categories to reduce memory usage</li> <li>Save Large Datasets: Use file output for datasets &gt; 1GB</li> <li>Check Data Quality: Validate timestamp alignment and missing values</li> <li>Document Choices: Record which categories and filters were used</li> </ol>"},{"location":"wide_dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"wide_dataset/#common-issues","title":"Common Issues","text":"<p>Memory Errors <pre><code># Use sampling or filtering\nwide_df = clif.create_wide_dataset(sample=True)  # or\nwide_df = clif.create_wide_dataset(hospitalization_ids=small_list)\n</code></pre></p> <p>Missing Columns <pre><code># Check available categories in your data first\nprint(clif.vitals.df['vital_category'].unique())\n</code></pre></p> <p>Empty Results <pre><code># Verify data exists for your filters\nprint(clif.hospitalization.df['hospitalization_id'].nunique())\n</code></pre></p>"},{"location":"wide_dataset/#integration-with-existing-workflow","title":"Integration with Existing Workflow","text":"<p>The wide dataset function is designed to integrate seamlessly with existing pyCLIF workflows:</p> <pre><code># Traditional approach\nclif = CLIF(data_dir=\"/path/to/data\")\nclif.initialize(['patient', 'vitals', 'labs'])\n\n# Enhanced with wide dataset\nwide_df = clif.create_wide_dataset(\n    optional_tables=['vitals', 'labs'],\n    category_filters={'vitals': ['map'], 'labs': ['hemoglobin']}\n)\n\n# Continue with analysis\nanalysis_results = analyze_wide_dataset(wide_df)\n</code></pre> <p>This functionality brings the power of the original notebook's wide dataset creation into a reusable, configurable, and robust function that can be easily integrated into any pyCLIF workflow.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the complete API documentation for CLIFpy, automatically generated from the source code docstrings.</p>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#cliforchestrator","title":"ClifOrchestrator","text":"<p>The main orchestration class for managing multiple CLIF tables with consistent configuration.</p>"},{"location":"api/#basetable","title":"BaseTable","text":"<p>The base class that all CLIF table implementations inherit from, providing common functionality for data loading, validation, and reporting.</p>"},{"location":"api/#table-classes","title":"Table Classes","text":""},{"location":"api/#tables-overview","title":"Tables Overview","text":"<p>Complete API documentation for all CLIF table implementations:</p> <ul> <li>Patient - Patient demographics and identification</li> <li>Adt - Admission, discharge, and transfer events  </li> <li>Hospitalization - Hospital stay information</li> <li>Labs - Laboratory test results</li> <li>Vitals - Vital signs measurements</li> <li>RespiratorySupport - Ventilation and oxygen therapy</li> <li>MedicationAdminContinuous - Continuous medication infusions</li> <li>PatientAssessments - Clinical assessment scores</li> <li>Position - Patient positioning data</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":""},{"location":"api/#utility-functions","title":"Utility Functions","text":"<p>Helper functions for data loading, validation, and I/O operations:</p> <ul> <li>io - Data loading utilities</li> <li>validator - Data validation functions</li> </ul>"},{"location":"api/#quick-links","title":"Quick Links","text":"<ul> <li>ClifOrchestrator API - Multi-table management</li> <li>BaseTable API - Common table functionality</li> <li>Table Classes API - Individual table implementations</li> <li>Utilities API - Helper functions</li> </ul>"},{"location":"api/#usage-example","title":"Usage Example","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\nfrom clifpy.tables import Patient, Labs, Vitals\n\n# Using the orchestrator\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Using individual tables\npatient = Patient.from_file('/path/to/data', 'parquet')\npatient.validate()\n</code></pre>"},{"location":"api/base-table/","title":"BaseTable","text":""},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable","title":"clifpy.tables.base_table.BaseTable","text":"<pre><code>BaseTable(\n    data_directory,\n    filetype,\n    timezone,\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>Base class for all pyCLIF table classes.</p> <p>Provides common functionality for loading data, running validations, and generating reports. All table-specific classes should inherit from this.</p> <p>Attributes:</p> Name Type Description <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>table_name</code> <code>str</code> <p>Name of the table (from class name)</p> <code>df</code> <code>DataFrame</code> <p>The loaded data</p> <code>schema</code> <code>dict</code> <p>The YAML schema for this table</p> <code>errors</code> <code>List[dict]</code> <p>Validation errors from last validation run</p> <code>logger</code> <code>Logger</code> <p>Logger for this table</p> <p>Initialize the BaseTable.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> required <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> required <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> required <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not provided, creates an 'output' directory in the current working directory.</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def __init__(\n    self, \n    data_directory: str,\n    filetype: str,\n    timezone: str,\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the BaseTable.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs.\n            If not provided, creates an 'output' directory in the current working directory.\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Store configuration\n    self.data_directory = data_directory\n    self.filetype = filetype\n    self.timezone = timezone\n\n    # Set output directory\n    if output_directory is None:\n        output_directory = os.path.join(os.getcwd(), 'output')\n    self.output_directory = output_directory\n    os.makedirs(self.output_directory, exist_ok=True)\n\n    # Derive snake_case table name from PascalCase class name\n    # Example: Adt -&gt; adt, RespiratorySupport -&gt; respiratory_support\n    self.table_name = ''.join(['_' + c.lower() if c.isupper() else c for c in self.__class__.__name__]).lstrip('_')\n\n    # Initialize data and validation state\n    self.df: Optional[pd.DataFrame] = data\n    self.errors: List[Dict[str, Any]] = []\n    self.schema: Optional[Dict[str, Any]] = None\n    self._validated: bool = False\n\n    # Setup logging\n    self._setup_logging()\n\n    # Load schema\n    self._load_schema()\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    data_directory,\n    filetype,\n    timezone=\"UTC\",\n    output_directory=None,\n    sample_size=None,\n    columns=None,\n    filters=None,\n)\n</code></pre> <p>Load data from file and create a table instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> required <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> required <code>timezone</code> <code>str</code> <p>Timezone for datetime columns (default: UTC)</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of rows to load</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>Specific columns to load</p> <code>None</code> <code>filters</code> <code>Dict</code> <p>Filters to apply when loading</p> <code>None</code> <p>Returns:</p> Type Description <p>Instance of the table class with loaded data</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, \n    data_directory: str,\n    filetype: str,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n):\n    \"\"\"\n    Load data from file and create a table instance.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns (default: UTC)\n        output_directory (str, optional): Directory for saving output files and logs\n        sample_size (int, optional): Number of rows to load\n        columns (List[str], optional): Specific columns to load\n        filters (Dict, optional): Filters to apply when loading\n\n    Returns:\n        Instance of the table class with loaded data\n    \"\"\"\n    # Derive snake_case table name from PascalCase class name\n    table_name = ''.join(['_' + c.lower() if c.isupper() else c for c in cls.__name__]).lstrip('_')\n\n    # Load data using existing io utility\n    data = load_data(\n        table_name, \n        data_directory, \n        filetype, \n        sample_size=sample_size,\n        columns=columns,\n        filters=filters,\n        site_tz=timezone\n    )\n\n    # Create instance with loaded data\n    return cls(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.get_summary","title":"get_summary","text":"<pre><code>get_summary()\n</code></pre> <p>Get a summary of the table data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Summary statistics and information about the table</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def get_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get a summary of the table data.\n\n    Returns:\n        dict: Summary statistics and information about the table\n    \"\"\"\n    if self.df is None:\n        return {\"status\": \"No data loaded\"}\n\n    summary = {\n        \"table_name\": self.table_name,\n        \"num_rows\": len(self.df),\n        \"num_columns\": len(self.df.columns),\n        \"columns\": list(self.df.columns),\n        \"memory_usage_mb\": self.df.memory_usage(deep=True).sum() / 1024 / 1024,\n        \"validation_run\": self._validated,\n        \"validation_errors\": len(self.errors) if self._validated else None,\n        \"is_valid\": self.isvalid()\n    }\n\n    # Add basic statistics for numeric columns\n    numeric_cols = self.df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) &gt; 0:\n        summary[\"numeric_columns\"] = list(numeric_cols)\n        summary[\"numeric_stats\"] = self.df[numeric_cols].describe().to_dict()\n\n    # Add missing data summary\n    missing_counts = self.df.isnull().sum()\n    if missing_counts.any():\n        summary[\"missing_data\"] = missing_counts[missing_counts &gt; 0].to_dict()\n\n    return summary\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Check if the data is valid based on the last validation run.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if validation has been run and no errors were found,   False if validation found errors or hasn't been run yet</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"\n    Check if the data is valid based on the last validation run.\n\n    Returns:\n        bool: True if validation has been run and no errors were found,\n              False if validation found errors or hasn't been run yet\n    \"\"\"\n    if not self._validated:\n        print(\"Validation has not been run yet. Please call validate() first.\")\n        return False\n    return not self.errors\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.save_summary","title":"save_summary","text":"<pre><code>save_summary()\n</code></pre> <p>Save table summary to a JSON file.</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def save_summary(self):\n    \"\"\"Save table summary to a JSON file.\"\"\"\n    try:\n        import json\n\n        summary = self.get_summary()\n\n        # Save to JSON\n        summary_file = os.path.join(\n            self.output_directory,\n            f'summary_{self.table_name}.json'\n        )\n\n        with open(summary_file, 'w') as f:\n            json.dump(summary, f, indent=2, default=str)\n\n        self.logger.info(f\"Saved summary to {summary_file}\")\n\n    except Exception as e:\n        self.logger.error(f\"Error saving summary: {str(e)}\")\n</code></pre>"},{"location":"api/base-table/#clifpy.tables.base_table.BaseTable.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Run comprehensive validation on the data.</p> <p>This method runs all validation checks including: - Schema validation (required columns, data types, categories) - Missing data analysis - Duplicate checking - Statistical analysis - Table-specific validations (if overridden in child class)</p> Source code in <code>clifpy/tables/base_table.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Run comprehensive validation on the data.\n\n    This method runs all validation checks including:\n    - Schema validation (required columns, data types, categories)\n    - Missing data analysis\n    - Duplicate checking\n    - Statistical analysis\n    - Table-specific validations (if overridden in child class)\n    \"\"\"\n    if self.df is None:\n        self.logger.warning(\"No dataframe to validate\")\n        print(\"No dataframe to validate.\")\n        return\n\n    self.logger.info(\"Starting validation\")\n    self.errors = []\n    self._validated = True\n\n    try:\n        # Run basic schema validation\n        if self.schema:\n            self.logger.info(\"Running schema validation\")\n            schema_errors = validator.validate_dataframe(self.df, self.schema)\n            self.errors.extend(schema_errors)\n\n            if schema_errors:\n                self.logger.warning(f\"Schema validation found {len(schema_errors)} errors\")\n            else:\n                self.logger.info(\"Schema validation passed\")\n\n        # Run enhanced validations (these will be implemented in Phase 3)\n        self._run_enhanced_validations()\n\n        # Run table-specific validations (can be overridden in child classes)\n        self._run_table_specific_validations()\n\n        # Log validation results\n        if not self.errors:\n            self.logger.info(\"Validation completed successfully\")\n            print(\"Validation completed successfully.\")\n        else:\n            self.logger.warning(f\"Validation completed with {len(self.errors)} error(s)\")\n            print(f\"Validation completed with {len(self.errors)} error(s). See `errors` attribute.\")\n\n            # Save errors to CSV\n            self._save_validation_errors()\n\n    except Exception as e:\n        self.logger.error(f\"Error during validation: {str(e)}\")\n        self.errors.append({\n            \"type\": \"validation_error\",\n            \"message\": str(e)\n        })\n</code></pre>"},{"location":"api/orchestrator/","title":"ClifOrchestrator","text":""},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator","title":"clifpy.clif_orchestrator.ClifOrchestrator","text":"<pre><code>ClifOrchestrator(\n    data_directory,\n    filetype=\"csv\",\n    timezone=\"UTC\",\n    output_directory=None,\n)\n</code></pre> <p>Orchestrator class for managing multiple CLIF table objects.</p> <p>This class provides a centralized interface for loading, managing, and validating multiple CLIF tables with consistent configuration.</p> <p>Attributes:</p> Name Type Description <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>patient</code> <code>Patient</code> <p>Patient table object</p> <code>hospitalization</code> <code>Hospitalization</code> <p>Hospitalization table object</p> <code>adt</code> <code>Adt</code> <p>ADT table object</p> <code>labs</code> <code>Labs</code> <p>Labs table object</p> <code>vitals</code> <code>Vitals</code> <p>Vitals table object</p> <code>medication_admin_continuous</code> <code>MedicationAdminContinuous</code> <p>Medication administration table object</p> <code>patient_assessments</code> <code>PatientAssessments</code> <p>Patient assessments table object</p> <code>respiratory_support</code> <code>RespiratorySupport</code> <p>Respiratory support table object</p> <code>position</code> <code>Position</code> <p>Position table object</p> <p>Initialize the ClifOrchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> required <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>'csv'</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs. If not provided, creates an 'output' directory in the current working directory.</p> <code>None</code> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str,\n    filetype: str = 'csv',\n    timezone: str = 'UTC',\n    output_directory: Optional[str] = None\n):\n    \"\"\"\n    Initialize the ClifOrchestrator.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs.\n            If not provided, creates an 'output' directory in the current working directory.\n    \"\"\"\n    self.data_directory = data_directory\n    self.filetype = filetype\n    self.timezone = timezone\n\n    # Set output directory (same logic as BaseTable)\n    if output_directory is None:\n        output_directory = os.path.join(os.getcwd(), 'output')\n    self.output_directory = output_directory\n    os.makedirs(self.output_directory, exist_ok=True)\n\n    # Initialize all table attributes to None\n    self.patient = None\n    self.hospitalization = None\n    self.adt = None\n    self.labs = None\n    self.vitals = None\n    self.medication_admin_continuous = None\n    self.patient_assessments = None\n    self.respiratory_support = None\n    self.position = None\n\n    print('ClifOrchestrator initialized.')\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.convert_wide_to_hourly","title":"convert_wide_to_hourly","text":"<pre><code>convert_wide_to_hourly(\n    wide_df,\n    aggregation_config,\n    memory_limit=\"4GB\",\n    temp_directory=None,\n    batch_size=None,\n)\n</code></pre> <p>Convert wide dataset to hourly aggregation using DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>wide_df</code> <code>DataFrame</code> <p>Wide dataset from create_wide_dataset()</p> required <code>aggregation_config</code> <code>Dict[str, List[str]]</code> <p>Dict mapping aggregation methods to columns Example: {     'mean': ['heart_rate', 'sbp'],     'max': ['spo2'],     'min': ['map'],     'median': ['glucose'],     'first': ['gcs_total'],     'last': ['assessment_value'],     'boolean': ['norepinephrine'],     'one_hot_encode': ['device_category'] }</p> required <code>memory_limit</code> <code>str</code> <p>DuckDB memory limit (e.g., '4GB', '8GB')</p> <code>'4GB'</code> <code>temp_directory</code> <code>Optional[str]</code> <p>Directory for DuckDB temp files</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Process in batches if specified</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Hourly aggregated DataFrame with nth_hour column</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def convert_wide_to_hourly(\n    self,\n    wide_df: pd.DataFrame,\n    aggregation_config: Dict[str, List[str]],\n    memory_limit: str = '4GB',\n    temp_directory: Optional[str] = None,\n    batch_size: Optional[int] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert wide dataset to hourly aggregation using DuckDB.\n\n    Parameters:\n        wide_df: Wide dataset from create_wide_dataset()\n        aggregation_config: Dict mapping aggregation methods to columns\n            Example: {\n                'mean': ['heart_rate', 'sbp'],\n                'max': ['spo2'],\n                'min': ['map'],\n                'median': ['glucose'],\n                'first': ['gcs_total'],\n                'last': ['assessment_value'],\n                'boolean': ['norepinephrine'],\n                'one_hot_encode': ['device_category']\n            }\n        memory_limit: DuckDB memory limit (e.g., '4GB', '8GB')\n        temp_directory: Directory for DuckDB temp files\n        batch_size: Process in batches if specified\n\n    Returns:\n        Hourly aggregated DataFrame with nth_hour column\n    \"\"\"\n    from clifpy.utils.wide_dataset import convert_wide_to_hourly\n\n    return convert_wide_to_hourly(\n        wide_df=wide_df,\n        aggregation_config=aggregation_config,\n        memory_limit=memory_limit,\n        temp_directory=temp_directory,\n        batch_size=batch_size\n    )\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.create_wide_dataset","title":"create_wide_dataset","text":"<pre><code>create_wide_dataset(\n    tables_to_load=None,\n    category_filters=None,\n    sample=False,\n    hospitalization_ids=None,\n    cohort_df=None,\n    output_format=\"dataframe\",\n    save_to_data_location=False,\n    output_filename=None,\n    return_dataframe=True,\n    batch_size=1000,\n    memory_limit=None,\n    threads=None,\n    show_progress=True,\n)\n</code></pre> <p>Create wide time-series dataset using DuckDB for high performance.</p> <p>Parameters:</p> Name Type Description Default <code>tables_to_load</code> <code>Optional[List[str]]</code> <p>List of tables to include (e.g., ['vitals', 'labs'])</p> <code>None</code> <code>category_filters</code> <code>Optional[Dict[str, List[str]]]</code> <p>Dict of categories to pivot for each table Example: {     'vitals': ['heart_rate', 'sbp', 'spo2'],     'labs': ['hemoglobin', 'sodium'],     'respiratory_support': ['device_category'] }</p> <code>None</code> <code>sample</code> <code>bool</code> <p>If True, use 20 random hospitalizations</p> <code>False</code> <code>hospitalization_ids</code> <code>Optional[List[str]]</code> <p>Specific hospitalization IDs to include</p> <code>None</code> <code>cohort_df</code> <code>Optional[DataFrame]</code> <p>DataFrame with time windows for filtering</p> <code>None</code> <code>output_format</code> <code>str</code> <p>'dataframe', 'csv', or 'parquet'</p> <code>'dataframe'</code> <code>save_to_data_location</code> <code>bool</code> <p>Save output to data directory</p> <code>False</code> <code>output_filename</code> <code>Optional[str]</code> <p>Custom filename for output</p> <code>None</code> <code>return_dataframe</code> <code>bool</code> <p>Return DataFrame even when saving</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Number of hospitalizations per batch</p> <code>1000</code> <code>memory_limit</code> <code>Optional[str]</code> <p>DuckDB memory limit (e.g., '8GB')</p> <code>None</code> <code>threads</code> <code>Optional[int]</code> <p>Number of threads for DuckDB</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Show progress bars</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>Wide dataset as DataFrame or None</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def create_wide_dataset(\n    self,\n    tables_to_load: Optional[List[str]] = None,\n    category_filters: Optional[Dict[str, List[str]]] = None,\n    sample: bool = False,\n    hospitalization_ids: Optional[List[str]] = None,\n    cohort_df: Optional[pd.DataFrame] = None,\n    output_format: str = 'dataframe',\n    save_to_data_location: bool = False,\n    output_filename: Optional[str] = None,\n    return_dataframe: bool = True,\n    batch_size: int = 1000,\n    memory_limit: Optional[str] = None,\n    threads: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Create wide time-series dataset using DuckDB for high performance.\n\n    Parameters:\n        tables_to_load: List of tables to include (e.g., ['vitals', 'labs'])\n        category_filters: Dict of categories to pivot for each table\n            Example: {\n                'vitals': ['heart_rate', 'sbp', 'spo2'],\n                'labs': ['hemoglobin', 'sodium'],\n                'respiratory_support': ['device_category']\n            }\n        sample: If True, use 20 random hospitalizations\n        hospitalization_ids: Specific hospitalization IDs to include\n        cohort_df: DataFrame with time windows for filtering\n        output_format: 'dataframe', 'csv', or 'parquet'\n        save_to_data_location: Save output to data directory\n        output_filename: Custom filename for output\n        return_dataframe: Return DataFrame even when saving\n        batch_size: Number of hospitalizations per batch\n        memory_limit: DuckDB memory limit (e.g., '8GB')\n        threads: Number of threads for DuckDB\n        show_progress: Show progress bars\n\n    Returns:\n        Wide dataset as DataFrame or None\n    \"\"\"\n    # Import the utility function\n    from clifpy.utils.wide_dataset import create_wide_dataset as _create_wide\n\n    # Auto-load base tables if not loaded\n    if self.patient is None:\n        print(\"Loading patient table...\")\n        self.load_table('patient')\n    if self.hospitalization is None:\n        print(\"Loading hospitalization table...\")\n        self.load_table('hospitalization')\n    if self.adt is None:\n        print(\"Loading adt table...\")\n        self.load_table('adt')\n\n    # Load optional tables only if not already loaded\n    if tables_to_load:\n        for table_name in tables_to_load:\n            if getattr(self, table_name, None) is None:\n                print(f\"Loading {table_name} table...\")\n                try:\n                    self.load_table(table_name)\n                except Exception as e:\n                    print(f\"Warning: Could not load {table_name}: {e}\")\n\n    # Call utility function with self as clif_instance\n    return _create_wide(\n        clif_instance=self,\n        optional_tables=tables_to_load,\n        category_filters=category_filters,\n        sample=sample,\n        hospitalization_ids=hospitalization_ids,\n        cohort_df=cohort_df,\n        output_format=output_format,\n        save_to_data_location=save_to_data_location,\n        output_filename=output_filename,\n        return_dataframe=return_dataframe,\n        batch_size=batch_size,\n        memory_limit=memory_limit,\n        threads=threads,\n        show_progress=show_progress\n    )\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_loaded_tables","title":"get_loaded_tables","text":"<pre><code>get_loaded_tables()\n</code></pre> <p>Return list of currently loaded table names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of loaded table names</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_loaded_tables(self) -&gt; List[str]:\n    \"\"\"\n    Return list of currently loaded table names.\n\n    Returns:\n        List[str]: List of loaded table names\n    \"\"\"\n    loaded = []\n    for table_name in ['patient', 'hospitalization', 'adt', 'labs', 'vitals',\n                      'medication_admin_continuous', 'patient_assessments',\n                      'respiratory_support', 'position']:\n        if getattr(self, table_name) is not None:\n            loaded.append(table_name)\n    return loaded\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_sys_resource_info","title":"get_sys_resource_info","text":"<pre><code>get_sys_resource_info(print_summary=True)\n</code></pre> <p>Get system resource information including CPU, memory, and practical thread limits.</p> <p>Parameters:</p> Name Type Description Default <code>print_summary</code> <code>bool</code> <p>Whether to print a formatted summary</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing system resource information:</p> <code>Dict[str, Any]</code> <ul> <li>cpu_count_physical: Number of physical CPU cores</li> </ul> <code>Dict[str, Any]</code> <ul> <li>cpu_count_logical: Number of logical CPU cores</li> </ul> <code>Dict[str, Any]</code> <ul> <li>cpu_usage_percent: Current CPU usage percentage</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_total_gb: Total RAM in GB</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_available_gb: Available RAM in GB</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_used_gb: Used RAM in GB</li> </ul> <code>Dict[str, Any]</code> <ul> <li>memory_usage_percent: Memory usage percentage</li> </ul> <code>Dict[str, Any]</code> <ul> <li>process_threads: Number of threads used by current process</li> </ul> <code>Dict[str, Any]</code> <ul> <li>max_recommended_threads: Recommended max threads for optimal performance</li> </ul> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_sys_resource_info(self, print_summary: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get system resource information including CPU, memory, and practical thread limits.\n\n    Parameters:\n        print_summary (bool): Whether to print a formatted summary\n\n    Returns:\n        Dict containing system resource information:\n        - cpu_count_physical: Number of physical CPU cores\n        - cpu_count_logical: Number of logical CPU cores\n        - cpu_usage_percent: Current CPU usage percentage\n        - memory_total_gb: Total RAM in GB\n        - memory_available_gb: Available RAM in GB\n        - memory_used_gb: Used RAM in GB\n        - memory_usage_percent: Memory usage percentage\n        - process_threads: Number of threads used by current process\n        - max_recommended_threads: Recommended max threads for optimal performance\n    \"\"\"\n    # Get current process\n    current_process = psutil.Process()\n\n    # CPU information\n    cpu_count_physical = psutil.cpu_count(logical=False)\n    cpu_count_logical = psutil.cpu_count(logical=True)\n    cpu_usage_percent = psutil.cpu_percent(interval=1)\n\n    # Memory information\n    memory = psutil.virtual_memory()\n    memory_total_gb = memory.total / (1024**3)\n    memory_available_gb = memory.available / (1024**3)\n    memory_used_gb = memory.used / (1024**3)\n    memory_usage_percent = memory.percent\n\n    # Thread information\n    process_threads = current_process.num_threads()\n    max_recommended_threads = cpu_count_physical  # Conservative recommendation\n\n    resource_info = {\n        'cpu_count_physical': cpu_count_physical,\n        'cpu_count_logical': cpu_count_logical,\n        'cpu_usage_percent': cpu_usage_percent,\n        'memory_total_gb': memory_total_gb,\n        'memory_available_gb': memory_available_gb,\n        'memory_used_gb': memory_used_gb,\n        'memory_usage_percent': memory_usage_percent,\n        'process_threads': process_threads,\n        'max_recommended_threads': max_recommended_threads\n    }\n\n    if print_summary:\n        print(\"=\" * 50)\n        print(\"SYSTEM RESOURCES\")\n        print(\"=\" * 50)\n        print(f\"CPU Cores (Physical): {cpu_count_physical}\")\n        print(f\"CPU Cores (Logical):  {cpu_count_logical}\")\n        print(f\"CPU Usage:            {cpu_usage_percent:.1f}%\")\n        print(\"-\" * 50)\n        print(f\"Total RAM:            {memory_total_gb:.1f} GB\")\n        print(f\"Available RAM:        {memory_available_gb:.1f} GB\")\n        print(f\"Used RAM:             {memory_used_gb:.1f} GB\")\n        print(f\"Memory Usage:         {memory_usage_percent:.1f}%\")\n        print(\"-\" * 50)\n        print(f\"Process Threads:      {process_threads}\")\n        print(f\"Max Recommended:      {max_recommended_threads} threads\")\n        print(\"-\" * 50)\n        print(f\"RECOMMENDATION: Use {max(1, cpu_count_physical-2)}-{cpu_count_physical} threads for optimal performance\")\n        print(f\"(Based on {cpu_count_physical} physical CPU cores)\")\n        print(\"=\" * 50)\n\n    return resource_info\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.get_tables_obj_list","title":"get_tables_obj_list","text":"<pre><code>get_tables_obj_list()\n</code></pre> <p>Return list of loaded table objects.</p> <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of loaded table objects</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def get_tables_obj_list(self) -&gt; List:\n    \"\"\"\n    Return list of loaded table objects.\n\n    Returns:\n        List: List of loaded table objects\n    \"\"\"\n    table_objects = []\n    for table_name in ['patient', 'hospitalization', 'adt', 'labs', 'vitals',\n                      'medication_admin_continuous', 'patient_assessments',\n                      'respiratory_support', 'position']:\n        table_obj = getattr(self, table_name)\n        if table_obj is not None:\n            table_objects.append(table_obj)\n    return table_objects\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.initialize","title":"initialize","text":"<pre><code>initialize(\n    tables=None,\n    sample_size=None,\n    columns=None,\n    filters=None,\n)\n</code></pre> <p>Initialize specified tables with optional filtering and column selection.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>List[str]</code> <p>List of table names to load. Defaults to ['patient'].</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of rows to load for each table.</p> <code>None</code> <code>columns</code> <code>Dict[str, List[str]]</code> <p>Dictionary mapping table names to lists of columns to load.</p> <code>None</code> <code>filters</code> <code>Dict[str, Dict]</code> <p>Dictionary mapping table names to filter dictionaries.</p> <code>None</code> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def initialize(\n    self,\n    tables: Optional[List[str]] = None,\n    sample_size: Optional[int] = None,\n    columns: Optional[Dict[str, List[str]]] = None,\n    filters: Optional[Dict[str, Dict[str, Any]]] = None\n):\n    \"\"\"\n    Initialize specified tables with optional filtering and column selection.\n\n    Parameters:\n        tables (List[str], optional): List of table names to load. Defaults to ['patient'].\n        sample_size (int, optional): Number of rows to load for each table.\n        columns (Dict[str, List[str]], optional): Dictionary mapping table names to lists of columns to load.\n        filters (Dict[str, Dict], optional): Dictionary mapping table names to filter dictionaries.\n    \"\"\"\n    if tables is None:\n        tables = ['patient']\n\n    for table in tables:\n        # Get table-specific columns and filters if provided\n        table_columns = columns.get(table) if columns else None\n        table_filters = filters.get(table) if filters else None\n\n        try:\n            self.load_table(table, sample_size, table_columns, table_filters)\n        except ValueError as e:\n            print(f\"Warning: {e}\")\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.load_table","title":"load_table","text":"<pre><code>load_table(\n    table_name, sample_size=None, columns=None, filters=None\n)\n</code></pre> <p>Load table data and create table object.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to load</p> required <code>sample_size</code> <code>int</code> <p>Number of rows to load</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>Specific columns to load</p> <code>None</code> <code>filters</code> <code>Dict</code> <p>Filters to apply when loading</p> <code>None</code> <p>Returns:</p> Type Description <p>The loaded table object</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def load_table(\n    self,\n    table_name: str,\n    sample_size: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    filters: Optional[Dict[str, Any]] = None\n):\n    \"\"\"\n    Load table data and create table object.\n\n    Parameters:\n        table_name (str): Name of the table to load\n        sample_size (int, optional): Number of rows to load\n        columns (List[str], optional): Specific columns to load\n        filters (Dict, optional): Filters to apply when loading\n\n    Returns:\n        The loaded table object\n    \"\"\"\n    if table_name not in TABLE_CLASSES:\n        raise ValueError(f\"Unknown table: {table_name}. Available tables: {list(TABLE_CLASSES.keys())}\")\n\n    table_class = TABLE_CLASSES[table_name]\n    table_object = table_class.from_file(\n        data_directory=self.data_directory,\n        filetype=self.filetype,\n        timezone=self.timezone,\n        output_directory=self.output_directory,\n        sample_size=sample_size,\n        columns=columns,\n        filters=filters\n    )\n    setattr(self, table_name, table_object)\n    return table_object\n</code></pre>"},{"location":"api/orchestrator/#clifpy.clif_orchestrator.ClifOrchestrator.validate_all","title":"validate_all","text":"<pre><code>validate_all()\n</code></pre> <p>Run validation on all loaded tables.</p> <p>This method runs the validate() method on each loaded table and reports the results.</p> Source code in <code>clifpy/clif_orchestrator.py</code> <pre><code>def validate_all(self):\n    \"\"\"\n    Run validation on all loaded tables.\n\n    This method runs the validate() method on each loaded table\n    and reports the results.\n    \"\"\"\n    loaded_tables = self.get_loaded_tables()\n\n    if not loaded_tables:\n        print(\"No tables loaded to validate.\")\n        return\n\n    print(f\"Validating {len(loaded_tables)} table(s)...\")\n\n    for table_name in loaded_tables:\n        table_obj = getattr(self, table_name)\n        print(f\"\\nValidating {table_name}...\")\n        table_obj.validate()\n</code></pre>"},{"location":"api/tables/","title":"Table Classes","text":""},{"location":"api/tables/#patient","title":"Patient","text":""},{"location":"api/tables/#clifpy.tables.patient.Patient","title":"clifpy.tables.patient.Patient","text":"<pre><code>Patient(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient table wrapper inheriting from BaseTable.</p> <p>This class handles patient-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#adt-admission-discharge-transfer","title":"ADT (Admission, Discharge, Transfer)","text":""},{"location":"api/tables/#clifpy.tables.adt.Adt","title":"clifpy.tables.adt.Adt","text":"<pre><code>Adt(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>ADT (Admission/Discharge/Transfer) table wrapper inheriting from BaseTable.</p> <p>This class handles ADT-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the ADT table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the ADT table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: adt(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.filter_by_date_range","title":"filter_by_date_range","text":"<pre><code>filter_by_date_range(\n    start_date, end_date, date_column=\"in_dttm\"\n)\n</code></pre> <p>Return records within a specific date range for a given datetime column.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def filter_by_date_range(self, start_date: datetime, end_date: datetime, \n                       date_column: str = 'in_dttm') -&gt; pd.DataFrame:\n    \"\"\"Return records within a specific date range for a given datetime column.\"\"\"\n    if self.df is None or date_column not in self.df.columns:\n        return pd.DataFrame()\n\n    # Convert datetime column to datetime if it's not already\n    df_copy = self.df.copy()\n    df_copy[date_column] = pd.to_datetime(df_copy[date_column])\n\n    mask = (df_copy[date_column] &gt;= start_date) &amp; (df_copy[date_column] &lt;= end_date)\n    return df_copy[mask]\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.filter_by_hospitalization","title":"filter_by_hospitalization","text":"<pre><code>filter_by_hospitalization(hospitalization_id)\n</code></pre> <p>Return all ADT records for a specific hospitalization.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def filter_by_hospitalization(self, hospitalization_id: str) -&gt; pd.DataFrame:\n    \"\"\"Return all ADT records for a specific hospitalization.\"\"\"\n    if self.df is None:\n        return pd.DataFrame()\n\n    return self.df[self.df['hospitalization_id'] == hospitalization_id].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.filter_by_location_category","title":"filter_by_location_category","text":"<pre><code>filter_by_location_category(location_category)\n</code></pre> <p>Return all records for a specific location category (e.g., 'icu', 'ward').</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def filter_by_location_category(self, location_category: str) -&gt; pd.DataFrame:\n    \"\"\"Return all records for a specific location category (e.g., 'icu', 'ward').\"\"\"\n    if self.df is None or 'location_category' not in self.df.columns:\n        return pd.DataFrame()\n\n    return self.df[self.df['location_category'] == location_category].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.get_hospital_types","title":"get_hospital_types","text":"<pre><code>get_hospital_types()\n</code></pre> <p>Return unique hospital types in the dataset.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def get_hospital_types(self) -&gt; List[str]:\n    \"\"\"Return unique hospital types in the dataset.\"\"\"\n    if self.df is None or 'hospital_type' not in self.df.columns:\n        return []\n    return self.df['hospital_type'].dropna().unique().tolist()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.get_location_categories","title":"get_location_categories","text":"<pre><code>get_location_categories()\n</code></pre> <p>Return unique location categories in the dataset.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def get_location_categories(self) -&gt; List[str]:\n    \"\"\"Return unique location categories in the dataset.\"\"\"\n    if self.df is None or 'location_category' not in self.df.columns:\n        return []\n    return self.df['location_category'].dropna().unique().tolist()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.adt.Adt.get_summary_stats","title":"get_summary_stats","text":"<pre><code>get_summary_stats()\n</code></pre> <p>Return summary statistics for the ADT data.</p> Source code in <code>clifpy/tables/adt.py</code> <pre><code>def get_summary_stats(self) -&gt; Dict:\n    \"\"\"Return summary statistics for the ADT data.\"\"\"\n    if self.df is None:\n        return {}\n\n    stats = {\n        'total_records': len(self.df),\n        'unique_hospitalizations': self.df['hospitalization_id'].nunique() if 'hospitalization_id' in self.df.columns else 0,\n        'unique_hospitals': self.df['hospital_id'].nunique() if 'hospital_id' in self.df.columns else 0,\n        'location_category_counts': self.df['location_category'].value_counts().to_dict() if 'location_category' in self.df.columns else {},\n        'hospital_type_counts': self.df['hospital_type'].value_counts().to_dict() if 'hospital_type' in self.df.columns else {},\n        'date_range': {\n            'earliest_in': self.df['in_dttm'].min() if 'in_dttm' in self.df.columns else None,\n            'latest_in': self.df['in_dttm'].max() if 'in_dttm' in self.df.columns else None,\n            'earliest_out': self.df['out_dttm'].min() if 'out_dttm' in self.df.columns else None,\n            'latest_out': self.df['out_dttm'].max() if 'out_dttm' in self.df.columns else None\n        }\n    }\n\n    return stats\n</code></pre>"},{"location":"api/tables/#hospitalization","title":"Hospitalization","text":""},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization","title":"clifpy.tables.hospitalization.Hospitalization","text":"<pre><code>Hospitalization(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Hospitalization table wrapper inheriting from BaseTable.</p> <p>This class handles hospitalization-specific data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the hospitalization table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the hospitalization table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: hospitalization(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.calculate_length_of_stay","title":"calculate_length_of_stay","text":"<pre><code>calculate_length_of_stay()\n</code></pre> <p>Calculate length of stay for each hospitalization and return DataFrame with LOS column.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def calculate_length_of_stay(self) -&gt; pd.DataFrame:\n    \"\"\"Calculate length of stay for each hospitalization and return DataFrame with LOS column.\"\"\"\n    if self.df is None:\n        return pd.DataFrame()\n\n    required_cols = ['admission_dttm', 'discharge_dttm']\n    if not all(col in self.df.columns for col in required_cols):\n        print(f\"Missing required columns: {[col for col in required_cols if col not in self.df.columns]}\")\n        return pd.DataFrame()\n\n    df_copy = self.df.copy()\n    df_copy['admission_dttm'] = pd.to_datetime(df_copy['admission_dttm'])\n    df_copy['discharge_dttm'] = pd.to_datetime(df_copy['discharge_dttm'])\n\n    # Calculate LOS in days\n    df_copy['length_of_stay_days'] = (df_copy['discharge_dttm'] - df_copy['admission_dttm']).dt.total_seconds() / (24 * 3600)\n\n    return df_copy\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_mortality_rate","title":"get_mortality_rate","text":"<pre><code>get_mortality_rate()\n</code></pre> <p>Calculate in-hospital mortality rate.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_mortality_rate(self) -&gt; float:\n    \"\"\"Calculate in-hospital mortality rate.\"\"\"\n    if self.df is None or 'discharge_category' not in self.df.columns:\n        return 0.0\n\n    total_hospitalizations = len(self.df)\n    if total_hospitalizations == 0:\n        return 0.0\n\n    expired_count = len(self.df[self.df['discharge_category'] == 'Expired'])\n    return (expired_count / total_hospitalizations) * 100\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_patient_hospitalization_counts","title":"get_patient_hospitalization_counts","text":"<pre><code>get_patient_hospitalization_counts()\n</code></pre> <p>Return DataFrame with hospitalization counts per patient.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_patient_hospitalization_counts(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with hospitalization counts per patient.\"\"\"\n    if self.df is None or 'patient_id' not in self.df.columns:\n        return pd.DataFrame()\n\n    patient_counts = (self.df.groupby('patient_id')\n                     .agg({\n                         'hospitalization_id': 'count',\n                         'admission_dttm': ['min', 'max']\n                     })\n                     .reset_index())\n\n    # Flatten column names\n    patient_counts.columns = ['patient_id', 'hospitalization_count', 'first_admission', 'last_admission']\n\n    # Calculate span of care\n    patient_counts['first_admission'] = pd.to_datetime(patient_counts['first_admission'])\n    patient_counts['last_admission'] = pd.to_datetime(patient_counts['last_admission'])\n    patient_counts['care_span_days'] = (patient_counts['last_admission'] - patient_counts['first_admission']).dt.total_seconds() / (24 * 3600)\n\n    return patient_counts.sort_values('hospitalization_count', ascending=False)\n</code></pre>"},{"location":"api/tables/#clifpy.tables.hospitalization.Hospitalization.get_summary_stats","title":"get_summary_stats","text":"<pre><code>get_summary_stats()\n</code></pre> <p>Return comprehensive summary statistics for hospitalization data.</p> Source code in <code>clifpy/tables/hospitalization.py</code> <pre><code>def get_summary_stats(self) -&gt; Dict:\n    \"\"\"Return comprehensive summary statistics for hospitalization data.\"\"\"\n    if self.df is None:\n        return {}\n\n    stats = {\n        'total_hospitalizations': len(self.df),\n        'unique_patients': self.df['patient_id'].nunique() if 'patient_id' in self.df.columns else 0,\n        'discharge_category_counts': self.df['discharge_category'].value_counts().to_dict() if 'discharge_category' in self.df.columns else {},\n        'admission_type_counts': self.df['admission_type_category'].value_counts().to_dict() if 'admission_type_category' in self.df.columns else {},\n        'date_range': {\n            'earliest_admission': self.df['admission_dttm'].min() if 'admission_dttm' in self.df.columns else None,\n            'latest_admission': self.df['admission_dttm'].max() if 'admission_dttm' in self.df.columns else None,\n            'earliest_discharge': self.df['discharge_dttm'].min() if 'discharge_dttm' in self.df.columns else None,\n            'latest_discharge': self.df['discharge_dttm'].max() if 'discharge_dttm' in self.df.columns else None\n        }\n    }\n\n    # Age statistics\n    if 'age_at_admission' in self.df.columns:\n        age_data = self.df['age_at_admission'].dropna()\n        if not age_data.empty:\n            stats['age_stats'] = {\n                'mean': round(age_data.mean(), 1),\n                'median': age_data.median(),\n                'min': age_data.min(),\n                'max': age_data.max(),\n                'std': round(age_data.std(), 1)\n            }\n\n    # Length of stay statistics\n    if all(col in self.df.columns for col in ['admission_dttm', 'discharge_dttm']):\n        los_df = self.calculate_length_of_stay()\n        if 'length_of_stay_days' in los_df.columns:\n            los_data = los_df['length_of_stay_days'].dropna()\n            if not los_data.empty:\n                stats['length_of_stay_stats'] = {\n                    'mean_days': round(los_data.mean(), 1),\n                    'median_days': round(los_data.median(), 1),\n                    'min_days': round(los_data.min(), 1),\n                    'max_days': round(los_data.max(), 1),\n                    'std_days': round(los_data.std(), 1)\n                }\n\n    # Mortality rate\n    stats['mortality_rate_percent'] = round(self.get_mortality_rate(), 2)\n\n    return stats\n</code></pre>"},{"location":"api/tables/#labs","title":"Labs","text":""},{"location":"api/tables/#clifpy.tables.labs.Labs","title":"clifpy.tables.labs.Labs","text":"<pre><code>Labs(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Labs table wrapper inheriting from BaseTable.</p> <p>This class handles laboratory data and validations including reference unit validation while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the labs table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the labs table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: labs(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Initialize lab reference units\n    self._lab_reference_units = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load lab-specific schema data\n    self._load_labs_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.lab_reference_units","title":"lab_reference_units  <code>property</code>","text":"<pre><code>lab_reference_units\n</code></pre> <p>Get the lab reference units mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_category_stats","title":"get_lab_category_stats","text":"<pre><code>get_lab_category_stats()\n</code></pre> <p>Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_category_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.\"\"\"\n    if (\n        self.df is None\n        or 'lab_value_numeric' not in self.df.columns\n        or 'hospitalization_id' not in self.df.columns        # remove this line if hosp-id is optional\n    ):\n        return {\"status\": \"Missing columns\"}\n\n    stats = (\n        self.df\n        .groupby('lab_category')\n        .agg(\n            count=('lab_value_numeric', 'count'),\n            unique=('hospitalization_id', 'nunique'),\n            missing_pct=('lab_value_numeric', lambda x: 100 * x.isna().mean()),\n            mean=('lab_value_numeric', 'mean'),\n            std=('lab_value_numeric', 'std'),\n            min=('lab_value_numeric', 'min'),\n            q1=('lab_value_numeric', lambda x: x.quantile(0.25)),\n            median=('lab_value_numeric', 'median'),\n            q3=('lab_value_numeric', lambda x: x.quantile(0.75)),\n            max=('lab_value_numeric', 'max'),\n        )\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.labs.Labs.get_lab_specimen_stats","title":"get_lab_specimen_stats","text":"<pre><code>get_lab_specimen_stats()\n</code></pre> <p>Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.</p> Source code in <code>clifpy/tables/labs.py</code> <pre><code>def get_lab_specimen_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each lab category, including missingness and unique hospitalization_id counts.\"\"\"\n    if (\n        self.df is None\n        or 'lab_value_numeric' not in self.df.columns\n        or 'hospitalization_id' not in self.df.columns \n        or 'lab_speciment_category' not in self.df.columns       # remove this line if hosp-id is optional\n    ):\n        return {\"status\": \"Missing columns\"}\n\n    stats = (\n        self.df\n        .groupby('lab_specimen_category')\n        .agg(\n            count=('lab_value_numeric', 'count'),\n            unique=('hospitalization_id', 'nunique'),\n            missing_pct=('lab_value_numeric', lambda x: 100 * x.isna().mean()),\n            mean=('lab_value_numeric', 'mean'),\n            std=('lab_value_numeric', 'std'),\n            min=('lab_value_numeric', 'min'),\n            q1=('lab_value_numeric', lambda x: x.quantile(0.25)),\n            median=('lab_value_numeric', 'median'),\n            q3=('lab_value_numeric', lambda x: x.quantile(0.75)),\n            max=('lab_value_numeric', 'max'),\n        )\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"api/tables/#vitals","title":"Vitals","text":""},{"location":"api/tables/#clifpy.tables.vitals.Vitals","title":"clifpy.tables.vitals.Vitals","text":"<pre><code>Vitals(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Vitals table wrapper inheriting from BaseTable.</p> <p>This class handles vitals-specific data and validations including range validation for vital signs.</p> <p>Initialize the vitals table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the vitals table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # Initialize range validation errors list\n    self.range_validation_errors: List[dict] = []\n\n    # Load vital ranges and units from schema\n    self._vital_units = None\n    self._vital_ranges = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load vital-specific schema data\n    self._load_vitals_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.vital_ranges","title":"vital_ranges  <code>property</code>","text":"<pre><code>vital_ranges\n</code></pre> <p>Get the vital ranges from the schema.</p>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.vital_units","title":"vital_units  <code>property</code>","text":"<pre><code>vital_units\n</code></pre> <p>Get the vital units mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.filter_by_vital_category","title":"filter_by_vital_category","text":"<pre><code>filter_by_vital_category(vital_category)\n</code></pre> <p>Return all records for a specific vital category (e.g., 'heart_rate', 'temp_c').</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def filter_by_vital_category(self, vital_category: str) -&gt; pd.DataFrame:\n    \"\"\"Return all records for a specific vital category (e.g., 'heart_rate', 'temp_c').\"\"\"\n    if self.df is None or 'vital_category' not in self.df.columns:\n        return pd.DataFrame()\n\n    return self.df[self.df['vital_category'] == vital_category].copy()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.get_vital_summary_stats","title":"get_vital_summary_stats","text":"<pre><code>get_vital_summary_stats()\n</code></pre> <p>Return summary statistics for each vital category.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def get_vital_summary_stats(self) -&gt; pd.DataFrame:\n    \"\"\"Return summary statistics for each vital category.\"\"\"\n    if self.df is None or 'vital_value' not in self.df.columns:\n        return pd.DataFrame()\n\n    # Convert vital_value to numeric\n    df_copy = self.df.copy()\n    df_copy['vital_value'] = pd.to_numeric(df_copy['vital_value'], errors='coerce')\n\n    # Group by vital category and calculate stats\n    stats = df_copy.groupby('vital_category')['vital_value'].agg([\n        'count', 'mean', 'std', 'min', 'max',\n        ('q1', lambda x: x.quantile(0.25)),\n        ('median', lambda x: x.quantile(0.5)),\n        ('q3', lambda x: x.quantile(0.75))\n    ]).round(2)\n\n    return stats\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.isvalid","title":"isvalid","text":"<pre><code>isvalid()\n</code></pre> <p>Return <code>True</code> if the last validation finished without errors.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def isvalid(self) -&gt; bool:\n    \"\"\"Return ``True`` if the last validation finished without errors.\"\"\"\n    return not self.errors and not self.range_validation_errors\n</code></pre>"},{"location":"api/tables/#clifpy.tables.vitals.Vitals.validate_vital_ranges","title":"validate_vital_ranges","text":"<pre><code>validate_vital_ranges()\n</code></pre> <p>Validate vital values against expected ranges using grouped data for efficiency.</p> Source code in <code>clifpy/tables/vitals.py</code> <pre><code>def validate_vital_ranges(self):\n    \"\"\"Validate vital values against expected ranges using grouped data for efficiency.\"\"\"\n    self.range_validation_errors = []\n\n    if self.df is None or not self._vital_ranges:\n        return\n\n    required_columns = ['vital_category', 'vital_value']\n    required_columns_for_df = ['vital_category', 'vital_value']\n    if not all(col in self.df.columns for col in required_columns_for_df):\n        self.range_validation_errors.append({\n            \"error_type\": \"missing_columns_for_range_validation\",\n            \"columns\": [col for col in required_columns_for_df if col not in self.df.columns],\n            \"message\": \"vital_category or vital_value column missing, cannot perform range validation.\"\n        })\n        return\n\n    # Work on a copy to safely convert vital_value to numeric for aggregation\n    df_for_stats = self.df[required_columns_for_df].copy()\n    df_for_stats['vital_value'] = pd.to_numeric(df_for_stats['vital_value'], errors='coerce')\n\n    # Filter out rows where vital_value could not be converted\n    df_for_stats.dropna(subset=['vital_value'], inplace=True)\n\n    if df_for_stats.empty:\n        # No numeric vital_value data to perform range validation on\n        return\n\n    vital_stats = (df_for_stats\n                   .groupby('vital_category')['vital_value']\n                   .agg(['min', 'max', 'mean', 'count'])\n                   .reset_index())\n\n    if vital_stats.empty:\n        return\n\n    # Check each vital category's ranges\n    for _, row in vital_stats.iterrows():\n        vital_category = row['vital_category']\n        min_val = row['min']\n        max_val = row['max']\n        count = row['count']\n        mean_val = row['mean']\n\n        # Check if vital category has defined ranges\n        if vital_category not in self._vital_ranges:\n            self.range_validation_errors.append({\n                'error_type': 'unknown_vital_category',\n                'vital_category': vital_category,\n                'affected_rows': count,\n                'observed_min': min_val,\n                'observed_max': max_val,\n                'message': f\"Unknown vital category '{vital_category}' found in data.\"\n            })\n            continue\n\n        expected_range = self._vital_ranges[vital_category]\n        expected_min = expected_range.get('min')\n        expected_max = expected_range.get('max')\n\n        # Check if any values are outside the expected range\n        if expected_min is not None and min_val &lt; expected_min:\n            self.range_validation_errors.append({\n                'error_type': 'below_range',\n                'vital_category': vital_category,\n                'observed_min': min_val,\n                'expected_min': expected_min,\n                'message': f\"Values below expected minimum for {vital_category}\"\n            })\n\n        if expected_max is not None and max_val &gt; expected_max:\n            self.range_validation_errors.append({\n                'error_type': 'above_range',\n                'vital_category': vital_category,\n                'observed_max': max_val,\n                'expected_max': expected_max,\n                'message': f\"Values above expected maximum for {vital_category}\"\n            })\n\n    # Add range validation errors to main errors list\n    if self.range_validation_errors:\n        self.errors.extend(self.range_validation_errors)\n        self.logger.warning(f\"Found {len(self.range_validation_errors)} range validation errors\")\n</code></pre>"},{"location":"api/tables/#respiratory-support","title":"Respiratory Support","text":""},{"location":"api/tables/#clifpy.tables.respiratory_support.RespiratorySupport","title":"clifpy.tables.respiratory_support.RespiratorySupport","text":"<pre><code>RespiratorySupport(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Respiratory support table wrapper inheriting from BaseTable.</p> <p>This class handles respiratory support data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the respiratory_support table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/respiratory_support.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the respiratory_support table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#medication-administration-continuous","title":"Medication Administration (Continuous)","text":""},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous","title":"clifpy.tables.medication_admin_continuous.MedicationAdminContinuous","text":"<pre><code>MedicationAdminContinuous(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Medication administration continuous table wrapper inheriting from BaseTable.</p> <p>This class handles medication administration continuous data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the MedicationAdminContinuous table.</p> <p>This class handles continuous medication administration data, including validation, dose unit standardization, and unit conversion capabilities.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous--parameters","title":"Parameters","text":"<p>data_directory : str, optional     Path to the directory containing data files. If None and data is provided,     defaults to current directory. filetype : str, optional     Type of data file (csv, parquet, etc.). If None and data is provided,     defaults to 'parquet'. timezone : str, default=\"UTC\"     Timezone for datetime columns. Used for proper timestamp handling. output_directory : str, optional     Directory for saving output files and logs. If not specified, outputs     are saved to the current working directory. data : pd.DataFrame, optional     Pre-loaded DataFrame to use instead of loading from file. Supports     backward compatibility with direct DataFrame initialization.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous--notes","title":"Notes","text":"<p>The class supports two initialization patterns: 1. Loading from file: provide data_directory and filetype 2. Direct DataFrame: provide data parameter (legacy support)</p> <p>Upon initialization, the class loads medication schema data including category-to-group mappings from the YAML schema.</p> Source code in <code>clifpy/tables/medication_admin_continuous.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the MedicationAdminContinuous table.\n\n    This class handles continuous medication administration data, including validation,\n    dose unit standardization, and unit conversion capabilities.\n\n    Parameters\n    ----------\n    data_directory : str, optional\n        Path to the directory containing data files. If None and data is provided,\n        defaults to current directory.\n    filetype : str, optional\n        Type of data file (csv, parquet, etc.). If None and data is provided,\n        defaults to 'parquet'.\n    timezone : str, default=\"UTC\"\n        Timezone for datetime columns. Used for proper timestamp handling.\n    output_directory : str, optional\n        Directory for saving output files and logs. If not specified, outputs\n        are saved to the current working directory.\n    data : pd.DataFrame, optional\n        Pre-loaded DataFrame to use instead of loading from file. Supports\n        backward compatibility with direct DataFrame initialization.\n\n    Notes\n    -----\n    The class supports two initialization patterns:\n    1. Loading from file: provide data_directory and filetype\n    2. Direct DataFrame: provide data parameter (legacy support)\n\n    Upon initialization, the class loads medication schema data including\n    category-to-group mappings from the YAML schema.\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: medication_admin_continuous(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Load medication mappings\n    self._med_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load medication-specific schema data\n    self._load_medication_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping","title":"med_category_to_group_mapping  <code>property</code>","text":"<pre><code>med_category_to_group_mapping\n</code></pre> <p>Get the medication category to group mapping from the schema.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping--returns","title":"Returns","text":"<p>Dict[str, str]     A dictionary mapping medication categories to their therapeutic groups.     Returns a copy to prevent external modification of the internal mapping.     Returns an empty dict if no mappings are loaded.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.med_category_to_group_mapping--examples","title":"Examples","text":"<p>mac = MedicationAdminContinuous(data) mappings = mac.med_category_to_group_mapping mappings['Antibiotics'] 'Antimicrobials'</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units","title":"convert_dose_to_limited_units","text":"<pre><code>convert_dose_to_limited_units(vitals_df, med_df=None)\n</code></pre> <p>Convert medication doses to standardized units per minute.</p> <p>This method converts all medication doses to one of three standard units: - mcg/min for mass-based medications - ml/min for volume-based medications - units/min for unit-based medications</p> <p>The conversion handles different time scales (per hour vs per minute) and weight-based dosing (per kg) by incorporating patient weights from vitals.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units--parameters","title":"Parameters","text":"<p>vitals_df : pd.DataFrame     DataFrame containing patient vital signs, must include:     - hospitalization_id: Patient identifier     - recorded_dttm: Timestamp of vital recording     - vital_category: Type of vital (looks for 'weight_kg')     - vital_value: Numeric value of the vital med_df : pd.DataFrame, optional     DataFrame containing medication administration data. If None, uses self.df.     Required columns:     - hospitalization_id: Patient identifier     - admin_dttm: Medication administration timestamp     - med_dose_unit: Original dose unit (case-insensitive)     - med_dose: Original dose value     - med_category: Medication category (used for SQL query)     Optional columns:     - weight_kg: Patient weight; if absent, pulled from vitals_df</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units--returns","title":"Returns","text":"<p>pd.DataFrame     Original med_df with additional columns:     - med_dose_unit_clean: Standardized unit pattern     - weight_kg: Patient weight used for conversion (if applicable)     - med_dose_converted: Dose value in standardized units     - med_dose_unit_converted: Standardized unit ('mcg/min', 'ml/min', or 'units/min')     - Additional calculation columns (time_multiplier, pt_weight_multiplier, amount_multiplier)</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units--raises","title":"Raises","text":"<p>ValueError     If med_df is None and self.df is also None, or if required columns are missing.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units--warnings","title":"Warnings","text":"<p>Logs warnings for unrecognized dose units that cannot be converted.</p>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units--notes","title":"Notes","text":"<ul> <li>Weight-based dosing (/kg) uses the most recent weight prior to administration</li> <li>Unrecognized dose units result in NULL converted values</li> <li>The conversion preserves the original columns and adds new ones</li> </ul>"},{"location":"api/tables/#clifpy.tables.medication_admin_continuous.MedicationAdminContinuous.convert_dose_to_limited_units--examples","title":"Examples","text":"<p>vitals = pd.DataFrame({ ...     'hospitalization_id': ['H001'], ...     'recorded_dttm': pd.to_datetime(['2023-01-01']), ...     'vital_category': ['weight_kg'], ...     'vital_value': [70.0] ... }) meds = pd.DataFrame({ ...     'hospitalization_id': ['H001'], ...     'admin_dttm': pd.to_datetime(['2023-01-02']), ...     'med_dose': [5.0], ...     'med_dose_unit': ['mcg/kg/hr'], ...     'med_category': ['Vasopressors'] ... }) result = mac.convert_dose_to_limited_units(vitals, meds) result['med_dose_converted'].iloc[0] 5.833333...  # 5 * 70 / 60 (mcg/kg/hr to mcg/min with 70kg patient)</p> Source code in <code>clifpy/tables/medication_admin_continuous.py</code> <pre><code>def convert_dose_to_limited_units(self, vitals_df: pd.DataFrame, med_df: pd.DataFrame = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert medication doses to standardized units per minute.\n\n    This method converts all medication doses to one of three standard units:\n    - mcg/min for mass-based medications\n    - ml/min for volume-based medications  \n    - units/min for unit-based medications\n\n    The conversion handles different time scales (per hour vs per minute) and\n    weight-based dosing (per kg) by incorporating patient weights from vitals.\n\n    Parameters\n    ----------\n    vitals_df : pd.DataFrame\n        DataFrame containing patient vital signs, must include:\n        - hospitalization_id: Patient identifier\n        - recorded_dttm: Timestamp of vital recording\n        - vital_category: Type of vital (looks for 'weight_kg')\n        - vital_value: Numeric value of the vital\n    med_df : pd.DataFrame, optional\n        DataFrame containing medication administration data. If None, uses self.df.\n        Required columns:\n        - hospitalization_id: Patient identifier\n        - admin_dttm: Medication administration timestamp\n        - med_dose_unit: Original dose unit (case-insensitive)\n        - med_dose: Original dose value\n        - med_category: Medication category (used for SQL query)\n        Optional columns:\n        - weight_kg: Patient weight; if absent, pulled from vitals_df\n\n    Returns\n    -------\n    pd.DataFrame\n        Original med_df with additional columns:\n        - med_dose_unit_clean: Standardized unit pattern\n        - weight_kg: Patient weight used for conversion (if applicable)\n        - med_dose_converted: Dose value in standardized units\n        - med_dose_unit_converted: Standardized unit ('mcg/min', 'ml/min', or 'units/min')\n        - Additional calculation columns (time_multiplier, pt_weight_multiplier, amount_multiplier)\n\n    Raises\n    ------\n    ValueError\n        If med_df is None and self.df is also None, or if required columns are missing.\n\n    Warnings\n    --------\n    Logs warnings for unrecognized dose units that cannot be converted.\n\n    Notes\n    -----\n    - Weight-based dosing (/kg) uses the most recent weight prior to administration\n    - Unrecognized dose units result in NULL converted values\n    - The conversion preserves the original columns and adds new ones\n\n    Examples\n    --------\n    &gt;&gt;&gt; vitals = pd.DataFrame({\n    ...     'hospitalization_id': ['H001'],\n    ...     'recorded_dttm': pd.to_datetime(['2023-01-01']),\n    ...     'vital_category': ['weight_kg'],\n    ...     'vital_value': [70.0]\n    ... })\n    &gt;&gt;&gt; meds = pd.DataFrame({\n    ...     'hospitalization_id': ['H001'],\n    ...     'admin_dttm': pd.to_datetime(['2023-01-02']),\n    ...     'med_dose': [5.0],\n    ...     'med_dose_unit': ['mcg/kg/hr'],\n    ...     'med_category': ['Vasopressors']\n    ... })\n    &gt;&gt;&gt; result = mac.convert_dose_to_limited_units(vitals, meds)\n    &gt;&gt;&gt; result['med_dose_converted'].iloc[0]\n    5.833333...  # 5 * 70 / 60 (mcg/kg/hr to mcg/min with 70kg patient)\n    \"\"\"\n    if med_df is None:\n        med_df = self.df\n    if med_df is None:\n        raise ValueError(\"No data provided\")\n\n    if 'weight_kg' not in med_df.columns:\n        self.logger.info(\"No weight_kg column found, adding the most recent from vitals\")\n        query = \"\"\"\n        SELECT m.*\n            , v.vital_value as weight_kg\n            , v.recorded_dttm as weight_recorded_dttm\n            , ROW_NUMBER() OVER (\n                PARTITION BY m.hospitalization_id, m.admin_dttm, m.med_category\n                ORDER BY v.recorded_dttm DESC\n                ) as rn\n        FROM med_df m\n        LEFT JOIN vitals_df v \n            ON m.hospitalization_id = v.hospitalization_id \n            AND v.vital_category = 'weight_kg' AND v.vital_value IS NOT NULL\n            AND v.recorded_dttm &lt;= m.admin_dttm  -- only past weights\n        -- rn = 1 for the weight w/ the latest recorded_dttm (and thus most recent)\n        QUALIFY (rn = 1) \n        ORDER BY m.hospitalization_id, m.admin_dttm, m.med_category, rn\n        \"\"\"\n        med_df = duckdb.sql(query).to_df()\n\n    # check if the required columns are present\n    required_columns = {'med_dose_unit', 'med_dose', 'weight_kg'}\n    missing_columns = required_columns - set(med_df.columns)\n    if missing_columns:\n        raise ValueError(f\"The following column(s) are required but not found: {missing_columns}\")\n\n    med_df, unrecognized = self._normalize_dose_unit_pattern(med_df)\n    if not unrecognized:\n        self.logger.info(\"No unrecognized dose units found, continuing with conversion\")\n    else:\n        self.logger.warning(f\"Unrecognized dose units found: {unrecognized}\")\n\n    acceptable_unit_patterns_str = \"','\".join(self._acceptable_dose_unit_patterns)\n\n    query = f\"\"\"\n    SELECT *\n        , CASE WHEN regexp_matches(med_dose_unit_clean, '/h(r|our)?\\\\b') THEN 1/60.0\n            WHEN regexp_matches(med_dose_unit_clean, '/m(in|inute)?\\\\b') THEN 1.0\n            ELSE NULL END as time_multiplier\n        , CASE WHEN contains(med_dose_unit_clean, '/kg/') THEN weight_kg\n            ELSE 1 END AS pt_weight_multiplier\n        , CASE WHEN contains(med_dose_unit_clean, 'mcg/') THEN 1.0\n            WHEN contains(med_dose_unit_clean, 'mg/') THEN 1000.0\n            WHEN contains(med_dose_unit_clean, 'ng/') THEN 0.001\n            WHEN contains(med_dose_unit_clean, 'milli') THEN 0.001\n            WHEN contains(med_dose_unit_clean, 'units/') THEN 1\n            WHEN contains(med_dose_unit_clean, 'ml/') THEN 1.0\n            WHEN contains(med_dose_unit_clean, 'l/') AND NOT contains(med_dose_unit_clean, 'ml/') THEN 1000.0\n            ELSE NULL END as amount_multiplier\n        , med_dose * time_multiplier * pt_weight_multiplier * amount_multiplier as med_dose_converted\n        , CASE WHEN med_dose_unit_clean NOT IN ('{acceptable_unit_patterns_str}') THEN NULL\n            WHEN contains(med_dose_unit_clean, 'units/') THEN 'units/min'\n            WHEN contains(med_dose_unit_clean, 'l/') THEN 'ml/min'\n            ELSE 'mcg/min' END as med_dose_unit_converted\n    FROM med_df\n    \"\"\"\n    return duckdb.sql(query).to_df()\n</code></pre>"},{"location":"api/tables/#patient-assessments","title":"Patient Assessments","text":""},{"location":"api/tables/#clifpy.tables.patient_assessments.PatientAssessments","title":"clifpy.tables.patient_assessments.PatientAssessments","text":"<pre><code>PatientAssessments(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Patient assessments table wrapper inheriting from BaseTable.</p> <p>This class handles patient assessment data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the patient_assessments table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/patient_assessments.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the patient_assessments table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: patient_assessments(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    # Initialize assessment mappings\n    self._assessment_category_to_group = None\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n\n    # Load assessment-specific schema data\n    self._load_assessment_schema_data()\n</code></pre>"},{"location":"api/tables/#clifpy.tables.patient_assessments.PatientAssessments.assessment_category_to_group_mapping","title":"assessment_category_to_group_mapping  <code>property</code>","text":"<pre><code>assessment_category_to_group_mapping\n</code></pre> <p>Get the assessment category to group mapping from the schema.</p>"},{"location":"api/tables/#position","title":"Position","text":""},{"location":"api/tables/#clifpy.tables.position.Position","title":"clifpy.tables.position.Position","text":"<pre><code>Position(\n    data_directory=None,\n    filetype=None,\n    timezone=\"UTC\",\n    output_directory=None,\n    data=None,\n)\n</code></pre> <p>               Bases: <code>BaseTable</code></p> <p>Position table wrapper inheriting from BaseTable.</p> <p>This class handles patient position data and validations while leveraging the common functionality provided by BaseTable.</p> <p>Initialize the position table.</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>Path to the directory containing data files</p> <code>None</code> <code>filetype</code> <code>str</code> <p>Type of data file (csv, parquet, etc.)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for datetime columns</p> <code>'UTC'</code> <code>output_directory</code> <code>str</code> <p>Directory for saving output files and logs</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>Pre-loaded data to use instead of loading from file</p> <code>None</code> Source code in <code>clifpy/tables/position.py</code> <pre><code>def __init__(\n    self,\n    data_directory: str = None,\n    filetype: str = None,\n    timezone: str = \"UTC\",\n    output_directory: Optional[str] = None,\n    data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Initialize the position table.\n\n    Parameters:\n        data_directory (str): Path to the directory containing data files\n        filetype (str): Type of data file (csv, parquet, etc.)\n        timezone (str): Timezone for datetime columns\n        output_directory (str, optional): Directory for saving output files and logs\n        data (pd.DataFrame, optional): Pre-loaded data to use instead of loading from file\n    \"\"\"\n    # For backward compatibility, handle the old signature\n    if data_directory is None and filetype is None and data is not None:\n        # Old signature: position(data)\n        # Use dummy values for required parameters\n        data_directory = \".\"\n        filetype = \"parquet\"\n\n    super().__init__(\n        data_directory=data_directory,\n        filetype=filetype,\n        timezone=timezone,\n        output_directory=output_directory,\n        data=data\n    )\n</code></pre>"},{"location":"api/tables/#clifpy.tables.position.Position.get_position_category_stats","title":"get_position_category_stats","text":"<pre><code>get_position_category_stats()\n</code></pre> <p>Return summary statistics for each position category, including missingness and unique patient counts. Expects columns: 'position_category', 'position_name', and optionally 'hospitalization_id'.</p> Source code in <code>clifpy/tables/position.py</code> <pre><code>def get_position_category_stats(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Return summary statistics for each position category, including missingness and unique patient counts.\n    Expects columns: 'position_category', 'position_name', and optionally 'hospitalization_id'.\n    \"\"\"\n    if self.df is None or 'position_category' not in self.df.columns or 'hospitalization_id' not in self.df.columns:\n        return {\"status\": \"Missing columns\"}\n\n    agg_dict = {\n        'count': ('position_category', 'count'),\n        'unique': ('hospitalization_id', 'nunique'),\n    }\n\n    stats = (\n        self.df\n        .groupby('position_category')\n        .agg(**agg_dict)\n        .round(2)\n    )\n\n    return stats\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples of using CLIFpy for common ICU data analysis tasks.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":""},{"location":"examples/#loading-data","title":"Loading Data","text":"<p>Learn different ways to load CLIF data, including: - Loading from CSV and Parquet files - Using filters and column selection - Working with sample data - Handling large datasets efficiently</p>"},{"location":"examples/#analyzing-icu-stays","title":"Analyzing ICU Stays","text":"<p>Common ICU analysis patterns: - Identifying ICU admissions - Calculating length of stay - Tracking patient movement - Analyzing severity of illness</p>"},{"location":"examples/#clinical-calculations","title":"Clinical Calculations","text":"<p>Implement clinical calculations and scores: - Calculating SOFA scores - Tracking vasopressor requirements - Monitoring ventilation parameters - Assessing prone positioning compliance</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#basic-data-loading","title":"Basic Data Loading","text":"<pre><code>from clifpy.tables import Patient, Labs, Vitals\nfrom clifpy.clif_orchestrator import ClifOrchestrator\n\n# Load individual tables\npatient = Patient.from_file('/data', 'parquet', timezone='US/Central')\nlabs = Labs.from_file('/data', 'parquet', timezone='US/Central')\n\n# Or use orchestrator for multiple tables\norchestrator = ClifOrchestrator('/data', 'parquet', 'US/Central')\norchestrator.initialize(tables=['patient', 'labs', 'vitals', 'adt'])\n</code></pre>"},{"location":"examples/#finding-icu-patients","title":"Finding ICU Patients","text":"<pre><code># Get ICU admissions\nicu_stays = orchestrator.adt.filter_by_location_category('icu')\nicu_patients = icu_stays['patient_id'].unique()\n\n# Get their demographics\nicu_demographics = orchestrator.patient.df[\n    orchestrator.patient.df['patient_id'].isin(icu_patients)\n]\n</code></pre>"},{"location":"examples/#analyzing-lab-trends","title":"Analyzing Lab Trends","text":"<pre><code># Get recent abnormal labs\nrecent_labs = orchestrator.labs.get_recent(hours=24)\nabnormal = recent_labs[\n    (recent_labs['lab_name'] == 'creatinine') &amp; \n    (recent_labs['lab_value'] &gt; 2.0)\n]\n\n# Track patient's lab trend\npatient_labs = orchestrator.labs.df[\n    orchestrator.labs.df['patient_id'] == 'P12345'\n].sort_values('lab_datetime')\n</code></pre>"},{"location":"examples/#medication-analysis","title":"Medication Analysis","text":"<pre><code># Find patients on multiple vasopressors\nvasopressors = orchestrator.medication_admin_continuous.filter_by_med_group('vasopressor')\nconcurrent = orchestrator.medication_admin_continuous.get_concurrent_medications('P12345')\nmulti_pressor = concurrent[concurrent['medication_group'] == 'vasopressor']\n</code></pre>"},{"location":"examples/#example-notebooks","title":"Example Notebooks","text":"<p>The repository includes Jupyter notebooks demonstrating: - <code>labs_demo.ipynb</code> - Laboratory data analysis - <code>respiratory_support_demo.ipynb</code> - Ventilation analysis - <code>position_demo.ipynb</code> - Prone positioning analysis</p>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Explore specific examples in detail</li> <li>Review the API documentation</li> <li>See the User Guide for comprehensive coverage</li> </ul>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the fundamental patterns for working with CLIFpy.</p>"},{"location":"getting-started/basic-usage/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/basic-usage/#table-classes","title":"Table Classes","text":"<p>Each CLIF table is represented by a Python class that inherits from <code>BaseTable</code>:</p> <ul> <li><code>Patient</code> - Demographics and patient identification</li> <li><code>Adt</code> - Admission, discharge, and transfer events</li> <li><code>Hospitalization</code> - Hospital stay information</li> <li><code>Labs</code> - Laboratory test results</li> <li><code>Vitals</code> - Vital signs measurements</li> <li><code>RespiratorySupport</code> - Ventilation and oxygen therapy</li> <li><code>MedicationAdminContinuous</code> - Continuous infusions</li> <li><code>PatientAssessments</code> - Clinical assessment scores</li> <li><code>Position</code> - Patient positioning</li> </ul>"},{"location":"getting-started/basic-usage/#data-loading","title":"Data Loading","text":"<p>All tables support two loading methods:</p> <pre><code># Method 1: From files\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',  # or 'csv'\n    timezone='US/Central'\n)\n\n# Method 2: From existing DataFrame\ntable = TableClass(\n    data=existing_dataframe,\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#validation","title":"Validation","text":"<p>Every table includes built-in validation:</p> <pre><code># Run validation\ntable.validate()\n\n# Check if valid\nif table.isvalid():\n    print(\"Validation passed!\")\nelse:\n    # Review errors\n    for error in table.errors[:5]:\n        print(f\"{error['type']}: {error['message']}\")\n</code></pre>"},{"location":"getting-started/basic-usage/#working-with-dataframes","title":"Working with DataFrames","text":"<p>All table data is accessible via the <code>df</code> attribute:</p> <pre><code># Access the underlying DataFrame\ndf = table.df\n\n# Use standard pandas operations\nprint(df.shape)\nprint(df.columns.tolist())\nprint(df.dtypes)\n\n# Filter data\nfiltered = df[df['some_column'] &gt; threshold]\n</code></pre>"},{"location":"getting-started/basic-usage/#common-operations","title":"Common Operations","text":""},{"location":"getting-started/basic-usage/#date-range-filtering","title":"Date Range Filtering","text":"<p>Most tables with datetime columns support date range filtering:</p> <pre><code>from datetime import datetime\n\n# Filter by date range\nstart = datetime(2023, 1, 1)\nend = datetime(2023, 12, 31)\n\n# For tables with custom methods\nfiltered = table.filter_by_date_range(start, end)\n\n# Or using pandas\nmask = (df['datetime_column'] &gt;= start) &amp; (df['datetime_column'] &lt;= end)\nfiltered = df[mask]\n</code></pre>"},{"location":"getting-started/basic-usage/#category-filtering","title":"Category Filtering","text":"<p>Tables with standardized categories provide filtering methods:</p> <pre><code># Labs by category\nchemistry = labs.filter_by_category('chemistry')\nhematology = labs.filter_by_category('hematology')\n\n# ADT by location\nicu_stays = adt.filter_by_location_category('icu')\ned_visits = adt.filter_by_location_category('ed')\n\n# Medications by group\nvasopressors = meds.filter_by_med_group('vasopressor')\nsedatives = meds.filter_by_med_group('sedative')\n</code></pre>"},{"location":"getting-started/basic-usage/#patient-specific-data","title":"Patient-specific Data","text":"<pre><code># Single patient\npatient_id = 'P12345'\npatient_labs = labs.df[labs.df['patient_id'] == patient_id]\n\n# Multiple patients\npatient_ids = ['P001', 'P002', 'P003']\ncohort_data = vitals.df[vitals.df['patient_id'].isin(patient_ids)]\n</code></pre>"},{"location":"getting-started/basic-usage/#output-and-reporting","title":"Output and Reporting","text":""},{"location":"getting-started/basic-usage/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Get table summary\nsummary = table.get_summary()\nprint(f\"Rows: {summary['num_rows']}\")\nprint(f\"Columns: {summary['num_columns']}\")\nprint(f\"Memory usage: {summary['memory_usage_mb']:.2f} MB\")\n\n# Save summary to file\ntable.save_summary()\n</code></pre>"},{"location":"getting-started/basic-usage/#validation-reports","title":"Validation Reports","text":"<p>Validation results are automatically saved to the output directory:</p> <pre><code># Set custom output directory\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    output_directory='/path/to/reports'\n)\n\n# After validation, check output files:\n# - validation_log_[table_name].log\n# - validation_errors_[table_name].csv\n# - missing_data_stats_[table_name].csv\n</code></pre>"},{"location":"getting-started/basic-usage/#timezone-handling","title":"Timezone Handling","text":"<p>CLIFpy ensures consistent timezone handling:</p> <pre><code># Specify timezone when loading\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'  # All datetime columns converted to this timezone\n)\n\n# Datetime columns are timezone-aware\nprint(table.df['datetime_column'].dt.tz)\n</code></pre>"},{"location":"getting-started/basic-usage/#memory-management","title":"Memory Management","text":"<p>For large datasets:</p> <pre><code># Load only specific columns\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    columns=['patient_id', 'datetime', 'value']\n)\n\n# Load a sample\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    sample_size=10000\n)\n\n# Apply filters during loading\ntable = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    filters={'patient_id': patient_list}\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    table = TableClass.from_file('/path/to/data', 'parquet')\n    table.validate()\n\n    if not table.isvalid():\n        # Handle validation errors\n        error_df = pd.DataFrame(table.errors)\n        error_df.to_csv('validation_errors.csv', index=False)\n\nexcept FileNotFoundError:\n    print(\"Data files not found\")\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the full User Guide</li> <li>Learn about the Orchestrator</li> <li>See table-specific guides</li> <li>View practical examples</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install CLIFpy and its dependencies.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install clifpy\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Clone the repository and install in development mode:</p> <pre><code># Clone the repository\ngit clone https://github.com/Common-Longitudinal-ICU-data-Format/CLIFpy.git\ncd CLIFpy\n\n# Create a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<p>To build the documentation locally:</p> <pre><code>pip install clifpy[docs]\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that CLIFpy is properly installed:</p> <pre><code>import clifpy\nprint(clifpy.__version__)\n</code></pre> <p>You should see the version number (e.g., <code>0.0.1</code>).</p>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>CLIFpy automatically installs the following dependencies:</p> <ul> <li>pandas: Data manipulation and analysis</li> <li>duckdb: SQL analytics engine</li> <li>pyarrow: Parquet file support</li> <li>pytz: Timezone handling</li> <li>matplotlib &amp; seaborn: Visualization (for demos)</li> <li>pytest: Testing framework</li> <li>tqdm: Progress bars</li> <li>marimo: Interactive notebooks</li> </ul>"},{"location":"getting-started/installation/#platform-support","title":"Platform Support","text":"<p>CLIFpy is tested on:</p> <ul> <li>Linux (Ubuntu 20.04+)</li> <li>macOS (10.15+)</li> <li>Windows (10+)</li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using the correct Python environment:</p> <pre><code>which python\npython --version\n</code></pre>"},{"location":"getting-started/installation/#permission-errors","title":"Permission Errors","text":"<p>On some systems, you may need to use <code>pip install --user</code>:</p> <pre><code>pip install --user clifpy\n</code></pre>"},{"location":"getting-started/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency conflicts, consider using a virtual environment:</p> <pre><code>python -m venv clifpy-env\nsource clifpy-env/bin/activate  # On Windows: clifpy-env\\Scripts\\activate\npip install clifpy\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quick Start guide</li> <li>Learn about basic usage</li> <li>Explore the User Guide</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will get you up and running with CLIFpy in just a few minutes.</p>"},{"location":"getting-started/quickstart/#loading-demo-data","title":"Loading Demo Data","text":"<p>CLIFpy includes demo data to help you get started:</p> <pre><code>from clifpy.data import load_dataset\n\n# Load all demo tables\ntables = load_dataset()\n\n# Access individual tables\npatient_df = tables['patient']\nlabs_df = tables['labs']\nvitals_df = tables['vitals']\n</code></pre>"},{"location":"getting-started/quickstart/#using-individual-tables","title":"Using Individual Tables","text":""},{"location":"getting-started/quickstart/#loading-a-single-table","title":"Loading a Single Table","text":"<pre><code>from clifpy.tables import Patient\n\n# Load patient data from files\npatient = Patient.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Validate the data\npatient.validate()\n\n# Check if data is valid\nif patient.isvalid():\n    print(\"Data validation passed!\")\nelse:\n    print(f\"Found {len(patient.errors)} validation errors\")\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-lab-data","title":"Working with Lab Data","text":"<pre><code>from clifpy.tables import Labs\n\n# Load lab data\nlabs = Labs.from_file('/path/to/data', 'parquet')\n\n# Get recent lab results\nrecent_labs = labs.get_recent(hours=24)\n\n# Filter by lab category\nchemistry_labs = labs.filter_by_category('chemistry')\n\n# Get common lab panels\ncbc = labs.get_common_labs('cbc')\nbmp = labs.get_common_labs('bmp')\n</code></pre>"},{"location":"getting-started/quickstart/#analyzing-vital-signs","title":"Analyzing Vital Signs","text":"<pre><code>from clifpy.tables import Vitals\n\n# Load vitals data\nvitals = Vitals.from_file('/path/to/data', 'parquet')\n\n# Get specific vital types\nheart_rates = vitals.filter_by_vital_type('heart_rate')\nblood_pressures = vitals.filter_by_vital_type('sbp')\n\n# Calculate summary statistics\nhr_stats = vitals.get_summary_by_vital_type()\nprint(hr_stats)\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-orchestrator","title":"Using the Orchestrator","text":"<p>For working with multiple tables at once:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize orchestrator\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Load multiple tables\norchestrator.initialize(\n    tables=['patient', 'labs', 'vitals', 'adt'],\n    sample_size=1000  # Optional: load sample for testing\n)\n\n# Validate all tables\norchestrator.validate_all()\n\n# Get summary of loaded tables\nloaded = orchestrator.get_loaded_tables()\nprint(f\"Loaded tables: {loaded}\")\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#filtering-by-patient","title":"Filtering by Patient","text":"<pre><code># Get data for specific patients\npatient_ids = ['P001', 'P002', 'P003']\n\n# Filter labs\npatient_labs = labs.df[labs.df['patient_id'].isin(patient_ids)]\n\n# Filter vitals\npatient_vitals = vitals.df[vitals.df['patient_id'].isin(patient_ids)]\n</code></pre>"},{"location":"getting-started/quickstart/#time-based-analysis","title":"Time-based Analysis","text":"<pre><code>from datetime import datetime, timedelta\n\n# Get data from the last 7 days\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=7)\n\n# Filter ADT movements\nrecent_movements = adt.filter_by_date_range(start_date, end_date)\n\n# Get ICU admissions\nicu_admissions = adt.filter_by_location_category('icu')\n</code></pre>"},{"location":"getting-started/quickstart/#clinical-calculations","title":"Clinical Calculations","text":"<pre><code># Calculate SOFA scores\nfrom clifpy.tables import PatientAssessments\n\nassessments = PatientAssessments.from_file('/path/to/data', 'parquet')\n\n# Get assessment trends\ngcs_trend = assessments.get_assessment_trend(\n    patient_id='P001',\n    assessment_category='neurological',\n    hours=48\n)\n\n# Check compliance\ncompliance = assessments.get_assessment_compliance(\n    assessment_category='pain',\n    expected_frequency_hours=4\n)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about basic usage patterns</li> <li>Explore the full User Guide</li> <li>See more examples</li> <li>Read the API documentation</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the CLIFpy User Guide. This guide provides comprehensive documentation for working with CLIF data using CLIFpy.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>CLIFpy is designed to make working with CLIF (Common Longitudinal ICU data Format) data straightforward and efficient. Whether you're a researcher analyzing ICU outcomes, a data scientist building predictive models, or a clinician exploring patient data, this guide will help you make the most of CLIFpy.</p>"},{"location":"user-guide/#guide-organization","title":"Guide Organization","text":""},{"location":"user-guide/#clif-orchestrator","title":"CLIF Orchestrator","text":"<p>Learn how to manage multiple CLIF tables simultaneously with consistent configuration and validation.</p>"},{"location":"user-guide/#wide-dataset-creation","title":"Wide Dataset Creation","text":"<p>Create comprehensive time-series datasets by joining multiple CLIF tables with automatic pivoting and high-performance processing.</p>"},{"location":"user-guide/#outlier-handling","title":"Outlier Handling","text":"<p>Detect and remove physiologically implausible values using configurable ranges and category-specific validation.</p>"},{"location":"user-guide/#tables","title":"Tables","text":"<p>Detailed guides for each CLIF table type: </p> <ul> <li>Patient demographics</li> <li>ADT (Admission, Discharge, Transfer) events</li> <li>Hospitalization information</li> <li>Laboratory results</li> <li>Vital signs</li> <li>Respiratory support</li> <li>Medication administration</li> <li>Clinical assessments</li> <li>Patient positioning</li> </ul>"},{"location":"user-guide/#data-validation","title":"Data Validation","text":"<p>Understand how CLIFpy validates your data against CLIF schemas and how to interpret validation results.</p>"},{"location":"user-guide/#working-with-timezones","title":"Working with Timezones","text":"<p>Learn best practices for handling timezone-aware datetime data across different hospital systems.</p>"},{"location":"user-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"user-guide/#table-based-architecture","title":"Table-Based Architecture","text":"<p>CLIFpy organizes ICU data into standardized tables, each representing a specific aspect of patient care:</p> <pre><code>from clifpy.tables import Patient, Labs, Vitals\n\n# Each table is a self-contained unit\npatient = Patient.from_file('/data', 'parquet')\nlabs = Labs.from_file('/data', 'parquet')\nvitals = Vitals.from_file('/data', 'parquet')\n</code></pre>"},{"location":"user-guide/#consistent-interface","title":"Consistent Interface","text":"<p>All tables share common methods inherited from <code>BaseTable</code>: </p> <ul> <li><code>from_file()</code> - Load data from files</li> <li><code>validate()</code> - Run comprehensive validation</li> <li><code>isvalid()</code> - Check validation status</li> <li><code>get_summary()</code> - Get table statistics</li> </ul>"},{"location":"user-guide/#standardized-categories","title":"Standardized Categories","text":"<p>CLIF defines standardized categories for consistent data representation: - Lab categories: chemistry, hematology, coagulation, etc. - Location categories: icu, ward, ed, etc. - Medication groups: vasopressor, sedative, antibiotic, etc.</p>"},{"location":"user-guide/#timezone-awareness","title":"Timezone Awareness","text":"<p>All datetime columns are timezone-aware to handle data from different time zones correctly:</p> <pre><code># Specify timezone when loading\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"user-guide/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/#loading-and-validating-data","title":"Loading and Validating Data","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Load multiple tables\norchestrator = ClifOrchestrator('/data', 'parquet', 'US/Central')\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Validate all tables\norchestrator.validate_all()\n\n# Check validation status\nfor table_name in orchestrator.get_loaded_tables():\n    table = getattr(orchestrator, table_name)\n    print(f\"{table_name}: {'Valid' if table.isvalid() else 'Invalid'}\")\n</code></pre>"},{"location":"user-guide/#filtering-and-analysis","title":"Filtering and Analysis","text":"<pre><code># Category-based filtering\nicu_stays = adt.filter_by_location_category('icu')\n\n# Patient cohort analysis \ncohort_ids = ['P001', 'P002', 'P003']\ncohort_vitals = vitals.df[vitals.df['hospitalization_id'].isin(cohort_ids)]\n</code></pre>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate data after loading to ensure compliance with CLIF standards</li> <li>Use appropriate timezones for your data source</li> <li>Filter early to reduce memory usage with large datasets</li> <li>Review validation errors to understand data quality issues</li> <li>Use the orchestrator when working with multiple related tables</li> </ol>"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Explore specific table guides</li> <li>Learn about data validation</li> <li>See practical examples</li> <li>Review the API reference</li> </ul>"},{"location":"user-guide/orchestrator/","title":"CLIF Orchestrator","text":"<p>The <code>ClifOrchestrator</code> class provides a centralized interface for managing multiple CLIF tables with consistent configuration. This guide covers how to use the orchestrator effectively.</p>"},{"location":"user-guide/orchestrator/#overview","title":"Overview","text":"<p>The orchestrator simplifies working with multiple CLIF tables by:</p> <ul> <li>Ensuring consistent configuration across all tables</li> <li>Providing bulk operations (load, validate)</li> <li>Managing shared settings (timezone, file format, output directory)</li> <li>Offering a unified interface for multi-table workflows</li> </ul>"},{"location":"user-guide/orchestrator/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/orchestrator/#initialization","title":"Initialization","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Create orchestrator with your data configuration\norchestrator = ClifOrchestrator(\n    data_directory='/path/to/clif/data',\n    filetype='parquet',  # or 'csv'\n    timezone='US/Central',\n    output_directory='/path/to/outputs'  # Optional\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#loading-tables","title":"Loading Tables","text":"<pre><code># Load specific tables\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Load all available tables\nall_tables = ['patient', 'hospitalization', 'adt', 'labs', 'vitals',\n              'medication_admin_continuous', 'patient_assessments',\n              'respiratory_support', 'position']\norchestrator.initialize(tables=all_tables)\n\n# Load with sampling (useful for testing)\norchestrator.initialize(\n    tables=['patient', 'labs'],\n    sample_size=1000\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#accessing-tables","title":"Accessing Tables","text":"<p>Once loaded, tables are available as attributes:</p> <pre><code># Access individual tables\npatient_data = orchestrator.patient\nlabs_data = orchestrator.labs\nvitals_data = orchestrator.vitals\n\n# Get the underlying DataFrames\npatient_df = orchestrator.patient.df\nlabs_df = orchestrator.labs.df\n</code></pre>"},{"location":"user-guide/orchestrator/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/orchestrator/#selective-column-loading","title":"Selective Column Loading","text":"<p>Load only specific columns to reduce memory usage:</p> <pre><code>orchestrator.initialize(\n    tables=['labs', 'vitals'],\n    columns={\n        'labs': ['hospitalization_id', 'lab_result_dttm', 'lab_value', 'lab_name'],\n        'vitals': ['hospitalization_id', 'recorded_dttm', 'vital_value']\n    }\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#filtered-loading","title":"Filtered Loading","text":"<p>Apply filters during loading:</p> <pre><code># Filter by patient IDs\nhospitalization_ids = ['P001', 'P002', 'P003']\norchestrator.initialize(\n    tables=['labs', 'vitals'],\n    filters={\n        'labs': {'hospitalization_id': patient_ids},\n        'vitals': {'hospitalization_id': patient_ids}\n    }\n)\n\n# Filter by categories\norchestrator.initialize(\n    tables=['labs', 'adt'],\n    filters={\n        'labs': {'lab_category': 'lactate'},\n        'adt': {'location_category': 'icu'}\n    }\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#validation-workflow","title":"Validation Workflow","text":"<pre><code># Validate all loaded tables\norchestrator.validate_all()\n\n# Check which tables are valid\nfor table_name in orchestrator.get_loaded_tables():\n    table = getattr(orchestrator, table_name)\n    if table.isvalid():\n        print(f\"\u2713 {table_name} passed validation\")\n    else:\n        print(f\"\u2717 {table_name} has {len(table.errors)} errors\")\n</code></pre>"},{"location":"user-guide/orchestrator/#utility-methods","title":"Utility Methods","text":""},{"location":"user-guide/orchestrator/#get-loaded-tables","title":"Get Loaded Tables","text":"<pre><code># List of loaded table names\nloaded = orchestrator.get_loaded_tables()\nprint(f\"Loaded tables: {', '.join(loaded)}\")\n\n# Get table objects\ntable_objects = orchestrator.get_tables_obj_list()\nfor table in table_objects:\n    print(f\"{table.table_name}: {len(table.df)} rows\")\n</code></pre>"},{"location":"user-guide/orchestrator/#individual-table-loading","title":"Individual Table Loading","text":"<p>You can also load tables individually:</p> <pre><code># Load tables one at a time\norchestrator.load_patient_data(sample_size=1000)\norchestrator.load_labs_data(\n    columns=['hospitalization_id', 'lab_result_dttm', 'lab_value']\n)\norchestrator.load_vitals_data(\n    filters={'vital_category': ['heart_rate', 'sbp', 'dbp']}\n)\n</code></pre>"},{"location":"user-guide/orchestrator/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/orchestrator/#multi-table-analysis","title":"Multi-table Analysis","text":"<pre><code># Load related tables for ICU analysis\norchestrator.initialize(\n    tables=['patient', 'adt', 'labs', 'vitals', 'respiratory_support']\n)\n\n# Get ICU patients\nicu_stays = orchestrator.adt.filter_by_location_category('icu')\nicu_patient_ids = icu_stays['patient_id'].unique()\n\n# Analyze their labs\nicu_labs = orchestrator.labs.df[\n    orchestrator.labs.df['patient_id'].isin(icu_patient_ids)\n]\n\n# Check ventilation status\nvent_patients = orchestrator.respiratory_support.df[\n    orchestrator.respiratory_support.df['device_category'] == 'IMV'\n]['patient_id'].unique()\n</code></pre>"},{"location":"user-guide/orchestrator/#best-practices","title":"Best Practices","text":"<ol> <li>Load only what you need: Use column and filter parameters to reduce memory usage</li> <li>Validate early: Run validation immediately after loading to catch issues</li> <li>Use consistent timezones: The orchestrator ensures all tables use the same timezone</li> <li>Check output directory: Validation reports and logs are saved to the output directory</li> <li>Handle missing tables gracefully: Check if a table is loaded before accessing it</li> </ol>"},{"location":"user-guide/orchestrator/#error-handling","title":"Error Handling","text":"<pre><code># Check if table is loaded\nif orchestrator.labs is not None:\n    labs_data = orchestrator.labs.df\nelse:\n    print(\"Labs table not loaded\")\n\n# Handle validation errors\norchestrator.validate_all()\nfor table_name in orchestrator.get_loaded_tables():\n    table = getattr(orchestrator, table_name)\n    if not table.isvalid():\n        # Save errors for review\n        error_file = f\"{table_name}_errors.csv\"\n        pd.DataFrame(table.errors).to_csv(error_file, index=False)\n        print(f\"Saved {len(table.errors)} errors to {error_file}\")\n</code></pre>"},{"location":"user-guide/orchestrator/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about individual table types</li> <li>Understand data validation</li> <li>See practical examples</li> </ul>"},{"location":"user-guide/outlier-handling/","title":"Outlier Handling","text":"<p>The outlier handling functionality in CLIFpy automatically identifies and removes physiologically implausible values from clinical data. This data cleaning process converts outlier values to NaN while preserving the data structure, ensuring that downstream analysis operates on clinically reasonable values.</p>"},{"location":"user-guide/outlier-handling/#overview","title":"Overview","text":"<p>Outlier handling provides:</p> <ul> <li>Automated detection of values outside clinically reasonable ranges</li> <li>Category-specific ranges for different vital signs, lab tests, medications, and assessments</li> <li>Unit-aware validation for medication dosing based on category and unit combinations</li> <li>Configurable ranges using either CLIF standard ranges or custom configurations</li> <li>Detailed statistics showing the impact of outlier removal</li> <li>Non-destructive preview capability to assess outliers before removal</li> </ul>"},{"location":"user-guide/outlier-handling/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/outlier-handling/#apply_outlier_handling","title":"<code>apply_outlier_handling()</code>","text":"<p>Applies outlier handling by converting out-of-range values to NaN:</p> <pre><code>from clifpy.utils import apply_outlier_handling\n\n# Modify data in-place using CLIF standard ranges\napply_outlier_handling(vitals_table)\n\n# Or use custom configuration\napply_outlier_handling(vitals_table, outlier_config_path=\"/path/to/custom_config.yaml\")\n</code></pre> <p>Parameters: - <code>table_obj</code>: A CLIFpy table object with <code>.df</code> and <code>.table_name</code> attributes - <code>outlier_config_path</code> (optional): Path to custom YAML configuration file</p> <p>Returns: None (modifies table data in-place)</p>"},{"location":"user-guide/outlier-handling/#get_outlier_summary","title":"<code>get_outlier_summary()</code>","text":"<p>Provides a preview of outliers without modifying data:</p> <pre><code>from clifpy.utils import get_outlier_summary\n\n# Get summary without modifying data\nsummary = get_outlier_summary(vitals_table)\nprint(f\"Total rows: {summary['total_rows']}\")\nprint(f\"Config source: {summary['config_source']}\")\n</code></pre> <p>Parameters: - <code>table_obj</code>: A CLIFpy table object with <code>.df</code> and <code>.table_name</code> attributes - <code>outlier_config_path</code> (optional): Path to custom YAML configuration file</p> <p>Returns: Dictionary with outlier analysis summary</p>"},{"location":"user-guide/outlier-handling/#configuration-types","title":"Configuration Types","text":""},{"location":"user-guide/outlier-handling/#internal-clif-standard-configuration","title":"Internal CLIF Standard Configuration","text":"<p>By default, CLIFpy uses internal clinically-validated ranges:</p> <pre><code>from clifpy.utils import apply_outlier_handling\n\n# Uses internal CLIF standard ranges automatically\napply_outlier_handling(vitals_table)\n# Output: \"Using CLIF standard outlier ranges\"\n</code></pre> <p>The internal configuration includes ranges for: - Vitals: Heart rate (0-300), blood pressure (0-300/0-200), temperature (32-44\u00b0C), etc. - Labs: Hemoglobin (2-25), sodium (90-210), glucose (0-2000), lactate (0-30), etc. - Medications: Unit-specific dosing ranges (e.g., norepinephrine 0-3 mcg/kg/min) - Assessments: Scale-specific ranges (e.g., GCS 3-15, RASS -5 to +4)</p>"},{"location":"user-guide/outlier-handling/#custom-yaml-configuration","title":"Custom YAML Configuration","text":"<p>Create custom configurations for specific research needs:</p> <pre><code># Apply custom ranges\napply_outlier_handling(vitals_table, outlier_config_path=\"/path/to/custom_ranges.yaml\")\n# Output: \"Using custom outlier ranges from: /path/to/custom_ranges.yaml\"\n</code></pre>"},{"location":"user-guide/outlier-handling/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/outlier-handling/#example-1-basic-usage-with-standard-ranges","title":"Example 1: Basic Usage with Standard Ranges","text":"<pre><code>from clifpy import Vitals\nfrom clifpy.utils import apply_outlier_handling\n\n# Load vitals data\nvitals = Vitals.from_file('/data', 'parquet', 'UTC')\n\nprint(f\"Before: {vitals.df['vital_value'].notna().sum()} non-null values\")\n\n# Apply outlier handling with CLIF standard ranges\napply_outlier_handling(vitals)\n\nprint(f\"After: {vitals.df['vital_value'].notna().sum()} non-null values\")\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-2-preview-outliers-before-removal","title":"Example 2: Preview Outliers Before Removal","text":"<pre><code>from clifpy.utils import get_outlier_summary, apply_outlier_handling\n\n# Get summary without modifying data\nsummary = get_outlier_summary(vitals)\nprint(\"Outlier Analysis Summary:\")\nprint(f\"- Table: {summary['table_name']}\")\nprint(f\"- Total rows: {summary['total_rows']}\")\nprint(f\"- Configuration: {summary['config_source']}\")\n\n# Apply outlier handling after review\napply_outlier_handling(vitals)\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-3-custom-configuration-for-research","title":"Example 3: Custom Configuration for Research","text":"<pre><code># Create custom configuration file\ncustom_config = \"\"\"\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 40    # More restrictive than standard (0)\n        max: 180   # More restrictive than standard (300)\n      temp_c:\n        min: 35.0  # More restrictive than standard (32)\n        max: 42.0  # More restrictive than standard (44)\n\"\"\"\n\nwith open('research_config.yaml', 'w') as f:\n    f.write(custom_config)\n\n# Apply custom ranges\napply_outlier_handling(vitals, outlier_config_path='research_config.yaml')\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-4-multiple-tables-with-different-configurations","title":"Example 4: Multiple Tables with Different Configurations","text":"<pre><code>from clifpy import Labs, Vitals, MedicationAdminContinuous\nfrom clifpy.utils import apply_outlier_handling\n\n# Load tables\nvitals = Vitals.from_file('/data', 'parquet', 'UTC')\nlabs = Labs.from_file('/data', 'parquet', 'UTC')\nmeds = MedicationAdminContinuous.from_file('/data', 'parquet', 'UTC')\n\n# Apply outlier handling to each table\nfor table in [vitals, labs, meds]:\n    print(f\"\\n=== Processing {table.table_name} ===\")\n    apply_outlier_handling(table)\n</code></pre>"},{"location":"user-guide/outlier-handling/#table-specific-handling","title":"Table-Specific Handling","text":""},{"location":"user-guide/outlier-handling/#simple-range-columns","title":"Simple Range Columns","text":"<p>For columns with straightforward min/max ranges:</p> <pre><code># Example: Age at admission (0-120 years)\n# Configuration:\nhospitalization:\n  age_at_admission:\n    min: 0\n    max: 120\n\n# Output statistics:\n# age_at_admission              :   5432 values \u2192     23 nullified ( 0.4%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#category-dependent-columns","title":"Category-Dependent Columns","text":"<p>For vitals, labs, and assessments where ranges depend on the category:</p> <pre><code># Example: Vital signs with different ranges per category\n# Configuration:\nvitals:\n  vital_value:\n    heart_rate:\n      min: 0\n      max: 300\n    temp_c:\n      min: 32\n      max: 44\n\n# Output statistics:\n# Vitals Table - Category Statistics:\n#   heart_rate        :  15234 values \u2192    156 nullified ( 1.0%)\n#   temp_c           :   8765 values \u2192     23 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#unit-dependent-medication-dosing","title":"Unit-Dependent Medication Dosing","text":"<p>For medications where ranges depend on both category and unit:</p> <pre><code># Example: Norepinephrine dosing with different units\n# Configuration:\nmedication_admin_continuous:\n  med_dose:\n    norepinephrine:\n      \"mcg/kg/min\":\n        min: 0.0\n        max: 3.0\n      \"mcg/min\":\n        min: 0.0\n        max: 200.0\n\n# Output statistics:\n# Medication Table - Category/Unit Statistics:\n#   norepinephrine (mcg/kg/min)  :   2341 values \u2192     12 nullified ( 0.5%)\n#   norepinephrine (mcg/min)     :    876 values \u2192      4 nullified ( 0.5%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#custom-yaml-configuration-examples","title":"Custom YAML Configuration Examples","text":""},{"location":"user-guide/outlier-handling/#example-1-research-specific-vitals-ranges","title":"Example 1: Research-Specific Vitals Ranges","text":"<pre><code># custom_vitals_config.yaml\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 50     # More restrictive for adults\n        max: 150    # Exclude extreme tachycardia\n      sbp:\n        min: 70     # Focus on hypotension\n        max: 200    # Exclude severe hypertension\n      temp_c:\n        min: 36.0   # Normothermic range\n        max: 39.0   # Exclude extreme hyperthermia\n      spo2:\n        min: 88     # Allow mild hypoxemia\n        max: 100    # Standard upper bound\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-2-pediatric-specific-ranges","title":"Example 2: Pediatric-Specific Ranges","text":"<pre><code># pediatric_config.yaml\ntables:\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 60     # Pediatric range\n        max: 200    # Higher for children\n      sbp:\n        min: 60     # Lower for pediatrics\n        max: 140\n\n  hospitalization:\n    age_at_admission:\n      min: 0\n      max: 18     # Pediatric patients only\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-3-icu-specific-lab-ranges","title":"Example 3: ICU-Specific Lab Ranges","text":"<pre><code># icu_lab_config.yaml\ntables:\n  labs:\n    lab_value_numeric:\n      lactate:\n        min: 0.5    # Minimum detectable\n        max: 20.0   # ICU-relevant range\n      hemoglobin:\n        min: 4.0    # Severe anemia threshold\n        max: 20.0   # Exclude transfusion artifacts\n      creatinine:\n        min: 0.3    # Physiologic minimum\n        max: 15.0   # Include severe AKI\n</code></pre>"},{"location":"user-guide/outlier-handling/#example-4-complete-custom-configuration-template","title":"Example 4: Complete Custom Configuration Template","text":"<pre><code># complete_custom_config.yaml\ntables:\n  # Simple range columns\n  hospitalization:\n    age_at_admission:\n      min: 18      # Adult patients only\n      max: 100     # Exclude very elderly\n\n  respiratory_support:\n    fio2_set:\n      min: 0.21    # Room air minimum\n      max: 1.0     # 100% oxygen maximum\n    peep_set:\n      min: 0       # No PEEP\n      max: 25      # High PEEP limit\n\n  # Category-dependent columns\n  vitals:\n    vital_value:\n      heart_rate:\n        min: 40\n        max: 200\n      sbp:\n        min: 60\n        max: 250\n      temp_c:\n        min: 35.0\n        max: 42.0\n\n  labs:\n    lab_value_numeric:\n      hemoglobin:\n        min: 5.0\n        max: 18.0\n      sodium:\n        min: 120\n        max: 160\n\n  # Unit-dependent medication dosing\n  medication_admin_continuous:\n    med_dose:\n      norepinephrine:\n        \"mcg/kg/min\":\n          min: 0.01\n          max: 2.0\n        \"mcg/min\":\n          min: 1.0\n          max: 150.0\n      propofol:\n        \"mg/hr\":\n          min: 1.0\n          max: 300.0\n\n  # Assessment-specific ranges\n  patient_assessments:\n    numerical_value:\n      gcs_total:\n        min: 3\n        max: 15\n      RASS:\n        min: -5\n        max: 4\n</code></pre>"},{"location":"user-guide/outlier-handling/#understanding-output-statistics","title":"Understanding Output Statistics","text":"<p>The outlier handling provides detailed statistics showing the impact of data cleaning:</p>"},{"location":"user-guide/outlier-handling/#category-dependent-statistics","title":"Category-Dependent Statistics","text":"<pre><code>Vitals Table - Category Statistics:\n  heart_rate        :  15234 values \u2192    156 nullified ( 1.0%)\n  sbp              :  12876 values \u2192     45 nullified ( 0.3%)\n  temp_c           :   8765 values \u2192     23 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#medication-unit-dependent-statistics","title":"Medication Unit-Dependent Statistics","text":"<pre><code>Medication Table - Category/Unit Statistics:\n  norepinephrine (mcg/kg/min)  :   2341 values \u2192     12 nullified ( 0.5%)\n  propofol (mg/hr)            :   1876 values \u2192      8 nullified ( 0.4%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#simple-range-statistics","title":"Simple Range Statistics","text":"<pre><code>age_at_admission              :   5432 values \u2192     23 nullified ( 0.4%)\nfio2_set                     :   3456 values \u2192     12 nullified ( 0.3%)\n</code></pre>"},{"location":"user-guide/outlier-handling/#integration-with-cliforchestrator","title":"Integration with ClifOrchestrator","text":"<p>The outlier handling integrates seamlessly with the ClifOrchestrator workflow:</p> <pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\nfrom clifpy.utils import apply_outlier_handling\n\n# Initialize orchestrator and load tables\nco = ClifOrchestrator('/data', 'parquet', 'UTC')\nco.initialize(['vitals', 'labs', 'medication_admin_continuous'])\n\n# Apply outlier handling to all loaded tables\nfor table_name in co.get_loaded_tables():\n    table_obj = getattr(co, table_name)\n    print(f\"\\n=== Cleaning {table_name} ===\")\n    apply_outlier_handling(table_obj)\n\n# Validate after outlier handling\nco.validate_all()\n\n# Create wide dataset with clean data\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin', 'sodium']\n    }\n)\n</code></pre>"},{"location":"user-guide/outlier-handling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/outlier-handling/#1-preview-before-application","title":"1. Preview Before Application","text":"<pre><code># Always preview outliers first\nsummary = get_outlier_summary(table)\nprint(f\"Will affect {summary['total_rows']} rows\")\n\n# Apply after review\napply_outlier_handling(table)\n</code></pre>"},{"location":"user-guide/outlier-handling/#2-keep-original-data","title":"2. Keep Original Data","text":"<pre><code># Make backup before outlier handling\noriginal_df = vitals.df.copy()\n\n# Apply outlier handling\napply_outlier_handling(vitals)\n\n# Compare results\nprint(f\"Original: {original_df['vital_value'].notna().sum()} values\")\nprint(f\"Cleaned:  {vitals.df['vital_value'].notna().sum()} values\")\nprint(f\"Removed:  {original_df['vital_value'].notna().sum() - vitals.df['vital_value'].notna().sum()} values\")\n</code></pre>"},{"location":"user-guide/outlier-handling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/outlier-handling/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>No Configuration Found <pre><code># Error: \"No outlier configuration found for table: custom_table\"\n# Solution: Add table configuration to your custom YAML\n\ntables:\n  custom_table:\n    numeric_column:\n      min: 0\n      max: 100\n</code></pre></p> <p>Missing Columns <pre><code># Warning: Configuration references columns not in data\n# Solution: Check column names in your data vs. configuration\nprint(vitals.df.columns.tolist())  # Check actual column names\n</code></pre></p> <p>No Outliers Detected <pre><code># All values are within range - this is normal for clean data\n# The statistics will show \"0 nullified\" for all categories\n</code></pre></p>"},{"location":"user-guide/outlier-handling/#internal-clif-standard-ranges","title":"Internal CLIF Standard Ranges","text":"<p>The internal configuration includes clinically-validated ranges for:</p>"},{"location":"user-guide/outlier-handling/#vitals","title":"Vitals","text":"<ul> <li>Heart rate: 0-300 bpm</li> <li>Blood pressure: SBP 0-300, DBP 0-200, MAP 0-250 mmHg  </li> <li>Temperature: 32-44\u00b0C</li> <li>SpO2: 50-100%</li> <li>Respiratory rate: 0-60/min</li> <li>Height: 76-255 cm, Weight: 30-1100 kg</li> </ul>"},{"location":"user-guide/outlier-handling/#common-labs","title":"Common Labs","text":"<ul> <li>Hemoglobin: 2.0-25.0 g/dL</li> <li>Sodium: 90-210 mEq/L</li> <li>Potassium: 0-15 mEq/L</li> <li>Creatinine: 0-20 mg/dL</li> <li>Glucose: 0-2000 mg/dL</li> <li>Lactate: 0-30 mmol/L</li> </ul>"},{"location":"user-guide/outlier-handling/#medication-dosing-examples","title":"Medication Dosing (examples)","text":"<ul> <li>Norepinephrine: 0-3 mcg/kg/min, 0-200 mcg/min</li> <li>Propofol: 0-400 mg/hr, 0-200 mcg/kg/min</li> <li>Fentanyl: 0-500 mcg/hr, 0-10 mcg/kg/hr</li> </ul>"},{"location":"user-guide/outlier-handling/#clinical-assessments","title":"Clinical Assessments","text":"<ul> <li>GCS Total: 3-15</li> <li>RASS: -5 to +4</li> <li>Richmond Agitation Sedation Scale: 1-7</li> <li>Braden Total: 6-23</li> </ul>"},{"location":"user-guide/outlier-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about data validation to ensure data quality after outlier removal</li> <li>Explore wide dataset creation with cleaned data</li> <li>Review individual table guides for table-specific considerations</li> <li>See the orchestrator guide for workflow integration</li> </ul>"},{"location":"user-guide/timezones/","title":"Working with Timezones","text":"<p>Proper timezone handling is critical when working with ICU data from multiple sources. This guide explains how CLIFpy manages timezones and best practices for your data.</p>"},{"location":"user-guide/timezones/#overview","title":"Overview","text":"<p>CLIFpy ensures all datetime columns are timezone-aware to: - Prevent ambiguity in timestamp interpretation - Enable accurate time-based calculations - Support data from multiple time zones - Maintain consistency across tables</p>"},{"location":"user-guide/timezones/#timezone-specification","title":"Timezone Specification","text":""},{"location":"user-guide/timezones/#when-loading-data","title":"When Loading Data","text":"<p>Always specify the timezone when loading data:</p> <pre><code># Specify source data timezone\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'  # Source data timezone\n)\n\n# Common US timezones\n# 'US/Eastern', 'US/Central', 'US/Mountain', 'US/Pacific'\n# 'America/New_York', 'America/Chicago', 'America/Denver', 'America/Los_Angeles'\n</code></pre>"},{"location":"user-guide/timezones/#using-orchestrator","title":"Using Orchestrator","text":"<p>The orchestrator ensures consistent timezone across all tables:</p> <pre><code>orchestrator = ClifOrchestrator(\n    data_directory='/data',\n    filetype='parquet',\n    timezone='US/Central'  # Applied to all tables\n)\n</code></pre>"},{"location":"user-guide/timezones/#timezone-conversion","title":"Timezone Conversion","text":""},{"location":"user-guide/timezones/#during-loading","title":"During Loading","text":"<p>CLIFpy automatically converts datetime columns to the specified timezone:</p> <pre><code># Original data in UTC\ntable = TableClass.from_file('/data', 'parquet', timezone='UTC')\n\n# Convert to Central time during loading\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#after-loading","title":"After Loading","text":"<p>Convert between timezones using pandas:</p> <pre><code># Convert to different timezone\ndf = table.df.copy()\ndf['lab_datetime'] = df['lab_datetime'].dt.tz_convert('US/Eastern')\n\n# Localize timezone-naive data (not recommended)\n# df['datetime'] = df['datetime'].dt.tz_localize('US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#common-timezone-issues","title":"Common Timezone Issues","text":""},{"location":"user-guide/timezones/#issue-1-timezone-naive-data","title":"Issue 1: Timezone-Naive Data","text":"<p>Problem: Source data lacks timezone information</p> <pre><code># This will fail validation\ntable.validate()\n# Error: \"Datetime column 'admission_date' is not timezone-aware\"\n</code></pre> <p>Solution: Specify timezone during loading</p> <pre><code># CLIFpy will localize to specified timezone\ntable = TableClass.from_file(\n    '/data', \n    'parquet', \n    timezone='US/Central'  # Assumes data is in Central time\n)\n</code></pre>"},{"location":"user-guide/timezones/#issue-2-mixed-timezones","title":"Issue 2: Mixed Timezones","text":"<p>Problem: Different tables from different timezones</p> <pre><code># Hospital A in Eastern time\nlabs_a = Labs.from_file('/hospital_a/data', 'parquet', timezone='US/Eastern')\n\n# Hospital B in Pacific time  \nlabs_b = Labs.from_file('/hospital_b/data', 'parquet', timezone='US/Pacific')\n</code></pre> <p>Solution: Convert to common timezone</p> <pre><code># Convert both to UTC for analysis\nlabs_a.df['lab_datetime'] = labs_a.df['lab_datetime'].dt.tz_convert('UTC')\nlabs_b.df['lab_datetime'] = labs_b.df['lab_datetime'].dt.tz_convert('UTC')\n\n# Combine datasets\ncombined_labs = pd.concat([labs_a.df, labs_b.df])\n</code></pre>"},{"location":"user-guide/timezones/#issue-3-daylight-saving-time","title":"Issue 3: Daylight Saving Time","text":"<p>Problem: Ambiguous times during DST transitions</p> <pre><code># Fall back: 2:00 AM occurs twice\n# Spring forward: 2:00 AM doesn't exist\n</code></pre> <p>Solution: Use pytz-aware timezone names</p> <pre><code># Good - handles DST automatically\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n\n# Avoid - doesn't handle DST\n# table = TableClass.from_file('/data', 'parquet', timezone='CST6CDT')\n</code></pre>"},{"location":"user-guide/timezones/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/timezones/#1-know-your-source-timezone","title":"1. Know Your Source Timezone","text":"<pre><code># Document source timezone\n\"\"\"\nData extracted from Hospital EHR\nTimezone: US/Central (America/Chicago)\nIncludes DST adjustments\n\"\"\"\ntable = TableClass.from_file('/data', 'parquet', timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#2-use-consistent-timezones","title":"2. Use Consistent Timezones","text":"<pre><code># Use orchestrator for consistency\norchestrator = ClifOrchestrator('/data', 'parquet', timezone='US/Central')\norchestrator.initialize(tables=['labs', 'vitals', 'medications'])\n\n# All tables now use same timezone\n</code></pre>"},{"location":"user-guide/timezones/#3-validate-timezone-handling","title":"3. Validate Timezone Handling","text":"<pre><code># Check timezone after loading\nprint(f\"Lab datetime timezone: {table.df['lab_datetime'].dt.tz}\")\n\n# Verify reasonable time ranges\nprint(f\"Earliest: {table.df['lab_datetime'].min()}\")\nprint(f\"Latest: {table.df['lab_datetime'].max()}\")\n</code></pre>"},{"location":"user-guide/timezones/#4-document-timezone-conversions","title":"4. Document Timezone Conversions","text":"<pre><code># Keep audit trail of conversions\nmetadata = {\n    'original_timezone': 'US/Eastern',\n    'converted_timezone': 'UTC',\n    'conversion_date': datetime.now(),\n    'conversion_method': 'pandas dt.tz_convert'\n}\n</code></pre>"},{"location":"user-guide/timezones/#time-based-calculations","title":"Time-based Calculations","text":""},{"location":"user-guide/timezones/#duration-calculations","title":"Duration Calculations","text":"<p>Timezone-aware datetimes ensure accurate duration calculations:</p> <pre><code># Calculate length of stay\nlos = adt.df['out_dttm'] - adt.df['in_dttm']\nlos_hours = los.dt.total_seconds() / 3600\n\n# Time since admission\ncurrent_time = pd.Timestamp.now(tz='US/Central')\ntime_since = current_time - hosp.df['admission_dttm']\n</code></pre>"},{"location":"user-guide/timezones/#filtering-by-time","title":"Filtering by Time","text":"<pre><code># Get data from last 24 hours\ncutoff = pd.Timestamp.now(tz='US/Central') - pd.Timedelta(hours=24)\nrecent = table.df[table.df['datetime_column'] &gt;= cutoff]\n\n# Filter to specific date (timezone-aware)\ndate = pd.Timestamp('2023-01-01', tz='US/Central')\nday_data = table.df[table.df['datetime_column'].dt.date == date.date()]\n</code></pre>"},{"location":"user-guide/timezones/#aggregating-by-time","title":"Aggregating by Time","text":"<pre><code># Hourly aggregation\nhourly = table.df.set_index('datetime_column').resample('H').mean()\n\n# Daily aggregation (timezone affects day boundaries!)\ndaily = table.df.set_index('datetime_column').resample('D').count()\n</code></pre>"},{"location":"user-guide/timezones/#multi-site-considerations","title":"Multi-site Considerations","text":"<p>When combining data from multiple sites:</p> <pre><code># Strategy 1: Convert all to UTC\nsites = ['site_a', 'site_b', 'site_c']\nsite_timezones = {\n    'site_a': 'US/Eastern',\n    'site_b': 'US/Central', \n    'site_c': 'US/Pacific'\n}\n\nall_data = []\nfor site in sites:\n    table = Labs.from_file(f'/data/{site}', 'parquet', \n                          timezone=site_timezones[site])\n    # Convert to UTC\n    table.df['lab_datetime'] = table.df['lab_datetime'].dt.tz_convert('UTC')\n    table.df['site'] = site\n    all_data.append(table.df)\n\ncombined = pd.concat(all_data)\n</code></pre> <pre><code># Strategy 2: Use site's local time with site column\n# Keep original timezone but track source\nfor site in sites:\n    table = Labs.from_file(f'/data/{site}', 'parquet',\n                          timezone=site_timezones[site])\n    table.df['site'] = site\n    table.df['source_timezone'] = site_timezones[site]\n</code></pre>"},{"location":"user-guide/timezones/#timezone-reference","title":"Timezone Reference","text":"<p>Common medical facility timezones:</p> <pre><code>US_TIMEZONES = {\n    'Eastern': 'US/Eastern',     # NYC, Boston, Atlanta\n    'Central': 'US/Central',     # Chicago, Houston, Dallas\n    'Mountain': 'US/Mountain',   # Denver, Phoenix\n    'Pacific': 'US/Pacific',     # LA, Seattle, San Francisco\n    'Alaska': 'US/Alaska',       # Anchorage\n    'Hawaii': 'US/Hawaii'        # Honolulu\n}\n\n# International\nINTL_TIMEZONES = {\n    'London': 'Europe/London',\n    'Paris': 'Europe/Paris',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney',\n    'Toronto': 'America/Toronto'\n}\n</code></pre>"},{"location":"user-guide/timezones/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/timezones/#check-current-timezone","title":"Check Current Timezone","text":"<pre><code># For a datetime column\nprint(table.df['datetime_column'].dt.tz)\n\n# For a single timestamp\nprint(table.df['datetime_column'].iloc[0].tzinfo)\n</code></pre>"},{"location":"user-guide/timezones/#fix-timezone-issues","title":"Fix Timezone Issues","text":"<pre><code># If validation fails due to timezone\nif not table.isvalid():\n    tz_errors = [e for e in table.errors if 'timezone' in str(e)]\n    if tz_errors:\n        # Reload with proper timezone\n        table = TableClass.from_file('/data', 'parquet', \n                                   timezone='US/Central')\n</code></pre>"},{"location":"user-guide/timezones/#next-steps","title":"Next Steps","text":"<ul> <li>Review validation guide for timezone validation</li> <li>See examples of timezone handling</li> <li>Learn about multi-site analysis</li> </ul>"},{"location":"user-guide/validation/","title":"Data Validation","text":"<p>CLIFpy provides comprehensive validation to ensure your data conforms to CLIF standards. This guide explains the validation process and how to interpret results.</p>"},{"location":"user-guide/validation/#overview","title":"Overview","text":"<p>Validation in CLIFpy operates at multiple levels:</p> <ol> <li>Schema Validation - Ensures required columns exist with correct data types</li> <li>Category Validation - Verifies values match standardized categories</li> <li>Range Validation - Checks values fall within clinically reasonable ranges</li> <li>Timezone Validation - Ensures datetime columns are timezone-aware</li> <li>Duplicate Detection - Identifies duplicate records based on composite keys</li> <li>Completeness Checks - Analyzes missing data patterns</li> </ol>"},{"location":"user-guide/validation/#running-validation","title":"Running Validation","text":""},{"location":"user-guide/validation/#basic-validation","title":"Basic Validation","text":"<pre><code># Load and validate a table\ntable = TableClass.from_file('/data', 'parquet')\ntable.validate()\n\n# Check if valid\nif table.isvalid():\n    print(\"Validation passed!\")\nelse:\n    print(f\"Found {len(table.errors)} validation errors\")\n</code></pre>"},{"location":"user-guide/validation/#bulk-validation-with-orchestrator","title":"Bulk Validation with Orchestrator","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\norchestrator = ClifOrchestrator('/data', 'parquet')\norchestrator.initialize(tables=['patient', 'labs', 'vitals'])\n\n# Validate all tables\norchestrator.validate_all()\n</code></pre>"},{"location":"user-guide/validation/#understanding-validation-results","title":"Understanding Validation Results","text":""},{"location":"user-guide/validation/#error-types","title":"Error Types","text":"<p>Validation errors are stored in the <code>errors</code> attribute:</p> <pre><code># Review errors\nfor error in table.errors[:10]:  # First 10 errors\n    print(f\"Type: {error['type']}\")\n    print(f\"Message: {error['message']}\")\n    print(f\"Details: {error.get('details', 'N/A')}\")\n    print(\"-\" * 50)\n</code></pre> <p>Common error types: - <code>missing_column</code> - Required column not found - <code>invalid_category</code> - Value not in permissible list - <code>out_of_range</code> - Value outside acceptable range - <code>invalid_timezone</code> - Datetime column not timezone-aware - <code>duplicate_rows</code> - Duplicate records found</p>"},{"location":"user-guide/validation/#validation-reports","title":"Validation Reports","text":"<p>Validation results are automatically saved to the output directory:</p> <pre><code># Set custom output directory\ntable = TableClass.from_file(\n    data_directory='/data',\n    filetype='parquet',\n    output_directory='/path/to/reports'\n)\n\n# After validation, these files are created:\n# - validation_log_[table_name].log\n# - validation_errors_[table_name].csv\n# - missing_data_stats_[table_name].csv\n</code></pre>"},{"location":"user-guide/validation/#schema-validation","title":"Schema Validation","text":"<p>Each table has a YAML schema defining its structure:</p> <pre><code># Example from patient_schema.yaml\ncolumns:\n  - name: patient_id\n    data_type: VARCHAR\n    required: true\n    is_category_column: false\n  - name: sex_category\n    data_type: VARCHAR\n    required: true\n    is_category_column: true\n    permissible_values:\n      - Male\n      - Female\n      - Unknown\n</code></pre>"},{"location":"user-guide/validation/#required-columns","title":"Required Columns","text":"<pre><code># Check which required columns are missing\nif not table.isvalid():\n    missing_cols = [e for e in table.errors if e['type'] == 'missing_column']\n    for error in missing_cols:\n        print(f\"Missing required column: {error['column']}\")\n</code></pre>"},{"location":"user-guide/validation/#data-types","title":"Data Types","text":"<p>CLIFpy validates that columns have appropriate data types: - <code>VARCHAR</code> - String/text data - <code>DATETIME</code> - Timezone-aware datetime - <code>NUMERIC</code> - Numeric values (int or float)</p>"},{"location":"user-guide/validation/#category-validation","title":"Category Validation","text":"<p>Standardized categories ensure consistency across institutions:</p> <pre><code># Example: Validating location categories in ADT\nvalid_locations = ['ed', 'ward', 'stepdown', 'icu', 'procedural', \n                   'l&amp;d', 'hospice', 'psych', 'rehab', 'radiology', \n                   'dialysis', 'other']\n\n# Check for invalid categories\ncategory_errors = [e for e in table.errors \n                   if e['type'] == 'invalid_category']\n</code></pre>"},{"location":"user-guide/validation/#range-validation","title":"Range Validation","text":"<p>Clinical values are checked against reasonable ranges:</p> <pre><code># Example: Vital signs ranges\nranges = {\n    'heart_rate': (0, 300),\n    'sbp': (0, 300),\n    'dbp': (0, 200),\n    'temp_c': (25, 44),\n    'spo2': (50, 100)\n}\n\n# Identify out-of-range values\nrange_errors = [e for e in table.errors \n                if e['type'] == 'out_of_range']\n</code></pre>"},{"location":"user-guide/validation/#timezone-validation","title":"Timezone Validation","text":"<p>All datetime columns must be timezone-aware:</p> <pre><code># Check timezone issues\ntz_errors = [e for e in table.errors \n             if 'timezone' in e.get('message', '').lower()]\n\nif tz_errors:\n    print(\"Datetime columns must be timezone-aware\")\n    print(\"Consider reloading with explicit timezone:\")\n    print(\"table = TableClass.from_file('/data', 'parquet', timezone='US/Central')\")\n</code></pre>"},{"location":"user-guide/validation/#duplicate-detection","title":"Duplicate Detection","text":"<p>Duplicates are identified based on composite keys:</p> <pre><code># Check for duplicates\nduplicate_errors = [e for e in table.errors \n                    if e['type'] == 'duplicate_rows']\n\nif duplicate_errors:\n    for error in duplicate_errors:\n        print(f\"Found {error['count']} duplicate rows\")\n        print(f\"Composite keys: {error['keys']}\")\n</code></pre>"},{"location":"user-guide/validation/#missing-data-analysis","title":"Missing Data Analysis","text":"<p>CLIFpy analyzes missing data patterns:</p> <pre><code># Get missing data statistics\nsummary = table.get_summary()\nif 'missing_data' in summary:\n    print(\"Columns with missing data:\")\n    for col, count in summary['missing_data'].items():\n        pct = (count / summary['num_rows']) * 100\n        print(f\"  {col}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"user-guide/validation/#custom-validation","title":"Custom Validation","text":"<p>Tables may include specific validation logic:</p> <pre><code># Example: Labs table validates reference ranges\n# Example: Medications validates dose units match drug\n# Example: Respiratory support validates device/mode combinations\n</code></pre>"},{"location":"user-guide/validation/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate after loading - Catch issues early</li> <li>Review all error types - Don't just check if valid</li> <li>Save validation reports - Keep audit trail</li> <li>Fix data at source - Update extraction/ETL process</li> <li>Document exceptions - Some errors may be acceptable</li> </ol>"},{"location":"user-guide/validation/#handling-validation-errors","title":"Handling Validation Errors","text":""},{"location":"user-guide/validation/#option-1-fix-and-reload","title":"Option 1: Fix and Reload","text":"<pre><code># Identify issues\ntable.validate()\nerrors_df = pd.DataFrame(table.errors)\nerrors_df.to_csv('validation_errors.csv', index=False)\n\n# Fix source data based on errors\n# Then reload\ntable = TableClass.from_file('/fixed_data', 'parquet')\ntable.validate()\n</code></pre>"},{"location":"user-guide/validation/#option-2-filter-invalid-records","title":"Option 2: Filter Invalid Records","text":"<pre><code># Remove records with invalid categories\nvalid_categories = ['Male', 'Female', 'Unknown']\ncleaned_df = table.df[table.df['sex_category'].isin(valid_categories)]\n\n# Create new table instance with cleaned data\ntable = TableClass(data=cleaned_df, timezone='US/Central')\n</code></pre>"},{"location":"user-guide/validation/#option-3-document-and-proceed","title":"Option 3: Document and Proceed","text":"<pre><code># For acceptable validation errors\nif not table.isvalid():\n    # Document why proceeding despite errors\n    with open('validation_notes.txt', 'w') as f:\n        f.write(f\"Proceeding with {len(table.errors)} known issues:\\n\")\n        f.write(\"- Missing optional columns\\n\")\n        f.write(\"- Historical data outside current ranges\\n\")\n</code></pre>"},{"location":"user-guide/validation/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about timezone handling</li> <li>Explore table-specific guides</li> <li>See practical examples</li> </ul>"},{"location":"user-guide/wide-dataset/","title":"Wide Dataset Creation","text":"<p>The wide dataset functionality enables you to create comprehensive time-series dataset by joining multiple CLIF tables with automatic pivoting and high-performance processing using DuckDB. This feature transforms narrow, category-based data (like vitals and labs) into wide format suitable for machine learning and analysis.</p>"},{"location":"user-guide/wide-dataset/#overview","title":"Overview","text":"<p>Wide dataset creation through the ClifOrchestrator provides:</p> <ul> <li>Automatic table joining across patient, hospitalization, ADT, and optional tables</li> <li>Intelligent pivoting of category-based data (vitals, labs, medications, assessments) </li> <li>High-performance processing using DuckDB for large datasets</li> <li>Memory-efficient batch processing to handle datasets of any size</li> <li>Flexible filtering by hospitalization IDs, time windows, and categories</li> <li>System resource optimization with configurable memory and thread settings</li> </ul> <p>Important: Wide dataset functionality is only available through the <code>ClifOrchestrator</code> and requires specific tables to be loaded.</p>"},{"location":"user-guide/wide-dataset/#quick-start","title":"Quick Start","text":"<pre><code>from clifpy.clif_orchestrator import ClifOrchestrator\n\n# Initialize orchestrator\nco = ClifOrchestrator(\n    data_directory='/path/to/clif/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Create wide dataset with sample data\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['hemoglobin', 'sodium', 'glucose']\n    },\n    sample=True  # Use 20 random hospitalizations for testing\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#function-parameters","title":"Function Parameters","text":"<p>The <code>create_wide_dataset()</code> method accepts the following parameters:</p> Parameter Type Default Description <code>tables_to_load</code> List[str] None Tables to include: 'vitals', 'labs', 'medication_admin_continuous', 'patient_assessments', 'respiratory_support' <code>category_filters</code> Dict[str, List[str]] None Specific categories to pivot for each table <code>sample</code> bool False If True, randomly select 20 hospitalizations for testing <code>hospitalization_ids</code> List[str] None Specific hospitalization IDs to process <code>cohort_df</code> DataFrame None DataFrame with time windows (requires 'hospitalization_id', 'start_time', 'end_time' columns) <code>output_format</code> str 'dataframe' Output format: 'dataframe', 'csv', or 'parquet' <code>save_to_data_location</code> bool False Save output to data directory <code>output_filename</code> str None Custom filename (auto-generated if None) <code>return_dataframe</code> bool True Return DataFrame even when saving to file <code>batch_size</code> int 1000 Number of hospitalizations per batch (use -1 to disable batching) <code>memory_limit</code> str None DuckDB memory limit (e.g., '8GB', '16GB') <code>threads</code> int None Number of threads for DuckDB processing <code>show_progress</code> bool True Show progress bars for long operations"},{"location":"user-guide/wide-dataset/#best-practices-system-resource-management","title":"Best Practices: System Resource Management","text":"<p>Always check your system resources before running wide dataset creation on large datasets:</p> <pre><code># Check system resources first\nresources = co.get_sys_resource_info()\nprint(f\"Available RAM: {resources['memory_available_gb']:.1f} GB\")\nprint(f\"Recommended threads: {resources['max_recommended_threads']}\")\n\n# Configure based on available resources\nmemory_limit = f\"{int(resources['memory_available_gb'] * 0.7)}GB\"  # Use 70% of available RAM\nthreads = resources['max_recommended_threads']\n\n# Create wide dataset with optimized settings\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['hemoglobin', 'sodium']\n    },\n    memory_limit=memory_limit,\n    threads=threads,\n    batch_size=1000  # Adjust based on dataset size\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/wide-dataset/#example-1-development-and-testing","title":"Example 1: Development and Testing","text":"<p>Use sampling for initial development and testing:</p> <pre><code># Sample mode for development\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'dbp', 'spo2', 'respiratory_rate'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium', 'creatinine']\n    },\n    sample=True,  # Only 20 random hospitalizations\n    show_progress=True\n)\n\nprint(f\"Sample dataset shape: {wide_df.shape}\")\nprint(f\"Unique hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-2-production-use-with-resource-optimization","title":"Example 2: Production Use with Resource Optimization","text":"<p>For production use with large datasets:</p> <pre><code># Get system info and configure accordingly\nresources = co.get_sys_resource_info(print_summary=False)\n\n# Production settings\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs', 'medication_admin_continuous'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'dbp', 'spo2', 'temp_c'],\n        'labs': ['hemoglobin', 'wbc', 'sodium', 'potassium'],\n        'medication_admin_continuous': ['norepinephrine', 'propofol', 'fentanyl']\n    },\n    batch_size=500,  # Smaller batches for large datasets\n    memory_limit=\"12GB\",\n    threads=resources['max_recommended_threads'],\n    save_to_data_location=True,\n    output_format='parquet',\n    output_filename='wide_dataset_production'\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-3-targeted-analysis-with-specific-ids","title":"Example 3: Targeted Analysis with Specific IDs","text":"<p>Process specific hospitalizations:</p> <pre><code># Analyze specific patient cohort\ntarget_ids = ['12345', '67890', '11111', '22222']\n\nwide_df = co.create_wide_dataset(\n    hospitalization_ids=target_ids,\n    tables_to_load=['vitals', 'labs', 'patient_assessments'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp', 'spo2'],\n        'labs': ['lactate', 'hemoglobin'],\n        'patient_assessments': ['gcs_total', 'rass']\n    }\n)\n\nprint(f\"Analyzed {len(target_ids)} specific hospitalizations\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-4-time-window-filtering-with-cohort-dataframe","title":"Example 4: Time Window Filtering with Cohort DataFrame","text":"<p>Filter data to specific time windows:</p> <pre><code>import pandas as pd\n\n# Define cohort with time windows\ncohort_df = pd.DataFrame({\n    'hospitalization_id': ['12345', '67890', '11111'],\n    'start_time': ['2023-01-01 08:00:00', '2023-01-05 12:00:00', '2023-01-10 06:00:00'],\n    'end_time': ['2023-01-03 18:00:00', '2023-01-07 20:00:00', '2023-01-12 14:00:00']\n})\n\n# Convert to datetime\ncohort_df['start_time'] = pd.to_datetime(cohort_df['start_time'])\ncohort_df['end_time'] = pd.to_datetime(cohort_df['end_time'])\n\n# Create wide dataset with time filtering\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin', 'sodium']\n    },\n    cohort_df=cohort_df  # Only include data within specified time windows\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#example-5-no-batch-processing-for-small-datasets","title":"Example 5: No Batch Processing for Small Datasets","text":"<p>Disable batching for small datasets:</p> <pre><code># Small dataset - process all at once\nwide_df = co.create_wide_dataset(\n    hospitalization_ids=small_id_list,  # &lt; 100 hospitalizations\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin']\n    },\n    batch_size=-1  # Disable batching\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#memory-management-and-batch-processing","title":"Memory Management and Batch Processing","text":""},{"location":"user-guide/wide-dataset/#understanding-batch-processing","title":"Understanding Batch Processing","text":"<p>Batch processing divides hospitalizations into smaller groups to prevent memory (larger-than-memory, OOM) issues:</p> <pre><code># Large dataset - use batching\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    batch_size=1000,  # Process 1000 hospitalizations at a time\n    memory_limit=\"8GB\"\n)\n\n# Small dataset - disable batching for better performance\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={...},\n    batch_size=-1  # Process all at once\n)\n</code></pre>"},{"location":"user-guide/wide-dataset/#memory-optimization-guidelines","title":"Memory Optimization Guidelines","text":"Dataset Size Batch Size Memory Limit Threads &lt; 1,000 hospitalizations -1 (no batching) 4GB 2-4 1,000 - 10,000 hospitalizations 1000 8GB 4-8 &gt; 10,000 hospitalizations 500 16GB+ 6-12"},{"location":"user-guide/wide-dataset/#hourly-aggregation","title":"Hourly Aggregation","text":"<p>Convert the wide dataset to hourly aggregation:</p> <pre><code># Create aggregation configuration\naggregation_config = {\n    'max': ['sbp', 'map'],\n    'mean': ['heart_rate', 'respiratory_rate'],\n    'min': ['spo2'],\n    'median': ['temp_c'],\n    'first': ['gcs_total', 'rass'],\n    'last': ['assessment_value'],\n    'boolean': ['norepinephrine', 'propofol'],  # 1 if present, 0 if absent\n}\n\n# Convert to hourly\nhourly_df = co.convert_wide_to_hourly(\n    wide_df,\n    aggregation_config=aggregation_config,\n    memory_limit='8GB'\n)\n\nprint(f\"Hourly dataset: {hourly_df.shape}\")\nprint(f\"Hour range: {hourly_df['nth_hour'].min()} to {hourly_df['nth_hour'].max()}\")\n</code></pre>"},{"location":"user-guide/wide-dataset/#output-structure","title":"Output Structure","text":"<p>The wide dataset includes:</p>"},{"location":"user-guide/wide-dataset/#core-columns","title":"Core Columns","text":"<ul> <li><code>patient_id</code>: Patient identifier</li> <li><code>hospitalization_id</code>: Hospitalization identifier  </li> <li><code>event_time</code>: Timestamp for each event</li> <li><code>day_number</code>: Sequential day within hospitalization</li> <li><code>hosp_id_day_key</code>: Unique hospitalization-daily identifier</li> </ul>"},{"location":"user-guide/wide-dataset/#patient-demographics","title":"Patient Demographics","text":"<ul> <li><code>age_at_admission</code>: Patient age</li> <li>Additional patient table columns</li> </ul>"},{"location":"user-guide/wide-dataset/#adt-information","title":"ADT Information","text":"<ul> <li>Location and transfer data from ADT table</li> </ul>"},{"location":"user-guide/wide-dataset/#pivoted-data-columns","title":"Pivoted Data Columns","text":"<ul> <li>Individual columns for each category (e.g., <code>heart_rate</code>, <code>hemoglobin</code>, <code>norepinephrine</code>) as per use provided in <code>category_filters</code></li> <li>Values aligned by timestamp and hospitalization</li> </ul>"},{"location":"user-guide/wide-dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/wide-dataset/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Memory Errors <pre><code># Solution: Reduce batch size and set memory limit\nwide_df = co.create_wide_dataset(\n    batch_size=250,  # Smaller batches\n    memory_limit=\"4GB\",  # Conservative limit\n    sample=True  # Test with sample first\n)\n</code></pre></p> <p>System Crashes <pre><code># Solution: Check resources first and configure accordingly\nresources = co.get_sys_resource_info()\nif resources['memory_available_gb'] &lt; 8:\n    print(\"Warning: Low memory available. Consider using smaller batch_size.\")\n    batch_size = 250\nelse:\n    batch_size = 1000\n</code></pre></p> <p>Empty Results <pre><code># Check if tables and categories exist\nprint(\"Available tables:\", co.get_loaded_tables())\nif hasattr(co, 'vitals') and co.vitals is not None:\n    print(\"Vital categories:\", co.vitals.df['vital_category'].unique())\n</code></pre></p> <p>Slow Performance <pre><code># Use optimize settings\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals'],  # Start with one table\n    category_filters={\n        'vitals': ['heart_rate', 'sbp']  # Limit categories\n    },\n    threads=co.get_sys_resource_info(print_summary=False)['max_recommended_threads']\n)\n</code></pre></p>"},{"location":"user-guide/wide-dataset/#error-messages","title":"Error Messages","text":"Error Cause Solution \"Memory limit exceeded\" Dataset too large for available RAM Reduce batch_size, set memory_limit \"No event times found\" No data in specified tables/categories Check table data and category filters \"Missing required columns\" cohort_df missing required columns Ensure 'hospitalization_id', 'start_time', 'end_time' columns exist"},{"location":"user-guide/wide-dataset/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>Wide dataset creation integrates seamlessly with existing ClifOrchestrator workflows:</p> <pre><code># Traditional approach\nco = ClifOrchestrator('/path/to/data', 'parquet', 'UTC')\nco.initialize(['patient', 'hospitalization', 'adt', 'vitals', 'labs'])\n\n# Enhanced with wide dataset\nwide_df = co.create_wide_dataset(\n    tables_to_load=['vitals', 'labs'],\n    category_filters={\n        'vitals': ['heart_rate', 'sbp'],\n        'labs': ['hemoglobin']\n    }\n)\n\n# Continue with validation and analysis\nco.validate_all()\nanalysis_results = your_analysis_function(wide_df)\n</code></pre>"},{"location":"user-guide/wide-dataset/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about data validation to ensure data quality</li> <li>Explore individual table guides for detailed table documentation</li> <li>See the orchestrator guide for advanced orchestrator features</li> <li>Review timezone handling for multi-site data</li> </ul>"},{"location":"user-guide/tables/","title":"CLIF Tables Overview","text":"<p>CLIFpy implements all 9 tables defined in the CLIF 2.0.0 specification. Each table represents a specific aspect of ICU patient data, with standardized columns and validation rules. Detailed CLIF Data Dictionary is available here</p>"},{"location":"user-guide/tables/#data-standards","title":"Data Standards","text":"<p>Each table follows CLIF standards for:</p> <ul> <li> <p>\ud83c\udff7\ufe0f Standardized Categories</p> <p>Consistent values across institutions, validated against permissible value lists, and mapped from institution-specific values.</p> </li> <li> <p>\ud83c\udfe5 Source Preservation</p> <p>Original EHR data elements maintained alongside standardized mappings for institutional transparency.</p> </li> <li> <p>\ud83d\udd52 Timezone Handling</p> <p>All datetime columns are timezone-aware with consistent timezone across all tables and automatic conversion during loading.</p> </li> <li> <p>\ud83d\udd11 Composite Keys</p> <p>Unique record identification with duplicate detection and data integrity validation.</p> </li> </ul>"},{"location":"user-guide/tables/#available-tables","title":"Available Tables","text":"<pre><code>graph TD\n    Patient[Patient] --&gt; |patient_id| Hospitalization[Hospitalization]\n    Hospitalization --&gt; |hospitalization_id| ADT[ADT]\n    Hospitalization --&gt; |hospitalization_id| Labs[Labs]\n    Hospitalization --&gt; |hospitalization_id| Vitals[Vitals]\n    Hospitalization --&gt; |hospitalization_id| Meds[Medications]\n    Hospitalization --&gt; |hospitalization_id| Assess[Assessments]\n    Hospitalization --&gt; |hospitalization_id| Resp[Respiratory Support]\n    Hospitalization --&gt; |hospitalization_id| Pos[Position]</code></pre>"},{"location":"user-guide/tables/#patient","title":"Patient","text":"<p>Core demographic information including birth date, sex, race, ethnicity, and language. This is the primary table that links <code>patient_id</code> field to the <code>hospitalization_id</code> field in the Hospitalization table. For detailed API documentation, see Patient API</p>"},{"location":"user-guide/tables/#adt","title":"ADT","text":"<p>Admission, Discharge, and Transfer events tracking patient movement through different hospital locations (ICU, ward, ED, etc.).</p>"},{"location":"user-guide/tables/#hospitalization","title":"Hospitalization","text":"<p>Hospital admission and discharge information, including admission source, discharge disposition, and length of stay.</p>"},{"location":"user-guide/tables/#labs","title":"Labs","text":"<p>Laboratory test results with standardized categories (chemistry, hematology, etc.) and reference ranges.</p>"},{"location":"user-guide/tables/#vitals","title":"Vitals","text":"<p>Vital signs measurements including temperature, heart rate, blood pressure, respiratory rate, and oxygen saturation.</p>"},{"location":"user-guide/tables/#position","title":"Position","text":"<p>Patient positioning data, particularly important for prone positioning in ARDS management.</p>"},{"location":"user-guide/tables/#respiratory-support","title":"Respiratory Support","text":"<p>Ventilation and oxygen therapy data, including device types, settings, and observed values.</p>"},{"location":"user-guide/tables/#medications","title":"Medications","text":"<p>Continuous medication infusions with standardized drug categories and dosing information.</p>"},{"location":"user-guide/tables/#patient-assessments","title":"Patient Assessments","text":"<p>Clinical assessment scores including GCS, RASS, CAM-ICU, pain scores, and other standardized assessments.</p>"},{"location":"user-guide/tables/#common-table-features","title":"Common Table Features","text":"<p>All tables inherit from <code>BaseTable</code> and share these features:</p>"},{"location":"user-guide/tables/#data-loading","title":"Data Loading","text":"<pre><code>table = TableClass.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n</code></pre>"},{"location":"user-guide/tables/#validation","title":"Validation","text":"<pre><code>table.validate()\nif table.isvalid():\n    print(\"Validation passed\")\n</code></pre>"},{"location":"user-guide/tables/#summary-statistics","title":"Summary Statistics","text":"<pre><code>summary = table.get_summary()\nprint(f\"Rows: {summary['num_rows']}\")\nprint(f\"Memory: {summary['memory_usage_mb']} MB\")\n</code></pre>"},{"location":"user-guide/tables/#choosing-tables-for-your-analysis","title":"Choosing Tables for Your Analysis","text":""},{"location":"user-guide/tables/#for-patient-cohort-building","title":"For Patient Cohort Building","text":"<ul> <li>Start with <code>Patient</code> for demographics</li> <li>Add <code>Hospitalization</code> for admission criteria</li> <li>Include <code>ADT</code> for ICU/location-specific cohorts</li> </ul>"},{"location":"user-guide/tables/#for-clinical-outcomes","title":"For Clinical Outcomes","text":"<ul> <li>Use <code>Labs</code> for laboratory markers</li> <li>Add <code>Vitals</code> for physiological parameters</li> <li>Include <code>PatientAssessments</code> for severity scores</li> </ul>"},{"location":"user-guide/tables/#for-treatment-analysis","title":"For Treatment Analysis","text":"<ul> <li>Use <code>RespiratorySupport</code> for ventilation data</li> <li>Add <code>MedicationAdminContinuous</code> for drug therapy</li> <li>Include <code>Position</code> for positioning interventions</li> </ul>"},{"location":"user-guide/tables/#next-steps","title":"Next Steps","text":"<ul> <li>Explore individual table guides for detailed usage</li> <li>Learn about data validation</li> <li>See practical examples</li> <li>Review the API reference</li> </ul>"},{"location":"user-guide/tables/patient/","title":"Patient Table","text":"<p>The Patient table contains core demographic information and serves as the primary reference for all other CLIF tables through the <code>patient_id</code> field.</p>"},{"location":"user-guide/tables/patient/#overview","title":"Overview","text":"<p>The Patient table includes: - Unique patient identifiers - Birth and death dates - Demographics (sex, race, ethnicity) - Primary language</p>"},{"location":"user-guide/tables/patient/#required-columns","title":"Required Columns","text":"Column Type Description patient_id VARCHAR Unique patient identifier birth_date DATETIME Date of birth death_dttm DATETIME Date/time of death (null if alive) race_name VARCHAR Free-text race description race_category VARCHAR Standardized race category ethnicity_name VARCHAR Free-text ethnicity description ethnicity_category VARCHAR Standardized ethnicity category sex_name VARCHAR Free-text sex description sex_category VARCHAR Standardized sex category language_name VARCHAR Primary language language_category VARCHAR Standardized language category"},{"location":"user-guide/tables/patient/#standardized-categories","title":"Standardized Categories","text":""},{"location":"user-guide/tables/patient/#race-categories","title":"Race Categories","text":"<ul> <li>Black or African American</li> <li>White</li> <li>American Indian or Alaska Native</li> <li>Asian</li> <li>Native Hawaiian or Other Pacific Islander</li> <li>Unknown</li> <li>Other</li> </ul>"},{"location":"user-guide/tables/patient/#ethnicity-categories","title":"Ethnicity Categories","text":"<ul> <li>Hispanic</li> <li>Non-Hispanic</li> <li>Unknown</li> </ul>"},{"location":"user-guide/tables/patient/#sex-categories","title":"Sex Categories","text":"<ul> <li>Male</li> <li>Female</li> <li>Unknown</li> </ul>"},{"location":"user-guide/tables/patient/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/tables/patient/#loading-patient-data","title":"Loading Patient Data","text":"<pre><code>from clifpy.tables import Patient\n\n# Load from file\npatient = Patient.from_file(\n    data_directory='/path/to/data',\n    filetype='parquet',\n    timezone='US/Central'\n)\n\n# Validate the data\npatient.validate()\n</code></pre>"},{"location":"user-guide/tables/patient/#basic-analysis","title":"Basic Analysis","text":"<pre><code># Get summary statistics\nsummary = patient.get_summary()\nprint(f\"Total patients: {summary['num_rows']}\")\n\n# Demographics distribution\ndemographics = patient.df.groupby(['sex_category', 'race_category']).size()\nprint(demographics)\n\n# Age calculation (if needed)\npatient.df['age'] = (\n    pd.Timestamp.now() - patient.df['birth_date']\n).dt.days / 365.25\n\n# Find elderly patients\nelderly = patient.df[patient.df['age'] &gt;= 65]\n</code></pre>"},{"location":"user-guide/tables/patient/#cohort-building","title":"Cohort Building","text":"<pre><code># Female patients over 65\ncohort = patient.df[\n    (patient.df['sex_category'] == 'Female') &amp; \n    (patient.df['age'] &gt;= 65)\n]\n\n# Living patients\nalive = patient.df[patient.df['death_dttm'].isna()]\n\n# Specific ethnicity\nhispanic = patient.df[patient.df['ethnicity_category'] == 'Hispanic']\n</code></pre>"},{"location":"user-guide/tables/patient/#joining-with-other-tables","title":"Joining with Other Tables","text":"<pre><code># Get patient demographics for lab results\nlabs_with_demographics = labs.df.merge(\n    patient.df[['patient_id', 'age', 'sex_category']],\n    on='patient_id',\n    how='left'\n)\n\n# Analyze by demographic groups\nlab_by_sex = labs_with_demographics.groupby('sex_category')['lab_value'].mean()\n</code></pre>"},{"location":"user-guide/tables/patient/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Check for missing demographics\nmissing_sex = patient.df[patient.df['sex_category'].isna()]\nmissing_race = patient.df[patient.df['race_category'].isna()]\n\n# Validate age ranges\npatient.df['age'] = (pd.Timestamp.now() - patient.df['birth_date']).dt.days / 365.25\ninvalid_age = patient.df[(patient.df['age'] &lt; 0) | (patient.df['age'] &gt; 120)]\n\n# Check death date consistency\ninvalid_death = patient.df[\n    patient.df['death_dttm'] &lt; patient.df['birth_date']\n]\n</code></pre>"},{"location":"user-guide/tables/patient/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate demographic categories against standardized values</li> <li>Handle missing data appropriately for demographic fields</li> <li>Calculate age at time of admission, not current date</li> <li>Protect PHI by using only de-identified patient_ids</li> <li>Document any demographic data transformations</li> </ol>"},{"location":"user-guide/tables/patient/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see Patient API</p>"}]}